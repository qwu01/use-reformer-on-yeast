{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from transformers import ReformerConfig, ReformerModelWithLMHead, ReformerTokenizer, EncoderDecoderConfig, EncoderDecoderModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "NUM_BATCHES = None\n",
    "BATCH_SIZE = 20\n",
    "LEARNING_RATE = 0.001 #1e-4 #1e-4\n",
    "VALIDATE_EVERY  = 10\n",
    "SEQ_LEN = 4608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm.SentencePieceTrainer.Train(\"--input=./data/tokenizer_training/AAresiduals.txt \\\n",
    "#                                 --vocab_size=28 \\\n",
    "#                                 --model_prefix=sequence_tokenizer \\\n",
    "#                                 --model_type=char \\\n",
    "#                                 --character_coverage=1.0\")\n",
    "tokenizer = ReformerTokenizer(vocab_file=\"sequence_tokenizer.model\", do_lower_case=False, model_max_length=SEQ_LEN)\n",
    "tokenizer.max_model_input_sizes = SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def split_file(file,out1,out2,percentage=0.75,isShuffle=True,seed=42):\n",
    "#     random.seed(seed)\n",
    "#     with open(file, 'r',encoding=\"utf-8\") as fin, open(out1, 'w') as foutBig, open(out2, 'w') as foutSmall:\n",
    "#         nLines = sum(1 for line in fin)\n",
    "#         fin.seek(0)\n",
    "\n",
    "#         nTrain = int(nLines*percentage) \n",
    "#         nValid = nLines - nTrain\n",
    "\n",
    "#         i = 0\n",
    "#         for line in fin:\n",
    "#             r = random.random() if isShuffle else 0 # so that always evaluated to true when not isShuffle\n",
    "#             if (i < nTrain and r < percentage) or (nLines - i > nValid):\n",
    "#                 foutBig.write(line)\n",
    "#                 i += 1\n",
    "#             else:\n",
    "#                 foutSmall.write(line)\n",
    "                \n",
    "# split_file(\"data/yeast/yeast.txt\", \n",
    "#            \"data/yeast/yeast_train.txt\",\n",
    "#            \"data/yeast/yeast_val.txt\",\n",
    "#            percentage=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels, tokenizer, _len):\n",
    "        super().__init__()\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self._len = _len\n",
    "\n",
    "    @classmethod\n",
    "    def prepare_from_file(cls, file_path, tokenizer):\n",
    "        \n",
    "        with open(file_path) as file:\n",
    "            \n",
    "            X = [l.strip() for l in file]\n",
    "            X = [tokenizer.encode(sequence)[1:tokenizer.max_len+1] for sequence in X]\n",
    "            \n",
    "            temp = [tokenizer.prepare_for_model(sequence) for sequence in X]\n",
    "            \n",
    "            input_ids = [np.pad(x[\"input_ids\"], \n",
    "                                (0, tokenizer.max_len - len(x[\"input_ids\"])), \n",
    "                                'constant', constant_values=0) for x in temp]\n",
    "\n",
    "            attention_mask = [np.pad(x[\"attention_mask\"], \n",
    "                                     (0, tokenizer.max_len - len(x[\"attention_mask\"])),\n",
    "                                     'constant', constant_values=0) for x in temp]\n",
    "            \n",
    "            labels = [np.pad(x[\"input_ids\"], \n",
    "                             (0, tokenizer.max_len - len(x[\"input_ids\"])), \n",
    "                             'constant', constant_values=-100) for x in temp]\n",
    "\n",
    "            input_ids = [torch.tensor(x, dtype=torch.int64) for x in input_ids]\n",
    "            attention_mask = [torch.tensor(x, dtype=torch.int64) for x in attention_mask]\n",
    "            labels = [torch.tensor(x, dtype=torch.int64) for x in labels]\n",
    "            \n",
    "            input_ids = torch.stack([input_ids[i] for i in range(len(input_ids))]).squeeze()\n",
    "            attention_mask = torch.stack([attention_mask[i] for i in range(len(attention_mask))]).squeeze()\n",
    "            labels = torch.stack([labels[i] for i in range(len(labels))]).squeeze()\n",
    "            \n",
    "            del(temp); del(X);\n",
    "        return cls(input_ids, attention_mask, labels, tokenizer, len(input_ids))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"input_ids\": self.input_ids[index, ].cuda(), \n",
    "                \"attention_mask\": self.attention_mask[index, ].cuda(),\n",
    "                \"labels\": self.labels[index, ].cuda()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4911 > 4608). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_dataset = SequenceDataset.prepare_from_file(\"data/yeast/yeast_train.txt\", tokenizer)\n",
    "val_dataset = SequenceDataset.prepare_from_file(\"data/yeast/yeast_val.txt\", tokenizer)\n",
    "train_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\n",
    "val_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(train_loader)['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerModelWithLMHead(\n",
       "  (reformer): ReformerModel(\n",
       "    (embeddings): ReformerEmbeddings(\n",
       "      (word_embeddings): Embedding(28, 256)\n",
       "      (position_embeddings): AxialPositionEmbeddings(\n",
       "        (weights): ParameterList(\n",
       "            (0): Parameter containing: [torch.cuda.FloatTensor of size 64x1x64 (GPU 0)]\n",
       "            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x72x192 (GPU 0)]\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): ReformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (key): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=128, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=128, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (key): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=128, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=128, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (key): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=128, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=256, out_features=128, bias=False)\n",
       "              (value): Linear(in_features=256, out_features=128, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): ReformerOnlyLMHead(\n",
       "    (decoder): Linear(in_features=512, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configuration = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\")\n",
    "# configuration.axial_pos_shape = (64, 72)\n",
    "# configuration.max_position_embeddings=SEQ_LEN\n",
    "# configuration.vocab_size=tokenizer.vocab_size\n",
    "# configuration.save_pretrained('model/config/')\n",
    "configuration = ReformerConfig.from_pretrained('model/config/')\n",
    "model = ReformerModelWithLMHead(configuration)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCHES = len(train_dataset)//BATCH_SIZE\n",
    "NUM_BATCHES\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "step 0\n",
      "training loss: 2.8765008449554443\n",
      "validation loss: 2.8412394523620605\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.884265184402466\n",
      "step 2\n",
      "training loss: 2.874288320541382\n",
      "step 3\n",
      "training loss: 2.856649160385132\n",
      "step 4\n",
      "training loss: 2.862579822540283\n",
      "step 5\n",
      "training loss: 2.8804471492767334\n",
      "step 6\n",
      "training loss: 2.8804876804351807\n",
      "step 7\n",
      "training loss: 2.8839030265808105\n",
      "step 8\n",
      "training loss: 2.868535041809082\n",
      "step 9\n",
      "training loss: 2.885714054107666\n",
      "step 10\n",
      "training loss: 2.8828983306884766\n",
      "validation loss: 2.8691999912261963\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.844682216644287\n",
      "step 12\n",
      "training loss: 2.8486948013305664\n",
      "step 13\n",
      "training loss: 2.8649795055389404\n",
      "step 14\n",
      "training loss: 2.8672597408294678\n",
      "step 15\n",
      "training loss: 2.875302314758301\n",
      "step 16\n",
      "training loss: 2.8728182315826416\n",
      "step 17\n",
      "training loss: 2.873661756515503\n",
      "step 18\n",
      "training loss: 2.877591371536255\n",
      "step 19\n",
      "training loss: 2.873741388320923\n",
      "step 20\n",
      "training loss: 2.886409044265747\n",
      "validation loss: 2.9047112464904785\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.867300510406494\n",
      "step 22\n",
      "training loss: 2.877129316329956\n",
      "step 23\n",
      "training loss: 2.8853909969329834\n",
      "step 24\n",
      "training loss: 2.8808207511901855\n",
      "step 25\n",
      "training loss: 2.875570297241211\n",
      "step 26\n",
      "training loss: 2.878633499145508\n",
      "step 27\n",
      "training loss: 2.863851308822632\n",
      "step 28\n",
      "training loss: 2.845071315765381\n",
      "step 29\n",
      "training loss: 2.8710598945617676\n",
      "step 30\n",
      "training loss: 2.8787081241607666\n",
      "validation loss: 2.884591579437256\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8373165130615234\n",
      "step 32\n",
      "training loss: 2.8655970096588135\n",
      "step 33\n",
      "training loss: 2.8716647624969482\n",
      "step 34\n",
      "training loss: 2.8603944778442383\n",
      "step 35\n",
      "training loss: 2.88401460647583\n",
      "step 36\n",
      "training loss: 2.8603463172912598\n",
      "step 37\n",
      "training loss: 2.8768911361694336\n",
      "step 38\n",
      "training loss: 2.8581628799438477\n",
      "step 39\n",
      "training loss: 2.8729653358459473\n",
      "step 40\n",
      "training loss: 2.823460817337036\n",
      "validation loss: 2.882335901260376\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8843822479248047\n",
      "step 42\n",
      "training loss: 2.8533122539520264\n",
      "step 43\n",
      "training loss: 2.886078357696533\n",
      "step 44\n",
      "training loss: 2.8722920417785645\n",
      "step 45\n",
      "training loss: 2.8515801429748535\n",
      "step 46\n",
      "training loss: 2.8469412326812744\n",
      "step 47\n",
      "training loss: 2.8691470623016357\n",
      "step 48\n",
      "training loss: 2.859290361404419\n",
      "step 49\n",
      "training loss: 2.851703643798828\n",
      "step 50\n",
      "training loss: 2.8863587379455566\n",
      "validation loss: 2.9536690711975098\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.874526023864746\n",
      "step 52\n",
      "training loss: 2.864100694656372\n",
      "step 53\n",
      "training loss: 2.8494327068328857\n",
      "step 54\n",
      "training loss: 2.8618552684783936\n",
      "step 55\n",
      "training loss: 2.863661289215088\n",
      "step 56\n",
      "training loss: 2.8377130031585693\n",
      "step 57\n",
      "training loss: 2.8548049926757812\n",
      "step 58\n",
      "training loss: 2.856732130050659\n",
      "step 59\n",
      "training loss: 2.8483264446258545\n",
      "step 60\n",
      "training loss: 2.840471029281616\n",
      "validation loss: 2.8940608501434326\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8787312507629395\n",
      "step 62\n",
      "training loss: 2.8556180000305176\n",
      "step 63\n",
      "training loss: 2.886589765548706\n",
      "step 64\n",
      "training loss: 2.8669180870056152\n",
      "step 65\n",
      "training loss: 2.849313259124756\n",
      "step 66\n",
      "training loss: 2.882150411605835\n",
      "step 67\n",
      "training loss: 2.857895612716675\n",
      "step 68\n",
      "training loss: 2.8674755096435547\n",
      "step 69\n",
      "training loss: 2.865777015686035\n",
      "step 70\n",
      "training loss: 2.8402011394500732\n",
      "validation loss: 2.860736608505249\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8803939819335938\n",
      "step 72\n",
      "training loss: 2.8640122413635254\n",
      "step 73\n",
      "training loss: 2.878472328186035\n",
      "step 74\n",
      "training loss: 2.874812602996826\n",
      "step 75\n",
      "training loss: 2.897843360900879\n",
      "step 76\n",
      "training loss: 2.881225347518921\n",
      "step 77\n",
      "training loss: 2.844585418701172\n",
      "step 78\n",
      "training loss: 2.879976987838745\n",
      "step 79\n",
      "training loss: 2.8638663291931152\n",
      "step 80\n",
      "training loss: 2.8542792797088623\n",
      "validation loss: 2.871460199356079\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8611128330230713\n",
      "step 82\n",
      "training loss: 2.8593664169311523\n",
      "step 83\n",
      "training loss: 2.8791074752807617\n",
      "step 84\n",
      "training loss: 2.8653385639190674\n",
      "step 85\n",
      "training loss: 2.8812458515167236\n",
      "step 86\n",
      "training loss: 2.8815128803253174\n",
      "step 87\n",
      "training loss: 2.8820860385894775\n",
      "step 88\n",
      "training loss: 2.859867811203003\n",
      "step 89\n",
      "training loss: 2.869070529937744\n",
      "step 90\n",
      "training loss: 2.868039608001709\n",
      "validation loss: 2.901237726211548\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.857326030731201\n",
      "step 92\n",
      "training loss: 2.870161771774292\n",
      "step 93\n",
      "training loss: 2.8471832275390625\n",
      "step 94\n",
      "training loss: 2.8735458850860596\n",
      "step 95\n",
      "training loss: 2.8770787715911865\n",
      "step 96\n",
      "training loss: 2.8254036903381348\n",
      "step 97\n",
      "training loss: 2.8608922958374023\n",
      "step 98\n",
      "training loss: 2.8535048961639404\n",
      "step 99\n",
      "training loss: 2.8677608966827393\n",
      "step 100\n",
      "training loss: 2.8767881393432617\n",
      "validation loss: 2.836087942123413\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8505897521972656\n",
      "step 102\n",
      "training loss: 2.87080717086792\n",
      "step 103\n",
      "training loss: 2.826887845993042\n",
      "step 104\n",
      "training loss: 2.858184337615967\n",
      "step 105\n",
      "training loss: 2.877929210662842\n",
      "step 106\n",
      "training loss: 2.869824171066284\n",
      "step 107\n",
      "training loss: 2.861727237701416\n",
      "step 108\n",
      "training loss: 2.851984977722168\n",
      "step 109\n",
      "training loss: 2.8714914321899414\n",
      "step 110\n",
      "training loss: 2.8581466674804688\n",
      "validation loss: 2.8595786094665527\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8646256923675537\n",
      "step 112\n",
      "training loss: 2.8764495849609375\n",
      "step 113\n",
      "training loss: 2.8662893772125244\n",
      "step 114\n",
      "training loss: 2.862306833267212\n",
      "step 115\n",
      "training loss: 2.8628385066986084\n",
      "step 116\n",
      "training loss: 2.8415544033050537\n",
      "step 117\n",
      "training loss: 2.8423023223876953\n",
      "step 118\n",
      "training loss: 2.8364577293395996\n",
      "step 119\n",
      "training loss: 2.867283821105957\n",
      "step 120\n",
      "training loss: 2.8765878677368164\n",
      "validation loss: 2.86531138420105\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8645706176757812\n",
      "step 122\n",
      "training loss: 2.864525318145752\n",
      "step 123\n",
      "training loss: 2.8578641414642334\n",
      "step 124\n",
      "training loss: 2.874575138092041\n",
      "step 125\n",
      "training loss: 2.8946149349212646\n",
      "step 126\n",
      "training loss: 2.8154494762420654\n",
      "step 127\n",
      "training loss: 2.86580491065979\n",
      "step 128\n",
      "training loss: 2.857024908065796\n",
      "step 129\n",
      "training loss: 2.881882667541504\n",
      "step 130\n",
      "training loss: 2.86082124710083\n",
      "validation loss: 2.877922296524048\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.887515068054199\n",
      "step 132\n",
      "training loss: 2.8561747074127197\n",
      "step 133\n",
      "training loss: 2.8446311950683594\n",
      "step 134\n",
      "training loss: 2.8565874099731445\n",
      "step 135\n",
      "training loss: 2.869008779525757\n",
      "step 136\n",
      "training loss: 2.8517444133758545\n",
      "step 137\n",
      "training loss: 2.883685350418091\n",
      "step 138\n",
      "training loss: 2.8675436973571777\n",
      "step 139\n",
      "training loss: 2.8320767879486084\n",
      "step 140\n",
      "training loss: 2.856851816177368\n",
      "validation loss: 2.9152932167053223\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.858273983001709\n",
      "step 142\n",
      "training loss: 2.858823776245117\n",
      "step 143\n",
      "training loss: 2.873549222946167\n",
      "step 144\n",
      "training loss: 2.868976354598999\n",
      "step 145\n",
      "training loss: 2.8705687522888184\n",
      "step 146\n",
      "training loss: 2.831777811050415\n",
      "step 147\n",
      "training loss: 2.8658456802368164\n",
      "step 148\n",
      "training loss: 2.8624255657196045\n",
      "step 149\n",
      "training loss: 2.870656967163086\n",
      "step 150\n",
      "training loss: 2.8675010204315186\n",
      "validation loss: 2.8696770668029785\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8797543048858643\n",
      "step 152\n",
      "training loss: 2.8732497692108154\n",
      "step 153\n",
      "training loss: 2.8548824787139893\n",
      "step 154\n",
      "training loss: 2.8831303119659424\n",
      "step 155\n",
      "training loss: 2.874462127685547\n",
      "step 156\n",
      "training loss: 2.8668649196624756\n",
      "step 157\n",
      "training loss: 2.8642499446868896\n",
      "step 158\n",
      "training loss: 2.871765375137329\n",
      "step 159\n",
      "training loss: 2.8612215518951416\n",
      "step 160\n",
      "training loss: 2.8609960079193115\n",
      "validation loss: 2.8654870986938477\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8459279537200928\n",
      "step 162\n",
      "training loss: 2.83062481880188\n",
      "step 163\n",
      "training loss: 2.874577522277832\n",
      "step 164\n",
      "training loss: 2.8511810302734375\n",
      "step 165\n",
      "training loss: 2.8526499271392822\n",
      "step 166\n",
      "training loss: 2.8680925369262695\n",
      "step 167\n",
      "training loss: 2.8953561782836914\n",
      "step 168\n",
      "training loss: 2.8494250774383545\n",
      "step 169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.873277425765991\n",
      "step 170\n",
      "training loss: 2.8667941093444824\n",
      "validation loss: 2.830172061920166\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8753302097320557\n",
      "step 172\n",
      "training loss: 2.85284161567688\n",
      "step 173\n",
      "training loss: 2.8603901863098145\n",
      "step 174\n",
      "training loss: 2.8387258052825928\n",
      "step 175\n",
      "training loss: 2.853752374649048\n",
      "step 176\n",
      "training loss: 2.860642671585083\n",
      "step 177\n",
      "training loss: 2.848814010620117\n",
      "step 178\n",
      "training loss: 2.8915841579437256\n",
      "step 179\n",
      "training loss: 2.8639743328094482\n",
      "step 180\n",
      "training loss: 2.855835437774658\n",
      "validation loss: 2.8348782062530518\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8677563667297363\n",
      "step 182\n",
      "training loss: 2.8688716888427734\n",
      "step 183\n",
      "training loss: 2.8593363761901855\n",
      "step 184\n",
      "training loss: 2.8514082431793213\n",
      "step 185\n",
      "training loss: 2.8339900970458984\n",
      "step 186\n",
      "training loss: 2.8253090381622314\n",
      "step 187\n",
      "training loss: 2.8785059452056885\n",
      "step 188\n",
      "training loss: 2.8570072650909424\n",
      "step 189\n",
      "training loss: 2.887006998062134\n",
      "step 190\n",
      "training loss: 2.8593218326568604\n",
      "validation loss: 2.8377790451049805\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8687877655029297\n",
      "step 192\n",
      "training loss: 2.88346791267395\n",
      "step 193\n",
      "training loss: 2.8632538318634033\n",
      "step 194\n",
      "training loss: 2.855631113052368\n",
      "step 195\n",
      "training loss: 2.869704484939575\n",
      "step 196\n",
      "training loss: 2.8496549129486084\n",
      "step 197\n",
      "training loss: 2.8570635318756104\n",
      "step 198\n",
      "training loss: 2.8492908477783203\n",
      "step 199\n",
      "training loss: 2.8538308143615723\n",
      "step 200\n",
      "training loss: 2.8426973819732666\n",
      "validation loss: 2.863759994506836\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.86175274848938\n",
      "step 202\n",
      "training loss: 2.8657422065734863\n",
      "step 203\n",
      "training loss: 2.848888635635376\n",
      "step 204\n",
      "training loss: 2.8757262229919434\n",
      "step 205\n",
      "training loss: 2.854081630706787\n",
      "step 206\n",
      "training loss: 2.8660764694213867\n",
      "step 207\n",
      "training loss: 2.858832836151123\n",
      "step 208\n",
      "training loss: 2.8711891174316406\n",
      "step 209\n",
      "training loss: 2.7970426082611084\n",
      "step 210\n",
      "training loss: 2.853489637374878\n",
      "validation loss: 2.866788864135742\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.836416482925415\n",
      "step 212\n",
      "training loss: 2.8512537479400635\n",
      "step 213\n",
      "training loss: 2.875347375869751\n",
      "step 214\n",
      "training loss: 2.8761818408966064\n",
      "step 215\n",
      "training loss: 2.868619203567505\n",
      "step 216\n",
      "training loss: 2.8664321899414062\n",
      "step 217\n",
      "training loss: 2.866455554962158\n",
      "step 218\n",
      "training loss: 2.854022741317749\n",
      "step 219\n",
      "training loss: 2.857632875442505\n",
      "step 220\n",
      "training loss: 2.8728675842285156\n",
      "validation loss: 2.865060567855835\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8867087364196777\n",
      "step 222\n",
      "training loss: 2.8638195991516113\n",
      "step 223\n",
      "training loss: 2.848114013671875\n",
      "step 224\n",
      "training loss: 2.8520405292510986\n",
      "step 225\n",
      "training loss: 2.8519861698150635\n",
      "step 226\n",
      "training loss: 2.865525245666504\n",
      "step 227\n",
      "training loss: 2.8681888580322266\n",
      "step 228\n",
      "training loss: 2.860177993774414\n",
      "step 229\n",
      "training loss: 2.8708832263946533\n",
      "step 230\n",
      "training loss: 2.876121997833252\n",
      "validation loss: 2.864093065261841\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.851454496383667\n",
      "step 232\n",
      "training loss: 2.8747916221618652\n",
      "step 233\n",
      "training loss: 2.878239154815674\n",
      "step 234\n",
      "training loss: 2.8481087684631348\n",
      "step 235\n",
      "training loss: 2.8505473136901855\n",
      "step 236\n",
      "training loss: 2.873870849609375\n",
      "step 237\n",
      "training loss: 2.8672707080841064\n",
      "step 238\n",
      "training loss: 2.8681299686431885\n",
      "step 239\n",
      "training loss: 2.868661880493164\n",
      "step 240\n",
      "training loss: 2.8706905841827393\n",
      "validation loss: 2.868342161178589\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8639626502990723\n",
      "step 242\n",
      "training loss: 2.86568546295166\n",
      "step 243\n",
      "training loss: 2.86631178855896\n",
      "step 244\n",
      "training loss: 2.8622827529907227\n",
      "step 245\n",
      "training loss: 2.862128973007202\n",
      "step 246\n",
      "training loss: 2.8258039951324463\n",
      "step 247\n",
      "training loss: 2.853484869003296\n",
      "step 248\n",
      "training loss: 2.874565362930298\n",
      "step 249\n",
      "training loss: 2.8502631187438965\n",
      "step 250\n",
      "training loss: 2.8615095615386963\n",
      "validation loss: 2.8440659046173096\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8950986862182617\n",
      "step 252\n",
      "training loss: 2.87155818939209\n",
      "step 253\n",
      "training loss: 2.887673854827881\n",
      "step 254\n",
      "training loss: 2.886521816253662\n",
      "step 255\n",
      "training loss: 2.8728973865509033\n",
      "step 256\n",
      "training loss: 2.85882306098938\n",
      "step 257\n",
      "training loss: 2.87855863571167\n",
      "step 258\n",
      "training loss: 2.879702091217041\n",
      "step 259\n",
      "training loss: 2.8741793632507324\n",
      "step 260\n",
      "training loss: 2.87247371673584\n",
      "validation loss: 2.8589210510253906\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.883141279220581\n",
      "step 262\n",
      "training loss: 2.892695426940918\n",
      "step 263\n",
      "training loss: 2.816159963607788\n",
      "step 264\n",
      "training loss: 2.8548879623413086\n",
      "step 265\n",
      "training loss: 2.823298931121826\n",
      "step 266\n",
      "training loss: 2.8562748432159424\n",
      "step 267\n",
      "training loss: 2.895076036453247\n",
      "step 268\n",
      "training loss: 2.8878257274627686\n",
      "step 269\n",
      "training loss: 2.8891708850860596\n",
      "step 270\n",
      "training loss: 2.885075569152832\n",
      "validation loss: 2.8337903022766113\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.813215732574463\n",
      "----------3.0 min per epoch----------\n",
      "epoch 1\n",
      "step 0\n",
      "training loss: 2.763002634048462\n",
      "validation loss: 2.830683469772339\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.875918388366699\n",
      "step 2\n",
      "training loss: 2.8922946453094482\n",
      "step 3\n",
      "training loss: 2.876228094100952\n",
      "step 4\n",
      "training loss: 2.863860607147217\n",
      "step 5\n",
      "training loss: 2.864506244659424\n",
      "step 6\n",
      "training loss: 2.8799357414245605\n",
      "step 7\n",
      "training loss: 2.8816213607788086\n",
      "step 8\n",
      "training loss: 2.884350061416626\n",
      "step 9\n",
      "training loss: 2.871427059173584\n",
      "step 10\n",
      "training loss: 2.8873379230499268\n",
      "validation loss: 2.8791279792785645\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8801698684692383\n",
      "step 12\n",
      "training loss: 2.8448002338409424\n",
      "step 13\n",
      "training loss: 2.849482297897339\n",
      "step 14\n",
      "training loss: 2.8650782108306885\n",
      "step 15\n",
      "training loss: 2.8695807456970215\n",
      "step 16\n",
      "training loss: 2.875026226043701\n",
      "step 17\n",
      "training loss: 2.8739776611328125\n",
      "step 18\n",
      "training loss: 2.8767964839935303\n",
      "step 19\n",
      "training loss: 2.8752987384796143\n",
      "step 20\n",
      "training loss: 2.873202323913574\n",
      "validation loss: 2.8814852237701416\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.886134624481201\n",
      "step 22\n",
      "training loss: 2.869849443435669\n",
      "step 23\n",
      "training loss: 2.8786232471466064\n",
      "step 24\n",
      "training loss: 2.8845739364624023\n",
      "step 25\n",
      "training loss: 2.8802859783172607\n",
      "step 26\n",
      "training loss: 2.878662109375\n",
      "step 27\n",
      "training loss: 2.8819735050201416\n",
      "step 28\n",
      "training loss: 2.8622324466705322\n",
      "step 29\n",
      "training loss: 2.845116138458252\n",
      "step 30\n",
      "training loss: 2.8672826290130615\n",
      "validation loss: 2.8395326137542725\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.879429817199707\n",
      "step 32\n",
      "training loss: 2.8463785648345947\n",
      "step 33\n",
      "training loss: 2.864716053009033\n",
      "step 34\n",
      "training loss: 2.869286060333252\n",
      "step 35\n",
      "training loss: 2.861678123474121\n",
      "step 36\n",
      "training loss: 2.8835091590881348\n",
      "step 37\n",
      "training loss: 2.862006187438965\n",
      "step 38\n",
      "training loss: 2.876464605331421\n",
      "step 39\n",
      "training loss: 2.85778546333313\n",
      "step 40\n",
      "training loss: 2.874464511871338\n",
      "validation loss: 2.8621299266815186\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.829415798187256\n",
      "step 42\n",
      "training loss: 2.8856546878814697\n",
      "step 43\n",
      "training loss: 2.8517942428588867\n",
      "step 44\n",
      "training loss: 2.8863213062286377\n",
      "step 45\n",
      "training loss: 2.872100591659546\n",
      "step 46\n",
      "training loss: 2.8523359298706055\n",
      "step 47\n",
      "training loss: 2.8462440967559814\n",
      "step 48\n",
      "training loss: 2.869061231613159\n",
      "step 49\n",
      "training loss: 2.861266851425171\n",
      "step 50\n",
      "training loss: 2.8528833389282227\n",
      "validation loss: 2.9089980125427246\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8839380741119385\n",
      "step 52\n",
      "training loss: 2.8716888427734375\n",
      "step 53\n",
      "training loss: 2.862445592880249\n",
      "step 54\n",
      "training loss: 2.8511931896209717\n",
      "step 55\n",
      "training loss: 2.8627593517303467\n",
      "step 56\n",
      "training loss: 2.8636863231658936\n",
      "step 57\n",
      "training loss: 2.8422698974609375\n",
      "step 58\n",
      "training loss: 2.8529562950134277\n",
      "step 59\n",
      "training loss: 2.8567135334014893\n",
      "step 60\n",
      "training loss: 2.847299337387085\n",
      "validation loss: 2.8903260231018066\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.839247226715088\n",
      "step 62\n",
      "training loss: 2.8777225017547607\n",
      "step 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8506546020507812\n",
      "step 64\n",
      "training loss: 2.8859617710113525\n",
      "step 65\n",
      "training loss: 2.8663883209228516\n",
      "step 66\n",
      "training loss: 2.8444063663482666\n",
      "step 67\n",
      "training loss: 2.8789453506469727\n",
      "step 68\n",
      "training loss: 2.8585915565490723\n",
      "step 69\n",
      "training loss: 2.8668394088745117\n",
      "step 70\n",
      "training loss: 2.8662843704223633\n",
      "validation loss: 2.8824474811553955\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8400392532348633\n",
      "step 72\n",
      "training loss: 2.877986431121826\n",
      "step 73\n",
      "training loss: 2.862908124923706\n",
      "step 74\n",
      "training loss: 2.8804614543914795\n",
      "step 75\n",
      "training loss: 2.8763272762298584\n",
      "step 76\n",
      "training loss: 2.897451400756836\n",
      "step 77\n",
      "training loss: 2.8809328079223633\n",
      "step 78\n",
      "training loss: 2.842601776123047\n",
      "step 79\n",
      "training loss: 2.879701852798462\n",
      "step 80\n",
      "training loss: 2.862748622894287\n",
      "validation loss: 2.9303300380706787\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.855797290802002\n",
      "step 82\n",
      "training loss: 2.860722541809082\n",
      "step 83\n",
      "training loss: 2.859924077987671\n",
      "step 84\n",
      "training loss: 2.8808674812316895\n",
      "step 85\n",
      "training loss: 2.8634746074676514\n",
      "step 86\n",
      "training loss: 2.8821122646331787\n",
      "step 87\n",
      "training loss: 2.881387948989868\n",
      "step 88\n",
      "training loss: 2.8819260597229004\n",
      "step 89\n",
      "training loss: 2.8592770099639893\n",
      "step 90\n",
      "training loss: 2.867914915084839\n",
      "validation loss: 2.883397340774536\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.868095636367798\n",
      "step 92\n",
      "training loss: 2.858632802963257\n",
      "step 93\n",
      "training loss: 2.867687702178955\n",
      "step 94\n",
      "training loss: 2.844475746154785\n",
      "step 95\n",
      "training loss: 2.8708066940307617\n",
      "step 96\n",
      "training loss: 2.874786376953125\n",
      "step 97\n",
      "training loss: 2.8241899013519287\n",
      "step 98\n",
      "training loss: 2.8599908351898193\n",
      "step 99\n",
      "training loss: 2.85212779045105\n",
      "step 100\n",
      "training loss: 2.8671581745147705\n",
      "validation loss: 2.8496456146240234\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8755605220794678\n",
      "step 102\n",
      "training loss: 2.8496034145355225\n",
      "step 103\n",
      "training loss: 2.87070631980896\n",
      "step 104\n",
      "training loss: 2.829219341278076\n",
      "step 105\n",
      "training loss: 2.855060338973999\n",
      "step 106\n",
      "training loss: 2.8772695064544678\n",
      "step 107\n",
      "training loss: 2.870173454284668\n",
      "step 108\n",
      "training loss: 2.861003875732422\n",
      "step 109\n",
      "training loss: 2.854295015335083\n",
      "step 110\n",
      "training loss: 2.8717870712280273\n",
      "validation loss: 2.8658292293548584\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8565330505371094\n",
      "step 112\n",
      "training loss: 2.8675997257232666\n",
      "step 113\n",
      "training loss: 2.873763084411621\n",
      "step 114\n",
      "training loss: 2.864698886871338\n",
      "step 115\n",
      "training loss: 2.8615548610687256\n",
      "step 116\n",
      "training loss: 2.8627541065216064\n",
      "step 117\n",
      "training loss: 2.8409457206726074\n",
      "step 118\n",
      "training loss: 2.844468832015991\n",
      "step 119\n",
      "training loss: 2.8375601768493652\n",
      "step 120\n",
      "training loss: 2.867933750152588\n",
      "validation loss: 2.9025983810424805\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8743956089019775\n",
      "step 122\n",
      "training loss: 2.869706869125366\n",
      "step 123\n",
      "training loss: 2.8646862506866455\n",
      "step 124\n",
      "training loss: 2.855015993118286\n",
      "step 125\n",
      "training loss: 2.873215436935425\n",
      "step 126\n",
      "training loss: 2.8939406871795654\n",
      "step 127\n",
      "training loss: 2.8160324096679688\n",
      "step 128\n",
      "training loss: 2.8633029460906982\n",
      "step 129\n",
      "training loss: 2.8561763763427734\n",
      "step 130\n",
      "training loss: 2.8831028938293457\n",
      "validation loss: 2.835754156112671\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8589048385620117\n",
      "step 132\n",
      "training loss: 2.8834762573242188\n",
      "step 133\n",
      "training loss: 2.8577401638031006\n",
      "step 134\n",
      "training loss: 2.8462421894073486\n",
      "step 135\n",
      "training loss: 2.8548672199249268\n",
      "step 136\n",
      "training loss: 2.8696067333221436\n",
      "step 137\n",
      "training loss: 2.851438522338867\n",
      "step 138\n",
      "training loss: 2.8829739093780518\n",
      "step 139\n",
      "training loss: 2.865884304046631\n",
      "step 140\n",
      "training loss: 2.82686448097229\n",
      "validation loss: 2.8608381748199463\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8568203449249268\n",
      "step 142\n",
      "training loss: 2.8575010299682617\n",
      "step 143\n",
      "training loss: 2.858565330505371\n",
      "step 144\n",
      "training loss: 2.8734686374664307\n",
      "step 145\n",
      "training loss: 2.8685295581817627\n",
      "step 146\n",
      "training loss: 2.8691439628601074\n",
      "step 147\n",
      "training loss: 2.825565814971924\n",
      "step 148\n",
      "training loss: 2.8641366958618164\n",
      "step 149\n",
      "training loss: 2.8583662509918213\n",
      "step 150\n",
      "training loss: 2.869690418243408\n",
      "validation loss: 2.864464044570923\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.869009017944336\n",
      "step 152\n",
      "training loss: 2.8789491653442383\n",
      "step 153\n",
      "training loss: 2.873105525970459\n",
      "step 154\n",
      "training loss: 2.853410243988037\n",
      "step 155\n",
      "training loss: 2.88148832321167\n",
      "step 156\n",
      "training loss: 2.875314235687256\n",
      "step 157\n",
      "training loss: 2.865600347518921\n",
      "step 158\n",
      "training loss: 2.8638062477111816\n",
      "step 159\n",
      "training loss: 2.871917963027954\n",
      "step 160\n",
      "training loss: 2.8621339797973633\n",
      "validation loss: 2.878038167953491\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.858243703842163\n",
      "step 162\n",
      "training loss: 2.8450803756713867\n",
      "step 163\n",
      "training loss: 2.8292183876037598\n",
      "step 164\n",
      "training loss: 2.8731794357299805\n",
      "step 165\n",
      "training loss: 2.852231025695801\n",
      "step 166\n",
      "training loss: 2.849705457687378\n",
      "step 167\n",
      "training loss: 2.867150068283081\n",
      "step 168\n",
      "training loss: 2.895108699798584\n",
      "step 169\n",
      "training loss: 2.847846269607544\n",
      "step 170\n",
      "training loss: 2.8744778633117676\n",
      "validation loss: 2.9061648845672607\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8663220405578613\n",
      "step 172\n",
      "training loss: 2.8734562397003174\n",
      "step 173\n",
      "training loss: 2.8514387607574463\n",
      "step 174\n",
      "training loss: 2.862478494644165\n",
      "step 175\n",
      "training loss: 2.839639902114868\n",
      "step 176\n",
      "training loss: 2.854374885559082\n",
      "step 177\n",
      "training loss: 2.8624825477600098\n",
      "step 178\n",
      "training loss: 2.847278594970703\n",
      "step 179\n",
      "training loss: 2.88957142829895\n",
      "step 180\n",
      "training loss: 2.8625638484954834\n",
      "validation loss: 2.869157075881958\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8542420864105225\n",
      "step 182\n",
      "training loss: 2.8677656650543213\n",
      "step 183\n",
      "training loss: 2.8704631328582764\n",
      "step 184\n",
      "training loss: 2.8592212200164795\n",
      "step 185\n",
      "training loss: 2.850640296936035\n",
      "step 186\n",
      "training loss: 2.833070993423462\n",
      "step 187\n",
      "training loss: 2.8242220878601074\n",
      "step 188\n",
      "training loss: 2.881277322769165\n",
      "step 189\n",
      "training loss: 2.857131004333496\n",
      "step 190\n",
      "training loss: 2.8853182792663574\n",
      "validation loss: 2.8677990436553955\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8607282638549805\n",
      "step 192\n",
      "training loss: 2.8709683418273926\n",
      "step 193\n",
      "training loss: 2.884509325027466\n",
      "step 194\n",
      "training loss: 2.863753318786621\n",
      "step 195\n",
      "training loss: 2.857337474822998\n",
      "step 196\n",
      "training loss: 2.870668888092041\n",
      "step 197\n",
      "training loss: 2.849578619003296\n",
      "step 198\n",
      "training loss: 2.855762243270874\n",
      "step 199\n",
      "training loss: 2.8486227989196777\n",
      "step 200\n",
      "training loss: 2.8546550273895264\n",
      "validation loss: 2.8309779167175293\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8435919284820557\n",
      "step 202\n",
      "training loss: 2.862379312515259\n",
      "step 203\n",
      "training loss: 2.864665985107422\n",
      "step 204\n",
      "training loss: 2.8475236892700195\n",
      "step 205\n",
      "training loss: 2.8755619525909424\n",
      "step 206\n",
      "training loss: 2.858452558517456\n",
      "step 207\n",
      "training loss: 2.8654091358184814\n",
      "step 208\n",
      "training loss: 2.8582329750061035\n",
      "step 209\n",
      "training loss: 2.869251012802124\n",
      "step 210\n",
      "training loss: 2.797076940536499\n",
      "validation loss: 2.832423448562622\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.852295160293579\n",
      "step 212\n",
      "training loss: 2.8391153812408447\n",
      "step 213\n",
      "training loss: 2.851396083831787\n",
      "step 214\n",
      "training loss: 2.874894618988037\n",
      "step 215\n",
      "training loss: 2.8770713806152344\n",
      "step 216\n",
      "training loss: 2.8646109104156494\n",
      "step 217\n",
      "training loss: 2.8648855686187744\n",
      "step 218\n",
      "training loss: 2.866875171661377\n",
      "step 219\n",
      "training loss: 2.852825403213501\n",
      "step 220\n",
      "training loss: 2.8596765995025635\n",
      "validation loss: 2.836291790008545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.873016595840454\n",
      "step 222\n",
      "training loss: 2.8886866569519043\n",
      "step 223\n",
      "training loss: 2.8605451583862305\n",
      "step 224\n",
      "training loss: 2.8494884967803955\n",
      "step 225\n",
      "training loss: 2.849076271057129\n",
      "step 226\n",
      "training loss: 2.850263833999634\n",
      "step 227\n",
      "training loss: 2.8631129264831543\n",
      "step 228\n",
      "training loss: 2.8708839416503906\n",
      "step 229\n",
      "training loss: 2.859440803527832\n",
      "step 230\n",
      "training loss: 2.8701741695404053\n",
      "validation loss: 2.8600194454193115\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8766047954559326\n",
      "step 232\n",
      "training loss: 2.8528780937194824\n",
      "step 233\n",
      "training loss: 2.8744068145751953\n",
      "step 234\n",
      "training loss: 2.8801419734954834\n",
      "step 235\n",
      "training loss: 2.8501930236816406\n",
      "step 236\n",
      "training loss: 2.8520689010620117\n",
      "step 237\n",
      "training loss: 2.8765742778778076\n",
      "step 238\n",
      "training loss: 2.865762948989868\n",
      "step 239\n",
      "training loss: 2.868450164794922\n",
      "step 240\n",
      "training loss: 2.8700549602508545\n",
      "validation loss: 2.8671011924743652\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8714592456817627\n",
      "step 242\n",
      "training loss: 2.8643758296966553\n",
      "step 243\n",
      "training loss: 2.8654067516326904\n",
      "step 244\n",
      "training loss: 2.8640589714050293\n",
      "step 245\n",
      "training loss: 2.8636748790740967\n",
      "step 246\n",
      "training loss: 2.860548734664917\n",
      "step 247\n",
      "training loss: 2.8278822898864746\n",
      "step 248\n",
      "training loss: 2.8555455207824707\n",
      "step 249\n",
      "training loss: 2.87644100189209\n",
      "step 250\n",
      "training loss: 2.855895757675171\n",
      "validation loss: 2.8675179481506348\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.861970901489258\n",
      "step 252\n",
      "training loss: 2.891623020172119\n",
      "step 253\n",
      "training loss: 2.8713290691375732\n",
      "step 254\n",
      "training loss: 2.8851656913757324\n",
      "step 255\n",
      "training loss: 2.882908821105957\n",
      "step 256\n",
      "training loss: 2.8740153312683105\n",
      "step 257\n",
      "training loss: 2.857048988342285\n",
      "step 258\n",
      "training loss: 2.87842059135437\n",
      "step 259\n",
      "training loss: 2.881031036376953\n",
      "step 260\n",
      "training loss: 2.8717849254608154\n",
      "validation loss: 2.879969596862793\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.870391845703125\n",
      "step 262\n",
      "training loss: 2.880220890045166\n",
      "step 263\n",
      "training loss: 2.8919408321380615\n",
      "step 264\n",
      "training loss: 2.814568281173706\n",
      "step 265\n",
      "training loss: 2.8532609939575195\n",
      "step 266\n",
      "training loss: 2.816758632659912\n",
      "step 267\n",
      "training loss: 2.8518967628479004\n",
      "step 268\n",
      "training loss: 2.888258934020996\n",
      "step 269\n",
      "training loss: 2.883100748062134\n",
      "step 270\n",
      "training loss: 2.8865158557891846\n",
      "validation loss: 2.866802215576172\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.876614570617676\n",
      "----------3.0 min per epoch----------\n",
      "epoch 2\n",
      "step 0\n",
      "training loss: 2.8061728477478027\n",
      "validation loss: 2.8502893447875977\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.7590460777282715\n",
      "step 2\n",
      "training loss: 2.8782639503479004\n",
      "step 3\n",
      "training loss: 2.8861544132232666\n",
      "step 4\n",
      "training loss: 2.877859115600586\n",
      "step 5\n",
      "training loss: 2.8587207794189453\n",
      "step 6\n",
      "training loss: 2.8637728691101074\n",
      "step 7\n",
      "training loss: 2.8783645629882812\n",
      "step 8\n",
      "training loss: 2.876922369003296\n",
      "step 9\n",
      "training loss: 2.8817715644836426\n",
      "step 10\n",
      "training loss: 2.870187282562256\n",
      "validation loss: 2.860102891921997\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.884552001953125\n",
      "step 12\n",
      "training loss: 2.8809454441070557\n",
      "step 13\n",
      "training loss: 2.8443760871887207\n",
      "step 14\n",
      "training loss: 2.8505773544311523\n",
      "step 15\n",
      "training loss: 2.8648693561553955\n",
      "step 16\n",
      "training loss: 2.8688974380493164\n",
      "step 17\n",
      "training loss: 2.8721933364868164\n",
      "step 18\n",
      "training loss: 2.87341570854187\n",
      "step 19\n",
      "training loss: 2.8786461353302\n",
      "step 20\n",
      "training loss: 2.879448413848877\n",
      "validation loss: 2.84382963180542\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.872941732406616\n",
      "step 22\n",
      "training loss: 2.8901455402374268\n",
      "step 23\n",
      "training loss: 2.8697829246520996\n",
      "step 24\n",
      "training loss: 2.877027750015259\n",
      "step 25\n",
      "training loss: 2.88598370552063\n",
      "step 26\n",
      "training loss: 2.8808717727661133\n",
      "step 27\n",
      "training loss: 2.879760265350342\n",
      "step 28\n",
      "training loss: 2.879897117614746\n",
      "step 29\n",
      "training loss: 2.8644282817840576\n",
      "step 30\n",
      "training loss: 2.847787618637085\n",
      "validation loss: 2.8480985164642334\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.867333173751831\n",
      "step 32\n",
      "training loss: 2.880927801132202\n",
      "step 33\n",
      "training loss: 2.8458805084228516\n",
      "step 34\n",
      "training loss: 2.865088701248169\n",
      "step 35\n",
      "training loss: 2.869968891143799\n",
      "step 36\n",
      "training loss: 2.8592283725738525\n",
      "step 37\n",
      "training loss: 2.8850717544555664\n",
      "step 38\n",
      "training loss: 2.862316846847534\n",
      "step 39\n",
      "training loss: 2.8754234313964844\n",
      "step 40\n",
      "training loss: 2.860189437866211\n",
      "validation loss: 2.875180721282959\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8760361671447754\n",
      "step 42\n",
      "training loss: 2.8290512561798096\n",
      "step 43\n",
      "training loss: 2.8860011100769043\n",
      "step 44\n",
      "training loss: 2.8528075218200684\n",
      "step 45\n",
      "training loss: 2.8871140480041504\n",
      "step 46\n",
      "training loss: 2.8734264373779297\n",
      "step 47\n",
      "training loss: 2.852151870727539\n",
      "step 48\n",
      "training loss: 2.8473494052886963\n",
      "step 49\n",
      "training loss: 2.870394468307495\n",
      "step 50\n",
      "training loss: 2.8579156398773193\n",
      "validation loss: 2.879265069961548\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8532443046569824\n",
      "step 52\n",
      "training loss: 2.8837084770202637\n",
      "step 53\n",
      "training loss: 2.8746650218963623\n",
      "step 54\n",
      "training loss: 2.8636369705200195\n",
      "step 55\n",
      "training loss: 2.8498740196228027\n",
      "step 56\n",
      "training loss: 2.8635520935058594\n",
      "step 57\n",
      "training loss: 2.864706039428711\n",
      "step 58\n",
      "training loss: 2.8413631916046143\n",
      "step 59\n",
      "training loss: 2.856013059616089\n",
      "step 60\n",
      "training loss: 2.8557093143463135\n",
      "validation loss: 2.832679271697998\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8505215644836426\n",
      "step 62\n",
      "training loss: 2.8385531902313232\n",
      "step 63\n",
      "training loss: 2.8766512870788574\n",
      "step 64\n",
      "training loss: 2.8550734519958496\n",
      "step 65\n",
      "training loss: 2.8855628967285156\n",
      "step 66\n",
      "training loss: 2.8661394119262695\n",
      "step 67\n",
      "training loss: 2.8439855575561523\n",
      "step 68\n",
      "training loss: 2.8817853927612305\n",
      "step 69\n",
      "training loss: 2.8590619564056396\n",
      "step 70\n",
      "training loss: 2.8654072284698486\n",
      "validation loss: 2.857462167739868\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.865412712097168\n",
      "step 72\n",
      "training loss: 2.840449094772339\n",
      "step 73\n",
      "training loss: 2.881474494934082\n",
      "step 74\n",
      "training loss: 2.8624048233032227\n",
      "step 75\n",
      "training loss: 2.880941867828369\n",
      "step 76\n",
      "training loss: 2.875545024871826\n",
      "step 77\n",
      "training loss: 2.8954029083251953\n",
      "step 78\n",
      "training loss: 2.8823604583740234\n",
      "step 79\n",
      "training loss: 2.8410274982452393\n",
      "step 80\n",
      "training loss: 2.879162311553955\n",
      "validation loss: 2.898362398147583\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.863751173019409\n",
      "step 82\n",
      "training loss: 2.853361129760742\n",
      "step 83\n",
      "training loss: 2.8568620681762695\n",
      "step 84\n",
      "training loss: 2.8648083209991455\n",
      "step 85\n",
      "training loss: 2.8814756870269775\n",
      "step 86\n",
      "training loss: 2.864959239959717\n",
      "step 87\n",
      "training loss: 2.8836395740509033\n",
      "step 88\n",
      "training loss: 2.881061553955078\n",
      "step 89\n",
      "training loss: 2.883877992630005\n",
      "step 90\n",
      "training loss: 2.8569395542144775\n",
      "validation loss: 2.883873462677002\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.868624448776245\n",
      "step 92\n",
      "training loss: 2.86712908744812\n",
      "step 93\n",
      "training loss: 2.8583924770355225\n",
      "step 94\n",
      "training loss: 2.8691532611846924\n",
      "step 95\n",
      "training loss: 2.845623254776001\n",
      "step 96\n",
      "training loss: 2.871464729309082\n",
      "step 97\n",
      "training loss: 2.876157760620117\n",
      "step 98\n",
      "training loss: 2.82411789894104\n",
      "step 99\n",
      "training loss: 2.861518621444702\n",
      "step 100\n",
      "training loss: 2.8530681133270264\n",
      "validation loss: 2.8739819526672363\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8676259517669678\n",
      "step 102\n",
      "training loss: 2.875310182571411\n",
      "step 103\n",
      "training loss: 2.8498144149780273\n",
      "step 104\n",
      "training loss: 2.8695836067199707\n",
      "step 105\n",
      "training loss: 2.8297297954559326\n",
      "step 106\n",
      "training loss: 2.856764078140259\n",
      "step 107\n",
      "training loss: 2.8777451515197754\n",
      "step 108\n",
      "training loss: 2.8685688972473145\n",
      "step 109\n",
      "training loss: 2.860807180404663\n",
      "step 110\n",
      "training loss: 2.8499653339385986\n",
      "validation loss: 2.9424262046813965\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.869288921356201\n",
      "step 112\n",
      "training loss: 2.8559887409210205\n",
      "step 113\n",
      "training loss: 2.8661882877349854\n",
      "step 114\n",
      "training loss: 2.8740580081939697\n",
      "step 115\n",
      "training loss: 2.865852117538452\n",
      "step 116\n",
      "training loss: 2.8596975803375244\n",
      "step 117\n",
      "training loss: 2.8622806072235107\n",
      "step 118\n",
      "training loss: 2.838137626647949\n",
      "step 119\n",
      "training loss: 2.8439855575561523\n",
      "step 120\n",
      "training loss: 2.835192918777466\n",
      "validation loss: 2.879581928253174\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8676905632019043\n",
      "step 122\n",
      "training loss: 2.8725428581237793\n",
      "step 123\n",
      "training loss: 2.8646621704101562\n",
      "step 124\n",
      "training loss: 2.8615670204162598\n",
      "step 125\n",
      "training loss: 2.853823184967041\n",
      "step 126\n",
      "training loss: 2.874006986618042\n",
      "step 127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.892695665359497\n",
      "step 128\n",
      "training loss: 2.812361240386963\n",
      "step 129\n",
      "training loss: 2.864100456237793\n",
      "step 130\n",
      "training loss: 2.856313705444336\n",
      "validation loss: 2.852213144302368\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.880898952484131\n",
      "step 132\n",
      "training loss: 2.8603715896606445\n",
      "step 133\n",
      "training loss: 2.884674549102783\n",
      "step 134\n",
      "training loss: 2.856658697128296\n",
      "step 135\n",
      "training loss: 2.8428714275360107\n",
      "step 136\n",
      "training loss: 2.8543310165405273\n",
      "step 137\n",
      "training loss: 2.8710803985595703\n",
      "step 138\n",
      "training loss: 2.8517658710479736\n",
      "step 139\n",
      "training loss: 2.882544755935669\n",
      "step 140\n",
      "training loss: 2.868260145187378\n",
      "validation loss: 2.868175506591797\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8277041912078857\n",
      "step 142\n",
      "training loss: 2.8546416759490967\n",
      "step 143\n",
      "training loss: 2.8589553833007812\n",
      "step 144\n",
      "training loss: 2.85919451713562\n",
      "step 145\n",
      "training loss: 2.871857166290283\n",
      "step 146\n",
      "training loss: 2.8695826530456543\n",
      "step 147\n",
      "training loss: 2.8712940216064453\n",
      "step 148\n",
      "training loss: 2.8235838413238525\n",
      "step 149\n",
      "training loss: 2.868049383163452\n",
      "step 150\n",
      "training loss: 2.8597333431243896\n",
      "validation loss: 2.890014410018921\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.868302345275879\n",
      "step 152\n",
      "training loss: 2.8674116134643555\n",
      "step 153\n",
      "training loss: 2.879408836364746\n",
      "step 154\n",
      "training loss: 2.874089479446411\n",
      "step 155\n",
      "training loss: 2.854039192199707\n",
      "step 156\n",
      "training loss: 2.881659746170044\n",
      "step 157\n",
      "training loss: 2.8755061626434326\n",
      "step 158\n",
      "training loss: 2.86725115776062\n",
      "step 159\n",
      "training loss: 2.860626220703125\n",
      "step 160\n",
      "training loss: 2.8725321292877197\n",
      "validation loss: 2.841214179992676\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.861132860183716\n",
      "step 162\n",
      "training loss: 2.8616878986358643\n",
      "step 163\n",
      "training loss: 2.847200632095337\n",
      "step 164\n",
      "training loss: 2.830549955368042\n",
      "step 165\n",
      "training loss: 2.8733813762664795\n",
      "step 166\n",
      "training loss: 2.8520779609680176\n",
      "step 167\n",
      "training loss: 2.8513009548187256\n",
      "step 168\n",
      "training loss: 2.867806911468506\n",
      "step 169\n",
      "training loss: 2.8990206718444824\n",
      "step 170\n",
      "training loss: 2.848597288131714\n",
      "validation loss: 2.8627710342407227\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.875771999359131\n",
      "step 172\n",
      "training loss: 2.8681578636169434\n",
      "step 173\n",
      "training loss: 2.8747401237487793\n",
      "step 174\n",
      "training loss: 2.850679397583008\n",
      "step 175\n",
      "training loss: 2.862178325653076\n",
      "step 176\n",
      "training loss: 2.840208053588867\n",
      "step 177\n",
      "training loss: 2.8537535667419434\n",
      "step 178\n",
      "training loss: 2.860402822494507\n",
      "step 179\n",
      "training loss: 2.8496832847595215\n",
      "step 180\n",
      "training loss: 2.888367176055908\n",
      "validation loss: 2.865225315093994\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8621084690093994\n",
      "step 182\n",
      "training loss: 2.8507773876190186\n",
      "step 183\n",
      "training loss: 2.8686015605926514\n",
      "step 184\n",
      "training loss: 2.871143102645874\n",
      "step 185\n",
      "training loss: 2.8586599826812744\n",
      "step 186\n",
      "training loss: 2.848632574081421\n",
      "step 187\n",
      "training loss: 2.836174488067627\n",
      "step 188\n",
      "training loss: 2.826467275619507\n",
      "step 189\n",
      "training loss: 2.8817734718322754\n",
      "step 190\n",
      "training loss: 2.8553972244262695\n",
      "validation loss: 2.8838870525360107\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8856003284454346\n",
      "step 192\n",
      "training loss: 2.861036539077759\n",
      "step 193\n",
      "training loss: 2.867384195327759\n",
      "step 194\n",
      "training loss: 2.883997678756714\n",
      "step 195\n",
      "training loss: 2.861198663711548\n",
      "step 196\n",
      "training loss: 2.8572731018066406\n",
      "step 197\n",
      "training loss: 2.869574785232544\n",
      "step 198\n",
      "training loss: 2.847229242324829\n",
      "step 199\n",
      "training loss: 2.854383707046509\n",
      "step 200\n",
      "training loss: 2.851320505142212\n",
      "validation loss: 2.9178335666656494\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.854483127593994\n",
      "step 202\n",
      "training loss: 2.842240333557129\n",
      "step 203\n",
      "training loss: 2.863304853439331\n",
      "step 204\n",
      "training loss: 2.866544723510742\n",
      "step 205\n",
      "training loss: 2.8480072021484375\n",
      "step 206\n",
      "training loss: 2.873987913131714\n",
      "step 207\n",
      "training loss: 2.853788137435913\n",
      "step 208\n",
      "training loss: 2.8661117553710938\n",
      "step 209\n",
      "training loss: 2.8591489791870117\n",
      "step 210\n",
      "training loss: 2.871263027191162\n",
      "validation loss: 2.8685529232025146\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.79947829246521\n",
      "step 212\n",
      "training loss: 2.8531882762908936\n",
      "step 213\n",
      "training loss: 2.8378963470458984\n",
      "step 214\n",
      "training loss: 2.853717565536499\n",
      "step 215\n",
      "training loss: 2.876390218734741\n",
      "step 216\n",
      "training loss: 2.8758018016815186\n",
      "step 217\n",
      "training loss: 2.866420030593872\n",
      "step 218\n",
      "training loss: 2.8651766777038574\n",
      "step 219\n",
      "training loss: 2.8647754192352295\n",
      "step 220\n",
      "training loss: 2.850208044052124\n",
      "validation loss: 2.863924503326416\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.857360363006592\n",
      "step 222\n",
      "training loss: 2.8705081939697266\n",
      "step 223\n",
      "training loss: 2.8887906074523926\n",
      "step 224\n",
      "training loss: 2.8586153984069824\n",
      "step 225\n",
      "training loss: 2.8470635414123535\n",
      "step 226\n",
      "training loss: 2.8499746322631836\n",
      "step 227\n",
      "training loss: 2.851627826690674\n",
      "step 228\n",
      "training loss: 2.8630053997039795\n",
      "step 229\n",
      "training loss: 2.870490789413452\n",
      "step 230\n",
      "training loss: 2.8593146800994873\n",
      "validation loss: 2.827890396118164\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.869389295578003\n",
      "step 232\n",
      "training loss: 2.8766143321990967\n",
      "step 233\n",
      "training loss: 2.851395845413208\n",
      "step 234\n",
      "training loss: 2.874854564666748\n",
      "step 235\n",
      "training loss: 2.8800909519195557\n",
      "step 236\n",
      "training loss: 2.8463034629821777\n",
      "step 237\n",
      "training loss: 2.8500072956085205\n",
      "step 238\n",
      "training loss: 2.8740179538726807\n",
      "step 239\n",
      "training loss: 2.865818738937378\n",
      "step 240\n",
      "training loss: 2.86476469039917\n",
      "validation loss: 2.833956241607666\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.868004322052002\n",
      "step 242\n",
      "training loss: 2.8705954551696777\n",
      "step 243\n",
      "training loss: 2.863138198852539\n",
      "step 244\n",
      "training loss: 2.8663575649261475\n",
      "step 245\n",
      "training loss: 2.86650013923645\n",
      "step 246\n",
      "training loss: 2.860657215118408\n",
      "step 247\n",
      "training loss: 2.859398603439331\n",
      "step 248\n",
      "training loss: 2.8274080753326416\n",
      "step 249\n",
      "training loss: 2.8565797805786133\n",
      "step 250\n",
      "training loss: 2.8724441528320312\n",
      "validation loss: 2.827578544616699\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8504741191864014\n",
      "step 252\n",
      "training loss: 2.8561601638793945\n",
      "step 253\n",
      "training loss: 2.8909854888916016\n",
      "step 254\n",
      "training loss: 2.8704373836517334\n",
      "step 255\n",
      "training loss: 2.8844616413116455\n",
      "step 256\n",
      "training loss: 2.884242057800293\n",
      "step 257\n",
      "training loss: 2.8706865310668945\n",
      "step 258\n",
      "training loss: 2.8549764156341553\n",
      "step 259\n",
      "training loss: 2.877230167388916\n",
      "step 260\n",
      "training loss: 2.880491018295288\n",
      "validation loss: 2.8767263889312744\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8717598915100098\n",
      "step 262\n",
      "training loss: 2.8688056468963623\n",
      "step 263\n",
      "training loss: 2.8811583518981934\n",
      "step 264\n",
      "training loss: 2.8872921466827393\n",
      "step 265\n",
      "training loss: 2.8101773262023926\n",
      "step 266\n",
      "training loss: 2.8536527156829834\n",
      "step 267\n",
      "training loss: 2.815087080001831\n",
      "step 268\n",
      "training loss: 2.8545713424682617\n",
      "step 269\n",
      "training loss: 2.891523838043213\n",
      "step 270\n",
      "training loss: 2.8802239894866943\n",
      "validation loss: 2.8728647232055664\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.886781930923462\n",
      "----------3.0 min per epoch----------\n",
      "epoch 3\n",
      "step 0\n",
      "training loss: 2.8761448860168457\n",
      "validation loss: 2.8737640380859375\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8040707111358643\n",
      "step 2\n",
      "training loss: 2.7601699829101562\n",
      "step 3\n",
      "training loss: 2.877243995666504\n",
      "step 4\n",
      "training loss: 2.8870787620544434\n",
      "step 5\n",
      "training loss: 2.8695545196533203\n",
      "step 6\n",
      "training loss: 2.8535666465759277\n",
      "step 7\n",
      "training loss: 2.8642518520355225\n",
      "step 8\n",
      "training loss: 2.878145456314087\n",
      "step 9\n",
      "training loss: 2.876934051513672\n",
      "step 10\n",
      "training loss: 2.8810651302337646\n",
      "validation loss: 2.8688883781433105\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8671345710754395\n",
      "step 12\n",
      "training loss: 2.8806300163269043\n",
      "step 13\n",
      "training loss: 2.879619598388672\n",
      "step 14\n",
      "training loss: 2.842130422592163\n",
      "step 15\n",
      "training loss: 2.846240997314453\n",
      "step 16\n",
      "training loss: 2.862825393676758\n",
      "step 17\n",
      "training loss: 2.865628957748413\n",
      "step 18\n",
      "training loss: 2.8750014305114746\n",
      "step 19\n",
      "training loss: 2.870420455932617\n",
      "step 20\n",
      "training loss: 2.8741230964660645\n",
      "validation loss: 2.8691186904907227\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8747196197509766\n",
      "step 22\n",
      "training loss: 2.8738818168640137\n",
      "step 23\n",
      "training loss: 2.8866724967956543\n",
      "step 24\n",
      "training loss: 2.8671531677246094\n",
      "step 25\n",
      "training loss: 2.877380132675171\n",
      "step 26\n",
      "training loss: 2.8861358165740967\n",
      "step 27\n",
      "training loss: 2.8813419342041016\n",
      "step 28\n",
      "training loss: 2.875819444656372\n",
      "step 29\n",
      "training loss: 2.877717971801758\n",
      "step 30\n",
      "training loss: 2.859591007232666\n",
      "validation loss: 2.8493006229400635\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.846294403076172\n",
      "step 32\n",
      "training loss: 2.8670737743377686\n",
      "step 33\n",
      "training loss: 2.8787028789520264\n",
      "step 34\n",
      "training loss: 2.8431503772735596\n",
      "step 35\n",
      "training loss: 2.866363763809204\n",
      "step 36\n",
      "training loss: 2.8715758323669434\n",
      "step 37\n",
      "training loss: 2.858286142349243\n",
      "step 38\n",
      "training loss: 2.884955406188965\n",
      "step 39\n",
      "training loss: 2.8578073978424072\n",
      "step 40\n",
      "training loss: 2.8765294551849365\n",
      "validation loss: 2.8487048149108887\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8563201427459717\n",
      "step 42\n",
      "training loss: 2.8753085136413574\n",
      "step 43\n",
      "training loss: 2.8270626068115234\n",
      "step 44\n",
      "training loss: 2.88307785987854\n",
      "step 45\n",
      "training loss: 2.8527190685272217\n",
      "step 46\n",
      "training loss: 2.8855347633361816\n",
      "step 47\n",
      "training loss: 2.8733010292053223\n",
      "step 48\n",
      "training loss: 2.853987216949463\n",
      "step 49\n",
      "training loss: 2.847503185272217\n",
      "step 50\n",
      "training loss: 2.8653411865234375\n",
      "validation loss: 2.83548641204834\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8569319248199463\n",
      "step 52\n",
      "training loss: 2.854813575744629\n",
      "step 53\n",
      "training loss: 2.8851985931396484\n",
      "step 54\n",
      "training loss: 2.8733837604522705\n",
      "step 55\n",
      "training loss: 2.862013339996338\n",
      "step 56\n",
      "training loss: 2.8468284606933594\n",
      "step 57\n",
      "training loss: 2.862508535385132\n",
      "step 58\n",
      "training loss: 2.8635854721069336\n",
      "step 59\n",
      "training loss: 2.839081048965454\n",
      "step 60\n",
      "training loss: 2.854405164718628\n",
      "validation loss: 2.8425230979919434\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.854567766189575\n",
      "step 62\n",
      "training loss: 2.8483729362487793\n",
      "step 63\n",
      "training loss: 2.838792562484741\n",
      "step 64\n",
      "training loss: 2.8783960342407227\n",
      "step 65\n",
      "training loss: 2.8526127338409424\n",
      "step 66\n",
      "training loss: 2.884706497192383\n",
      "step 67\n",
      "training loss: 2.8660025596618652\n",
      "step 68\n",
      "training loss: 2.844047784805298\n",
      "step 69\n",
      "training loss: 2.8787295818328857\n",
      "step 70\n",
      "training loss: 2.856445789337158\n",
      "validation loss: 2.8691582679748535\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.866924285888672\n",
      "step 72\n",
      "training loss: 2.868231773376465\n",
      "step 73\n",
      "training loss: 2.8388237953186035\n",
      "step 74\n",
      "training loss: 2.878603219985962\n",
      "step 75\n",
      "training loss: 2.86289644241333\n",
      "step 76\n",
      "training loss: 2.8790841102600098\n",
      "step 77\n",
      "training loss: 2.8744382858276367\n",
      "step 78\n",
      "training loss: 2.895418643951416\n",
      "step 79\n",
      "training loss: 2.8809914588928223\n",
      "step 80\n",
      "training loss: 2.8436527252197266\n",
      "validation loss: 2.878678798675537\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8794753551483154\n",
      "step 82\n",
      "training loss: 2.861660957336426\n",
      "step 83\n",
      "training loss: 2.8509998321533203\n",
      "step 84\n",
      "training loss: 2.8588311672210693\n",
      "step 85\n",
      "training loss: 2.8607990741729736\n",
      "step 86\n",
      "training loss: 2.87949538230896\n",
      "step 87\n",
      "training loss: 2.8657643795013428\n",
      "step 88\n",
      "training loss: 2.8823530673980713\n",
      "step 89\n",
      "training loss: 2.880040168762207\n",
      "step 90\n",
      "training loss: 2.883971691131592\n",
      "validation loss: 2.8333821296691895\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8570923805236816\n",
      "step 92\n",
      "training loss: 2.872068166732788\n",
      "step 93\n",
      "training loss: 2.8667805194854736\n",
      "step 94\n",
      "training loss: 2.859391212463379\n",
      "step 95\n",
      "training loss: 2.8693408966064453\n",
      "step 96\n",
      "training loss: 2.8473448753356934\n",
      "step 97\n",
      "training loss: 2.872523784637451\n",
      "step 98\n",
      "training loss: 2.8747994899749756\n",
      "step 99\n",
      "training loss: 2.8217811584472656\n",
      "step 100\n",
      "training loss: 2.8619110584259033\n",
      "validation loss: 2.8553125858306885\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.85506272315979\n",
      "step 102\n",
      "training loss: 2.8693931102752686\n",
      "step 103\n",
      "training loss: 2.873262882232666\n",
      "step 104\n",
      "training loss: 2.849311113357544\n",
      "step 105\n",
      "training loss: 2.8699042797088623\n",
      "step 106\n",
      "training loss: 2.8255045413970947\n",
      "step 107\n",
      "training loss: 2.8570141792297363\n",
      "step 108\n",
      "training loss: 2.8801891803741455\n",
      "step 109\n",
      "training loss: 2.869953155517578\n",
      "step 110\n",
      "training loss: 2.860257863998413\n",
      "validation loss: 2.892209768295288\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.852067470550537\n",
      "step 112\n",
      "training loss: 2.8699264526367188\n",
      "step 113\n",
      "training loss: 2.856165647506714\n",
      "step 114\n",
      "training loss: 2.866257429122925\n",
      "step 115\n",
      "training loss: 2.874589681625366\n",
      "step 116\n",
      "training loss: 2.8648881912231445\n",
      "step 117\n",
      "training loss: 2.863693952560425\n",
      "step 118\n",
      "training loss: 2.8622334003448486\n",
      "step 119\n",
      "training loss: 2.8424816131591797\n",
      "step 120\n",
      "training loss: 2.843649387359619\n",
      "validation loss: 2.8814001083374023\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.834967613220215\n",
      "step 122\n",
      "training loss: 2.8689661026000977\n",
      "step 123\n",
      "training loss: 2.8729631900787354\n",
      "step 124\n",
      "training loss: 2.8655056953430176\n",
      "step 125\n",
      "training loss: 2.862469434738159\n",
      "step 126\n",
      "training loss: 2.853424072265625\n",
      "step 127\n",
      "training loss: 2.877809762954712\n",
      "step 128\n",
      "training loss: 2.8961594104766846\n",
      "step 129\n",
      "training loss: 2.8141582012176514\n",
      "step 130\n",
      "training loss: 2.8647620677948\n",
      "validation loss: 2.8716769218444824\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8569276332855225\n",
      "step 132\n",
      "training loss: 2.882716655731201\n",
      "step 133\n",
      "training loss: 2.8610177040100098\n",
      "step 134\n",
      "training loss: 2.884603500366211\n",
      "step 135\n",
      "training loss: 2.858140707015991\n",
      "step 136\n",
      "training loss: 2.8444747924804688\n",
      "step 137\n",
      "training loss: 2.8551623821258545\n",
      "step 138\n",
      "training loss: 2.8696155548095703\n",
      "step 139\n",
      "training loss: 2.8508591651916504\n",
      "step 140\n",
      "training loss: 2.8827321529388428\n",
      "validation loss: 2.9399282932281494\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.86795973777771\n",
      "step 142\n",
      "training loss: 2.8262977600097656\n",
      "step 143\n",
      "training loss: 2.855722188949585\n",
      "step 144\n",
      "training loss: 2.859104871749878\n",
      "step 145\n",
      "training loss: 2.85902738571167\n",
      "step 146\n",
      "training loss: 2.872873544692993\n",
      "step 147\n",
      "training loss: 2.8691136837005615\n",
      "step 148\n",
      "training loss: 2.869554042816162\n",
      "step 149\n",
      "training loss: 2.830491065979004\n",
      "step 150\n",
      "training loss: 2.865969657897949\n",
      "validation loss: 2.88081431388855\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8599369525909424\n",
      "step 152\n",
      "training loss: 2.8693482875823975\n",
      "step 153\n",
      "training loss: 2.867950916290283\n",
      "step 154\n",
      "training loss: 2.8789455890655518\n",
      "step 155\n",
      "training loss: 2.8746204376220703\n",
      "step 156\n",
      "training loss: 2.857485294342041\n",
      "step 157\n",
      "training loss: 2.881824016571045\n",
      "step 158\n",
      "training loss: 2.875161647796631\n",
      "step 159\n",
      "training loss: 2.865419864654541\n",
      "step 160\n",
      "training loss: 2.8628010749816895\n",
      "validation loss: 2.85408878326416\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8733835220336914\n",
      "step 162\n",
      "training loss: 2.860888957977295\n",
      "step 163\n",
      "training loss: 2.860884189605713\n",
      "step 164\n",
      "training loss: 2.848459243774414\n",
      "step 165\n",
      "training loss: 2.827923059463501\n",
      "step 166\n",
      "training loss: 2.8728978633880615\n",
      "step 167\n",
      "training loss: 2.8513379096984863\n",
      "step 168\n",
      "training loss: 2.8520493507385254\n",
      "step 169\n",
      "training loss: 2.8641602993011475\n",
      "step 170\n",
      "training loss: 2.8965868949890137\n",
      "validation loss: 2.8709042072296143\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8493196964263916\n",
      "step 172\n",
      "training loss: 2.8740594387054443\n",
      "step 173\n",
      "training loss: 2.8661324977874756\n",
      "step 174\n",
      "training loss: 2.873577117919922\n",
      "step 175\n",
      "training loss: 2.8516924381256104\n",
      "step 176\n",
      "training loss: 2.862015962600708\n",
      "step 177\n",
      "training loss: 2.838386058807373\n",
      "step 178\n",
      "training loss: 2.8553528785705566\n",
      "step 179\n",
      "training loss: 2.8629324436187744\n",
      "step 180\n",
      "training loss: 2.8493494987487793\n",
      "validation loss: 2.8999178409576416\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8858673572540283\n",
      "step 182\n",
      "training loss: 2.863875389099121\n",
      "step 183\n",
      "training loss: 2.8547637462615967\n",
      "step 184\n",
      "training loss: 2.866866111755371\n",
      "step 185\n",
      "training loss: 2.8693976402282715\n",
      "step 186\n",
      "training loss: 2.8577466011047363\n",
      "step 187\n",
      "training loss: 2.8505680561065674\n",
      "step 188\n",
      "training loss: 2.8356120586395264\n",
      "step 189\n",
      "training loss: 2.8260035514831543\n",
      "step 190\n",
      "training loss: 2.8805530071258545\n",
      "validation loss: 2.841420888900757\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.85819673538208\n",
      "step 192\n",
      "training loss: 2.885554075241089\n",
      "step 193\n",
      "training loss: 2.862257957458496\n",
      "step 194\n",
      "training loss: 2.8728606700897217\n",
      "step 195\n",
      "training loss: 2.8820619583129883\n",
      "step 196\n",
      "training loss: 2.860199213027954\n",
      "step 197\n",
      "training loss: 2.8581786155700684\n",
      "step 198\n",
      "training loss: 2.8688337802886963\n",
      "step 199\n",
      "training loss: 2.84708833694458\n",
      "step 200\n",
      "training loss: 2.8557472229003906\n",
      "validation loss: 2.8601441383361816\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8487300872802734\n",
      "step 202\n",
      "training loss: 2.852926254272461\n",
      "step 203\n",
      "training loss: 2.8422319889068604\n",
      "step 204\n",
      "training loss: 2.862717628479004\n",
      "step 205\n",
      "training loss: 2.864762544631958\n",
      "step 206\n",
      "training loss: 2.8481600284576416\n",
      "step 207\n",
      "training loss: 2.875333070755005\n",
      "step 208\n",
      "training loss: 2.855764389038086\n",
      "step 209\n",
      "training loss: 2.866276264190674\n",
      "step 210\n",
      "training loss: 2.860015869140625\n",
      "validation loss: 2.864232301712036\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8700196743011475\n",
      "step 212\n",
      "training loss: 2.7956950664520264\n",
      "step 213\n",
      "training loss: 2.852508783340454\n",
      "step 214\n",
      "training loss: 2.8389596939086914\n",
      "step 215\n",
      "training loss: 2.8528056144714355\n",
      "step 216\n",
      "training loss: 2.8735554218292236\n",
      "step 217\n",
      "training loss: 2.875354766845703\n",
      "step 218\n",
      "training loss: 2.867292881011963\n",
      "step 219\n",
      "training loss: 2.8668930530548096\n",
      "step 220\n",
      "training loss: 2.8659889698028564\n",
      "validation loss: 2.874102830886841\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8501880168914795\n",
      "step 222\n",
      "training loss: 2.85854434967041\n",
      "step 223\n",
      "training loss: 2.8705499172210693\n",
      "step 224\n",
      "training loss: 2.889403820037842\n",
      "step 225\n",
      "training loss: 2.860698699951172\n",
      "step 226\n",
      "training loss: 2.846381187438965\n",
      "step 227\n",
      "training loss: 2.849759101867676\n",
      "step 228\n",
      "training loss: 2.852156639099121\n",
      "step 229\n",
      "training loss: 2.863955497741699\n",
      "step 230\n",
      "training loss: 2.8687188625335693\n",
      "validation loss: 2.907012939453125\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8578600883483887\n",
      "step 232\n",
      "training loss: 2.870948314666748\n",
      "step 233\n",
      "training loss: 2.8775672912597656\n",
      "step 234\n",
      "training loss: 2.8523082733154297\n",
      "step 235\n",
      "training loss: 2.875239849090576\n",
      "step 236\n",
      "training loss: 2.879343271255493\n",
      "step 237\n",
      "training loss: 2.8492696285247803\n",
      "step 238\n",
      "training loss: 2.8495023250579834\n",
      "step 239\n",
      "training loss: 2.873591899871826\n",
      "step 240\n",
      "training loss: 2.8679487705230713\n",
      "validation loss: 2.8667802810668945\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8679420948028564\n",
      "step 242\n",
      "training loss: 2.867203712463379\n",
      "step 243\n",
      "training loss: 2.870439291000366\n",
      "step 244\n",
      "training loss: 2.8638648986816406\n",
      "step 245\n",
      "training loss: 2.8664543628692627\n",
      "step 246\n",
      "training loss: 2.8642895221710205\n",
      "step 247\n",
      "training loss: 2.861135959625244\n",
      "step 248\n",
      "training loss: 2.8592960834503174\n",
      "step 249\n",
      "training loss: 2.828624963760376\n",
      "step 250\n",
      "training loss: 2.856177568435669\n",
      "validation loss: 2.8602499961853027\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8754477500915527\n",
      "step 252\n",
      "training loss: 2.8498752117156982\n",
      "step 253\n",
      "training loss: 2.8543055057525635\n",
      "step 254\n",
      "training loss: 2.893768310546875\n",
      "step 255\n",
      "training loss: 2.8710243701934814\n",
      "step 256\n",
      "training loss: 2.883453369140625\n",
      "step 257\n",
      "training loss: 2.8842973709106445\n",
      "step 258\n",
      "training loss: 2.87039852142334\n",
      "step 259\n",
      "training loss: 2.8542768955230713\n",
      "step 260\n",
      "training loss: 2.8775153160095215\n",
      "validation loss: 2.8428914546966553\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8792858123779297\n",
      "step 262\n",
      "training loss: 2.8721923828125\n",
      "step 263\n",
      "training loss: 2.8690125942230225\n",
      "step 264\n",
      "training loss: 2.8812108039855957\n",
      "step 265\n",
      "training loss: 2.8888661861419678\n",
      "step 266\n",
      "training loss: 2.810375452041626\n",
      "step 267\n",
      "training loss: 2.851919174194336\n",
      "step 268\n",
      "training loss: 2.820821762084961\n",
      "step 269\n",
      "training loss: 2.8542842864990234\n",
      "step 270\n",
      "training loss: 2.888181447982788\n",
      "validation loss: 2.84033203125\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8765769004821777\n",
      "----------3.0 min per epoch----------\n",
      "epoch 4\n",
      "step 0\n",
      "training loss: 2.886800527572632\n",
      "validation loss: 2.8336727619171143\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.87716007232666\n",
      "step 2\n",
      "training loss: 2.807063102722168\n",
      "step 3\n",
      "training loss: 2.764800786972046\n",
      "step 4\n",
      "training loss: 2.8699448108673096\n",
      "step 5\n",
      "training loss: 2.887586832046509\n",
      "step 6\n",
      "training loss: 2.873957633972168\n",
      "step 7\n",
      "training loss: 2.855031967163086\n",
      "step 8\n",
      "training loss: 2.8610613346099854\n",
      "step 9\n",
      "training loss: 2.8781118392944336\n",
      "step 10\n",
      "training loss: 2.878683090209961\n",
      "validation loss: 2.8695900440216064\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.881103515625\n",
      "step 12\n",
      "training loss: 2.8677330017089844\n",
      "step 13\n",
      "training loss: 2.8827173709869385\n",
      "step 14\n",
      "training loss: 2.8817458152770996\n",
      "step 15\n",
      "training loss: 2.843170642852783\n",
      "step 16\n",
      "training loss: 2.8473470211029053\n",
      "step 17\n",
      "training loss: 2.861346960067749\n",
      "step 18\n",
      "training loss: 2.8685648441314697\n",
      "step 19\n",
      "training loss: 2.8717634677886963\n",
      "step 20\n",
      "training loss: 2.8720123767852783\n",
      "validation loss: 2.8760719299316406\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.874845027923584\n",
      "step 22\n",
      "training loss: 2.8774125576019287\n",
      "step 23\n",
      "training loss: 2.8697357177734375\n",
      "step 24\n",
      "training loss: 2.886460542678833\n",
      "step 25\n",
      "training loss: 2.8655362129211426\n",
      "step 26\n",
      "training loss: 2.8762621879577637\n",
      "step 27\n",
      "training loss: 2.884908437728882\n",
      "step 28\n",
      "training loss: 2.881709575653076\n",
      "step 29\n",
      "training loss: 2.878037929534912\n",
      "step 30\n",
      "training loss: 2.8785970211029053\n",
      "validation loss: 2.8628416061401367\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.861644983291626\n",
      "step 32\n",
      "training loss: 2.8448238372802734\n",
      "step 33\n",
      "training loss: 2.8655128479003906\n",
      "step 34\n",
      "training loss: 2.8792948722839355\n",
      "step 35\n",
      "training loss: 2.847576856613159\n",
      "step 36\n",
      "training loss: 2.8662872314453125\n",
      "step 37\n",
      "training loss: 2.8705739974975586\n",
      "step 38\n",
      "training loss: 2.859819173812866\n",
      "step 39\n",
      "training loss: 2.8854458332061768\n",
      "step 40\n",
      "training loss: 2.8596136569976807\n",
      "validation loss: 2.865187406539917\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.876941204071045\n",
      "step 42\n",
      "training loss: 2.857090473175049\n",
      "step 43\n",
      "training loss: 2.874315023422241\n",
      "step 44\n",
      "training loss: 2.8299365043640137\n",
      "step 45\n",
      "training loss: 2.8845431804656982\n",
      "step 46\n",
      "training loss: 2.851733446121216\n",
      "step 47\n",
      "training loss: 2.8851847648620605\n",
      "step 48\n",
      "training loss: 2.8705549240112305\n",
      "step 49\n",
      "training loss: 2.852789878845215\n",
      "step 50\n",
      "training loss: 2.846938371658325\n",
      "validation loss: 2.8704981803894043\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8687942028045654\n",
      "step 52\n",
      "training loss: 2.8593926429748535\n",
      "step 53\n",
      "training loss: 2.8521878719329834\n",
      "step 54\n",
      "training loss: 2.8851706981658936\n",
      "step 55\n",
      "training loss: 2.871166706085205\n",
      "step 56\n",
      "training loss: 2.8615405559539795\n",
      "step 57\n",
      "training loss: 2.8501813411712646\n",
      "step 58\n",
      "training loss: 2.8620810508728027\n",
      "step 59\n",
      "training loss: 2.861016273498535\n",
      "step 60\n",
      "training loss: 2.8357865810394287\n",
      "validation loss: 2.849548101425171\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.852084159851074\n",
      "step 62\n",
      "training loss: 2.8550353050231934\n",
      "step 63\n",
      "training loss: 2.8484842777252197\n",
      "step 64\n",
      "training loss: 2.839118719100952\n",
      "step 65\n",
      "training loss: 2.8773138523101807\n",
      "step 66\n",
      "training loss: 2.8515069484710693\n",
      "step 67\n",
      "training loss: 2.884410858154297\n",
      "step 68\n",
      "training loss: 2.8659257888793945\n",
      "step 69\n",
      "training loss: 2.842751979827881\n",
      "step 70\n",
      "training loss: 2.8790156841278076\n",
      "validation loss: 2.853811025619507\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8554799556732178\n",
      "step 72\n",
      "training loss: 2.8646273612976074\n",
      "step 73\n",
      "training loss: 2.866161584854126\n",
      "step 74\n",
      "training loss: 2.8355631828308105\n",
      "step 75\n",
      "training loss: 2.879812717437744\n",
      "step 76\n",
      "training loss: 2.8608953952789307\n",
      "step 77\n",
      "training loss: 2.876621961593628\n",
      "step 78\n",
      "training loss: 2.8743975162506104\n",
      "step 79\n",
      "training loss: 2.8965256214141846\n",
      "step 80\n",
      "training loss: 2.88029146194458\n",
      "validation loss: 2.836127996444702\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.843080997467041\n",
      "step 82\n",
      "training loss: 2.8757753372192383\n",
      "step 83\n",
      "training loss: 2.8621859550476074\n",
      "step 84\n",
      "training loss: 2.8514082431793213\n",
      "step 85\n",
      "training loss: 2.8592538833618164\n",
      "step 86\n",
      "training loss: 2.85876202583313\n",
      "step 87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.877800464630127\n",
      "step 88\n",
      "training loss: 2.866589069366455\n",
      "step 89\n",
      "training loss: 2.8819758892059326\n",
      "step 90\n",
      "training loss: 2.8815674781799316\n",
      "validation loss: 2.8430330753326416\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8809802532196045\n",
      "step 92\n",
      "training loss: 2.8527932167053223\n",
      "step 93\n",
      "training loss: 2.868520736694336\n",
      "step 94\n",
      "training loss: 2.8667891025543213\n",
      "step 95\n",
      "training loss: 2.8578431606292725\n",
      "step 96\n",
      "training loss: 2.870490550994873\n",
      "step 97\n",
      "training loss: 2.845829486846924\n",
      "step 98\n",
      "training loss: 2.871175527572632\n",
      "step 99\n",
      "training loss: 2.8760905265808105\n",
      "step 100\n",
      "training loss: 2.820991277694702\n",
      "validation loss: 2.872887134552002\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.859174966812134\n",
      "step 102\n",
      "training loss: 2.85251784324646\n",
      "step 103\n",
      "training loss: 2.8687665462493896\n",
      "step 104\n",
      "training loss: 2.8753135204315186\n",
      "step 105\n",
      "training loss: 2.847226619720459\n",
      "step 106\n",
      "training loss: 2.8712642192840576\n",
      "step 107\n",
      "training loss: 2.8241214752197266\n",
      "step 108\n",
      "training loss: 2.8551361560821533\n",
      "step 109\n",
      "training loss: 2.8762412071228027\n",
      "step 110\n",
      "training loss: 2.8667171001434326\n",
      "validation loss: 2.8787548542022705\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.857933759689331\n",
      "step 112\n",
      "training loss: 2.851040840148926\n",
      "step 113\n",
      "training loss: 2.868476390838623\n",
      "step 114\n",
      "training loss: 2.85349178314209\n",
      "step 115\n",
      "training loss: 2.8645689487457275\n",
      "step 116\n",
      "training loss: 2.8737945556640625\n",
      "step 117\n",
      "training loss: 2.8621089458465576\n",
      "step 118\n",
      "training loss: 2.862205982208252\n",
      "step 119\n",
      "training loss: 2.863231897354126\n",
      "step 120\n",
      "training loss: 2.838358163833618\n",
      "validation loss: 2.8331210613250732\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.843552589416504\n",
      "step 122\n",
      "training loss: 2.833716869354248\n",
      "step 123\n",
      "training loss: 2.8676865100860596\n",
      "step 124\n",
      "training loss: 2.873835802078247\n",
      "step 125\n",
      "training loss: 2.8652586936950684\n",
      "step 126\n",
      "training loss: 2.8623554706573486\n",
      "step 127\n",
      "training loss: 2.854741334915161\n",
      "step 128\n",
      "training loss: 2.8731205463409424\n",
      "step 129\n",
      "training loss: 2.8941595554351807\n",
      "step 130\n",
      "training loss: 2.813570499420166\n",
      "validation loss: 2.8514301776885986\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8634963035583496\n",
      "step 132\n",
      "training loss: 2.8558690547943115\n",
      "step 133\n",
      "training loss: 2.8816232681274414\n",
      "step 134\n",
      "training loss: 2.860553026199341\n",
      "step 135\n",
      "training loss: 2.881622791290283\n",
      "step 136\n",
      "training loss: 2.8569061756134033\n",
      "step 137\n",
      "training loss: 2.8417975902557373\n",
      "step 138\n",
      "training loss: 2.855504035949707\n",
      "step 139\n",
      "training loss: 2.8679161071777344\n",
      "step 140\n",
      "training loss: 2.85146164894104\n",
      "validation loss: 2.8938066959381104\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8840038776397705\n",
      "step 142\n",
      "training loss: 2.864290475845337\n",
      "step 143\n",
      "training loss: 2.828151226043701\n",
      "step 144\n",
      "training loss: 2.8562567234039307\n",
      "step 145\n",
      "training loss: 2.859459161758423\n",
      "step 146\n",
      "training loss: 2.85913348197937\n",
      "step 147\n",
      "training loss: 2.87237286567688\n",
      "step 148\n",
      "training loss: 2.8676352500915527\n",
      "step 149\n",
      "training loss: 2.870758533477783\n",
      "step 150\n",
      "training loss: 2.827392101287842\n",
      "validation loss: 2.8801918029785156\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8646533489227295\n",
      "step 152\n",
      "training loss: 2.8590567111968994\n",
      "step 153\n",
      "training loss: 2.8704094886779785\n",
      "step 154\n",
      "training loss: 2.8657643795013428\n",
      "step 155\n",
      "training loss: 2.8770971298217773\n",
      "step 156\n",
      "training loss: 2.872859001159668\n",
      "step 157\n",
      "training loss: 2.8535146713256836\n",
      "step 158\n",
      "training loss: 2.8795604705810547\n",
      "step 159\n",
      "training loss: 2.8757970333099365\n",
      "step 160\n",
      "training loss: 2.868450880050659\n",
      "validation loss: 2.8712167739868164\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8617115020751953\n",
      "step 162\n",
      "training loss: 2.8713769912719727\n",
      "step 163\n",
      "training loss: 2.860170841217041\n",
      "step 164\n",
      "training loss: 2.8605916500091553\n",
      "step 165\n",
      "training loss: 2.8448171615600586\n",
      "step 166\n",
      "training loss: 2.828702926635742\n",
      "step 167\n",
      "training loss: 2.8707234859466553\n",
      "step 168\n",
      "training loss: 2.853205919265747\n",
      "step 169\n",
      "training loss: 2.851177453994751\n",
      "step 170\n",
      "training loss: 2.8676722049713135\n",
      "validation loss: 2.9340176582336426\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.895150899887085\n",
      "step 172\n",
      "training loss: 2.847407102584839\n",
      "step 173\n",
      "training loss: 2.875103235244751\n",
      "step 174\n",
      "training loss: 2.8648452758789062\n",
      "step 175\n",
      "training loss: 2.87139630317688\n",
      "step 176\n",
      "training loss: 2.850958824157715\n",
      "step 177\n",
      "training loss: 2.8620879650115967\n",
      "step 178\n",
      "training loss: 2.8377068042755127\n",
      "step 179\n",
      "training loss: 2.8526265621185303\n",
      "step 180\n",
      "training loss: 2.8598015308380127\n",
      "validation loss: 2.8799500465393066\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.849376916885376\n",
      "step 182\n",
      "training loss: 2.8870434761047363\n",
      "step 183\n",
      "training loss: 2.862445592880249\n",
      "step 184\n",
      "training loss: 2.8531694412231445\n",
      "step 185\n",
      "training loss: 2.8661599159240723\n",
      "step 186\n",
      "training loss: 2.866739511489868\n",
      "step 187\n",
      "training loss: 2.857184410095215\n",
      "step 188\n",
      "training loss: 2.8485019207000732\n",
      "step 189\n",
      "training loss: 2.8346121311187744\n",
      "step 190\n",
      "training loss: 2.8214190006256104\n",
      "validation loss: 2.858264446258545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8783557415008545\n",
      "step 192\n",
      "training loss: 2.853170394897461\n",
      "step 193\n",
      "training loss: 2.8839128017425537\n",
      "step 194\n",
      "training loss: 2.857874870300293\n",
      "step 195\n",
      "training loss: 2.868968963623047\n",
      "step 196\n",
      "training loss: 2.8815224170684814\n",
      "step 197\n",
      "training loss: 2.858032464981079\n",
      "step 198\n",
      "training loss: 2.8552467823028564\n",
      "step 199\n",
      "training loss: 2.8677518367767334\n",
      "step 200\n",
      "training loss: 2.848971128463745\n",
      "validation loss: 2.856205463409424\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8560409545898438\n",
      "step 202\n",
      "training loss: 2.846829891204834\n",
      "step 203\n",
      "training loss: 2.852402448654175\n",
      "step 204\n",
      "training loss: 2.8404054641723633\n",
      "step 205\n",
      "training loss: 2.861102342605591\n",
      "step 206\n",
      "training loss: 2.8646600246429443\n",
      "step 207\n",
      "training loss: 2.8466994762420654\n",
      "step 208\n",
      "training loss: 2.8760340213775635\n",
      "step 209\n",
      "training loss: 2.8530659675598145\n",
      "step 210\n",
      "training loss: 2.8630871772766113\n",
      "validation loss: 2.8917810916900635\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.860125780105591\n",
      "step 212\n",
      "training loss: 2.8679630756378174\n",
      "step 213\n",
      "training loss: 2.793181896209717\n",
      "step 214\n",
      "training loss: 2.8517541885375977\n",
      "step 215\n",
      "training loss: 2.8379573822021484\n",
      "step 216\n",
      "training loss: 2.8509418964385986\n",
      "step 217\n",
      "training loss: 2.8740248680114746\n",
      "step 218\n",
      "training loss: 2.8723058700561523\n",
      "step 219\n",
      "training loss: 2.8638546466827393\n",
      "step 220\n",
      "training loss: 2.863229513168335\n",
      "validation loss: 2.8284754753112793\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8618013858795166\n",
      "step 222\n",
      "training loss: 2.8506319522857666\n",
      "step 223\n",
      "training loss: 2.856459140777588\n",
      "step 224\n",
      "training loss: 2.8694472312927246\n",
      "step 225\n",
      "training loss: 2.885711669921875\n",
      "step 226\n",
      "training loss: 2.8577258586883545\n",
      "step 227\n",
      "training loss: 2.8479151725769043\n",
      "step 228\n",
      "training loss: 2.849883556365967\n",
      "step 229\n",
      "training loss: 2.8488993644714355\n",
      "step 230\n",
      "training loss: 2.8621253967285156\n",
      "validation loss: 2.8597497940063477\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.867114782333374\n",
      "step 232\n",
      "training loss: 2.857724905014038\n",
      "step 233\n",
      "training loss: 2.8694870471954346\n",
      "step 234\n",
      "training loss: 2.8755879402160645\n",
      "step 235\n",
      "training loss: 2.8496296405792236\n",
      "step 236\n",
      "training loss: 2.874708652496338\n",
      "step 237\n",
      "training loss: 2.8760180473327637\n",
      "step 238\n",
      "training loss: 2.8473398685455322\n",
      "step 239\n",
      "training loss: 2.8487548828125\n",
      "step 240\n",
      "training loss: 2.874138832092285\n",
      "validation loss: 2.862915277481079\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8662538528442383\n",
      "step 242\n",
      "training loss: 2.8641607761383057\n",
      "step 243\n",
      "training loss: 2.866248369216919\n",
      "step 244\n",
      "training loss: 2.870448350906372\n",
      "step 245\n",
      "training loss: 2.8615920543670654\n",
      "step 246\n",
      "training loss: 2.865487575531006\n",
      "step 247\n",
      "training loss: 2.8630456924438477\n",
      "step 248\n",
      "training loss: 2.863969564437866\n",
      "step 249\n",
      "training loss: 2.8586692810058594\n",
      "step 250\n",
      "training loss: 2.826542854309082\n",
      "validation loss: 2.8684279918670654\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8529293537139893\n",
      "step 252\n",
      "training loss: 2.873623847961426\n",
      "step 253\n",
      "training loss: 2.850445508956909\n",
      "step 254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8542749881744385\n",
      "step 255\n",
      "training loss: 2.8913674354553223\n",
      "step 256\n",
      "training loss: 2.869060516357422\n",
      "step 257\n",
      "training loss: 2.885226249694824\n",
      "step 258\n",
      "training loss: 2.886568069458008\n",
      "step 259\n",
      "training loss: 2.8731791973114014\n",
      "step 260\n",
      "training loss: 2.853893280029297\n",
      "validation loss: 2.9126265048980713\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8778929710388184\n",
      "step 262\n",
      "training loss: 2.8804280757904053\n",
      "step 263\n",
      "training loss: 2.872926950454712\n",
      "step 264\n",
      "training loss: 2.8712193965911865\n",
      "step 265\n",
      "training loss: 2.8791234493255615\n",
      "step 266\n",
      "training loss: 2.887068748474121\n",
      "step 267\n",
      "training loss: 2.8131625652313232\n",
      "step 268\n",
      "training loss: 2.8526172637939453\n",
      "step 269\n",
      "training loss: 2.8203816413879395\n",
      "step 270\n",
      "training loss: 2.853499412536621\n",
      "validation loss: 2.8793318271636963\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.889162302017212\n",
      "----------3.0 min per epoch----------\n",
      "epoch 5\n",
      "step 0\n",
      "training loss: 2.879019021987915\n",
      "validation loss: 2.8701024055480957\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8894875049591064\n",
      "step 2\n",
      "training loss: 2.8826467990875244\n",
      "step 3\n",
      "training loss: 2.8049120903015137\n",
      "step 4\n",
      "training loss: 2.756899833679199\n",
      "step 5\n",
      "training loss: 2.8619446754455566\n",
      "step 6\n",
      "training loss: 2.8864495754241943\n",
      "step 7\n",
      "training loss: 2.8717923164367676\n",
      "step 8\n",
      "training loss: 2.8571600914001465\n",
      "step 9\n",
      "training loss: 2.8613312244415283\n",
      "step 10\n",
      "training loss: 2.8789684772491455\n",
      "validation loss: 2.8386502265930176\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8806889057159424\n",
      "step 12\n",
      "training loss: 2.8812339305877686\n",
      "step 13\n",
      "training loss: 2.8657073974609375\n",
      "step 14\n",
      "training loss: 2.8817646503448486\n",
      "step 15\n",
      "training loss: 2.8811280727386475\n",
      "step 16\n",
      "training loss: 2.842426300048828\n",
      "step 17\n",
      "training loss: 2.84725022315979\n",
      "step 18\n",
      "training loss: 2.860172748565674\n",
      "step 19\n",
      "training loss: 2.8674232959747314\n",
      "step 20\n",
      "training loss: 2.8735835552215576\n",
      "validation loss: 2.837307929992676\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8701610565185547\n",
      "step 22\n",
      "training loss: 2.8726348876953125\n",
      "step 23\n",
      "training loss: 2.8759350776672363\n",
      "step 24\n",
      "training loss: 2.8695027828216553\n",
      "step 25\n",
      "training loss: 2.886307954788208\n",
      "step 26\n",
      "training loss: 2.8686635494232178\n",
      "step 27\n",
      "training loss: 2.8757946491241455\n",
      "step 28\n",
      "training loss: 2.8853533267974854\n",
      "step 29\n",
      "training loss: 2.8796274662017822\n",
      "step 30\n",
      "training loss: 2.8772146701812744\n",
      "validation loss: 2.8378000259399414\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8756957054138184\n",
      "step 32\n",
      "training loss: 2.8596973419189453\n",
      "step 33\n",
      "training loss: 2.8433165550231934\n",
      "step 34\n",
      "training loss: 2.867727041244507\n",
      "step 35\n",
      "training loss: 2.878863573074341\n",
      "step 36\n",
      "training loss: 2.8424160480499268\n",
      "step 37\n",
      "training loss: 2.8618979454040527\n",
      "step 38\n",
      "training loss: 2.866132974624634\n",
      "step 39\n",
      "training loss: 2.8544390201568604\n",
      "step 40\n",
      "training loss: 2.881835460662842\n",
      "validation loss: 2.8587887287139893\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8597254753112793\n",
      "step 42\n",
      "training loss: 2.873244047164917\n",
      "step 43\n",
      "training loss: 2.85787034034729\n",
      "step 44\n",
      "training loss: 2.873502254486084\n",
      "step 45\n",
      "training loss: 2.8240177631378174\n",
      "step 46\n",
      "training loss: 2.8832857608795166\n",
      "step 47\n",
      "training loss: 2.8523261547088623\n",
      "step 48\n",
      "training loss: 2.8844552040100098\n",
      "step 49\n",
      "training loss: 2.867929697036743\n",
      "step 50\n",
      "training loss: 2.8496057987213135\n",
      "validation loss: 2.869100570678711\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.847151041030884\n",
      "step 52\n",
      "training loss: 2.866978406906128\n",
      "step 53\n",
      "training loss: 2.8532965183258057\n",
      "step 54\n",
      "training loss: 2.8500986099243164\n",
      "step 55\n",
      "training loss: 2.881087064743042\n",
      "step 56\n",
      "training loss: 2.870042324066162\n",
      "step 57\n",
      "training loss: 2.8600270748138428\n",
      "step 58\n",
      "training loss: 2.8455679416656494\n",
      "step 59\n",
      "training loss: 2.8592982292175293\n",
      "step 60\n",
      "training loss: 2.8624839782714844\n",
      "validation loss: 2.8629045486450195\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8342087268829346\n",
      "step 62\n",
      "training loss: 2.852893590927124\n",
      "step 63\n",
      "training loss: 2.8541600704193115\n",
      "step 64\n",
      "training loss: 2.846799373626709\n",
      "step 65\n",
      "training loss: 2.8380043506622314\n",
      "step 66\n",
      "training loss: 2.872877359390259\n",
      "step 67\n",
      "training loss: 2.8502719402313232\n",
      "step 68\n",
      "training loss: 2.8836159706115723\n",
      "step 69\n",
      "training loss: 2.864096164703369\n",
      "step 70\n",
      "training loss: 2.840669631958008\n",
      "validation loss: 2.862929582595825\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.878401041030884\n",
      "step 72\n",
      "training loss: 2.853685140609741\n",
      "step 73\n",
      "training loss: 2.8645431995391846\n",
      "step 74\n",
      "training loss: 2.8659889698028564\n",
      "step 75\n",
      "training loss: 2.834641695022583\n",
      "step 76\n",
      "training loss: 2.8777384757995605\n",
      "step 77\n",
      "training loss: 2.8617193698883057\n",
      "step 78\n",
      "training loss: 2.8776979446411133\n",
      "step 79\n",
      "training loss: 2.873199701309204\n",
      "step 80\n",
      "training loss: 2.894132375717163\n",
      "validation loss: 2.8710734844207764\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8824656009674072\n",
      "step 82\n",
      "training loss: 2.8387091159820557\n",
      "step 83\n",
      "training loss: 2.877725124359131\n",
      "step 84\n",
      "training loss: 2.860372543334961\n",
      "step 85\n",
      "training loss: 2.8508963584899902\n",
      "step 86\n",
      "training loss: 2.8558411598205566\n",
      "step 87\n",
      "training loss: 2.8573827743530273\n",
      "step 88\n",
      "training loss: 2.8784356117248535\n",
      "step 89\n",
      "training loss: 2.864021062850952\n",
      "step 90\n",
      "training loss: 2.880596160888672\n",
      "validation loss: 2.847216844558716\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8805456161499023\n",
      "step 92\n",
      "training loss: 2.8807334899902344\n",
      "step 93\n",
      "training loss: 2.855624198913574\n",
      "step 94\n",
      "training loss: 2.8704171180725098\n",
      "step 95\n",
      "training loss: 2.8667612075805664\n",
      "step 96\n",
      "training loss: 2.8560519218444824\n",
      "step 97\n",
      "training loss: 2.8702850341796875\n",
      "step 98\n",
      "training loss: 2.846721649169922\n",
      "step 99\n",
      "training loss: 2.872347593307495\n",
      "step 100\n",
      "training loss: 2.874720573425293\n",
      "validation loss: 2.8508074283599854\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8201377391815186\n",
      "step 102\n",
      "training loss: 2.85798978805542\n",
      "step 103\n",
      "training loss: 2.8522496223449707\n",
      "step 104\n",
      "training loss: 2.8674025535583496\n",
      "step 105\n",
      "training loss: 2.8739583492279053\n",
      "step 106\n",
      "training loss: 2.8480618000030518\n",
      "step 107\n",
      "training loss: 2.8691494464874268\n",
      "step 108\n",
      "training loss: 2.8256373405456543\n",
      "step 109\n",
      "training loss: 2.8576343059539795\n",
      "step 110\n",
      "training loss: 2.8757753372192383\n",
      "validation loss: 2.8325564861297607\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.866708993911743\n",
      "step 112\n",
      "training loss: 2.8612120151519775\n",
      "step 113\n",
      "training loss: 2.851032018661499\n",
      "step 114\n",
      "training loss: 2.8684375286102295\n",
      "step 115\n",
      "training loss: 2.854548931121826\n",
      "step 116\n",
      "training loss: 2.864068031311035\n",
      "step 117\n",
      "training loss: 2.8737599849700928\n",
      "step 118\n",
      "training loss: 2.8634512424468994\n",
      "step 119\n",
      "training loss: 2.860403537750244\n",
      "step 120\n",
      "training loss: 2.8587887287139893\n",
      "validation loss: 2.842337131500244\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8395044803619385\n",
      "step 122\n",
      "training loss: 2.8442788124084473\n",
      "step 123\n",
      "training loss: 2.8346943855285645\n",
      "step 124\n",
      "training loss: 2.869527578353882\n",
      "step 125\n",
      "training loss: 2.8728275299072266\n",
      "step 126\n",
      "training loss: 2.8656983375549316\n",
      "step 127\n",
      "training loss: 2.8616116046905518\n",
      "step 128\n",
      "training loss: 2.8545079231262207\n",
      "step 129\n",
      "training loss: 2.8757474422454834\n",
      "step 130\n",
      "training loss: 2.8938539028167725\n",
      "validation loss: 2.8697755336761475\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8135910034179688\n",
      "step 132\n",
      "training loss: 2.861872434616089\n",
      "step 133\n",
      "training loss: 2.853954315185547\n",
      "step 134\n",
      "training loss: 2.8809654712677\n",
      "step 135\n",
      "training loss: 2.860844612121582\n",
      "step 136\n",
      "training loss: 2.8814587593078613\n",
      "step 137\n",
      "training loss: 2.8566324710845947\n",
      "step 138\n",
      "training loss: 2.842665433883667\n",
      "step 139\n",
      "training loss: 2.8554587364196777\n",
      "step 140\n",
      "training loss: 2.8674066066741943\n",
      "validation loss: 2.8782989978790283\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8519444465637207\n",
      "step 142\n",
      "training loss: 2.8826212882995605\n",
      "step 143\n",
      "training loss: 2.864151954650879\n",
      "step 144\n",
      "training loss: 2.8261754512786865\n",
      "step 145\n",
      "training loss: 2.855239152908325\n",
      "step 146\n",
      "training loss: 2.8590312004089355\n",
      "step 147\n",
      "training loss: 2.8595845699310303\n",
      "step 148\n",
      "training loss: 2.8705267906188965\n",
      "step 149\n",
      "training loss: 2.8693289756774902\n",
      "step 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.868687152862549\n",
      "validation loss: 2.831836462020874\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8222434520721436\n",
      "step 152\n",
      "training loss: 2.866262197494507\n",
      "step 153\n",
      "training loss: 2.8567349910736084\n",
      "step 154\n",
      "training loss: 2.8699233531951904\n",
      "step 155\n",
      "training loss: 2.864001750946045\n",
      "step 156\n",
      "training loss: 2.876800775527954\n",
      "step 157\n",
      "training loss: 2.8713717460632324\n",
      "step 158\n",
      "training loss: 2.855459451675415\n",
      "step 159\n",
      "training loss: 2.8790135383605957\n",
      "step 160\n",
      "training loss: 2.8745951652526855\n",
      "validation loss: 2.8561437129974365\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.865291118621826\n",
      "step 162\n",
      "training loss: 2.8619866371154785\n",
      "step 163\n",
      "training loss: 2.871971607208252\n",
      "step 164\n",
      "training loss: 2.860208034515381\n",
      "step 165\n",
      "training loss: 2.860464334487915\n",
      "step 166\n",
      "training loss: 2.8426156044006348\n",
      "step 167\n",
      "training loss: 2.8247764110565186\n",
      "step 168\n",
      "training loss: 2.8727543354034424\n",
      "step 169\n",
      "training loss: 2.8518388271331787\n",
      "step 170\n",
      "training loss: 2.850895643234253\n",
      "validation loss: 2.891421318054199\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.868851900100708\n",
      "step 172\n",
      "training loss: 2.894286870956421\n",
      "step 173\n",
      "training loss: 2.8442299365997314\n",
      "step 174\n",
      "training loss: 2.87208890914917\n",
      "step 175\n",
      "training loss: 2.8640618324279785\n",
      "step 176\n",
      "training loss: 2.874439239501953\n",
      "step 177\n",
      "training loss: 2.851383686065674\n",
      "step 178\n",
      "training loss: 2.860568046569824\n",
      "step 179\n",
      "training loss: 2.839303970336914\n",
      "step 180\n",
      "training loss: 2.8524506092071533\n",
      "validation loss: 2.878866672515869\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.860121011734009\n",
      "step 182\n",
      "training loss: 2.851613998413086\n",
      "step 183\n",
      "training loss: 2.8874597549438477\n",
      "step 184\n",
      "training loss: 2.859539747238159\n",
      "step 185\n",
      "training loss: 2.8503870964050293\n",
      "step 186\n",
      "training loss: 2.8650286197662354\n",
      "step 187\n",
      "training loss: 2.8685624599456787\n",
      "step 188\n",
      "training loss: 2.856560230255127\n",
      "step 189\n",
      "training loss: 2.8465867042541504\n",
      "step 190\n",
      "training loss: 2.832087278366089\n",
      "validation loss: 2.879817247390747\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.819782257080078\n",
      "step 192\n",
      "training loss: 2.8781578540802\n",
      "step 193\n",
      "training loss: 2.8528335094451904\n",
      "step 194\n",
      "training loss: 2.883366346359253\n",
      "step 195\n",
      "training loss: 2.859272003173828\n",
      "step 196\n",
      "training loss: 2.8689157962799072\n",
      "step 197\n",
      "training loss: 2.8840391635894775\n",
      "step 198\n",
      "training loss: 2.8601999282836914\n",
      "step 199\n",
      "training loss: 2.858412027359009\n",
      "step 200\n",
      "training loss: 2.868165969848633\n",
      "validation loss: 2.9299416542053223\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.848051071166992\n",
      "step 202\n",
      "training loss: 2.8566911220550537\n",
      "step 203\n",
      "training loss: 2.8478341102600098\n",
      "step 204\n",
      "training loss: 2.853529214859009\n",
      "step 205\n",
      "training loss: 2.840305805206299\n",
      "step 206\n",
      "training loss: 2.860464334487915\n",
      "step 207\n",
      "training loss: 2.866020917892456\n",
      "step 208\n",
      "training loss: 2.847954511642456\n",
      "step 209\n",
      "training loss: 2.8749196529388428\n",
      "step 210\n",
      "training loss: 2.8569693565368652\n",
      "validation loss: 2.8831281661987305\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8639657497406006\n",
      "step 212\n",
      "training loss: 2.862086057662964\n",
      "step 213\n",
      "training loss: 2.8683202266693115\n",
      "step 214\n",
      "training loss: 2.792313575744629\n",
      "step 215\n",
      "training loss: 2.8535826206207275\n",
      "step 216\n",
      "training loss: 2.836901903152466\n",
      "step 217\n",
      "training loss: 2.8505430221557617\n",
      "step 218\n",
      "training loss: 2.872584581375122\n",
      "step 219\n",
      "training loss: 2.8729989528656006\n",
      "step 220\n",
      "training loss: 2.8636844158172607\n",
      "validation loss: 2.844794988632202\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.864426612854004\n",
      "step 222\n",
      "training loss: 2.8626420497894287\n",
      "step 223\n",
      "training loss: 2.8510305881500244\n",
      "step 224\n",
      "training loss: 2.8575360774993896\n",
      "step 225\n",
      "training loss: 2.868675708770752\n",
      "step 226\n",
      "training loss: 2.8877508640289307\n",
      "step 227\n",
      "training loss: 2.8585660457611084\n",
      "step 228\n",
      "training loss: 2.8453919887542725\n",
      "step 229\n",
      "training loss: 2.8515141010284424\n",
      "step 230\n",
      "training loss: 2.8493130207061768\n",
      "validation loss: 2.877772331237793\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8634679317474365\n",
      "step 232\n",
      "training loss: 2.8674724102020264\n",
      "step 233\n",
      "training loss: 2.85880446434021\n",
      "step 234\n",
      "training loss: 2.8692569732666016\n",
      "step 235\n",
      "training loss: 2.8766212463378906\n",
      "step 236\n",
      "training loss: 2.8501288890838623\n",
      "step 237\n",
      "training loss: 2.8756473064422607\n",
      "step 238\n",
      "training loss: 2.879171848297119\n",
      "step 239\n",
      "training loss: 2.847766637802124\n",
      "step 240\n",
      "training loss: 2.850982666015625\n",
      "validation loss: 2.8942391872406006\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.871675491333008\n",
      "step 242\n",
      "training loss: 2.8662049770355225\n",
      "step 243\n",
      "training loss: 2.865238904953003\n",
      "step 244\n",
      "training loss: 2.8681297302246094\n",
      "step 245\n",
      "training loss: 2.872387647628784\n",
      "step 246\n",
      "training loss: 2.861600399017334\n",
      "step 247\n",
      "training loss: 2.8661956787109375\n",
      "step 248\n",
      "training loss: 2.864009380340576\n",
      "step 249\n",
      "training loss: 2.8607542514801025\n",
      "step 250\n",
      "training loss: 2.860222578048706\n",
      "validation loss: 2.830017566680908\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8237874507904053\n",
      "step 252\n",
      "training loss: 2.851715564727783\n",
      "step 253\n",
      "training loss: 2.8712191581726074\n",
      "step 254\n",
      "training loss: 2.8498642444610596\n",
      "step 255\n",
      "training loss: 2.8566131591796875\n",
      "step 256\n",
      "training loss: 2.8918840885162354\n",
      "step 257\n",
      "training loss: 2.8668265342712402\n",
      "step 258\n",
      "training loss: 2.8780503273010254\n",
      "step 259\n",
      "training loss: 2.8809988498687744\n",
      "step 260\n",
      "training loss: 2.87192964553833\n",
      "validation loss: 2.867948293685913\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.850507974624634\n",
      "step 262\n",
      "training loss: 2.87967848777771\n",
      "step 263\n",
      "training loss: 2.878880739212036\n",
      "step 264\n",
      "training loss: 2.8743674755096436\n",
      "step 265\n",
      "training loss: 2.8699216842651367\n",
      "step 266\n",
      "training loss: 2.8798649311065674\n",
      "step 267\n",
      "training loss: 2.887803316116333\n",
      "step 268\n",
      "training loss: 2.8087854385375977\n",
      "step 269\n",
      "training loss: 2.848175048828125\n",
      "step 270\n",
      "training loss: 2.817452907562256\n",
      "validation loss: 2.863654851913452\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.856783866882324\n",
      "----------3.0 min per epoch----------\n",
      "epoch 6\n",
      "step 0\n",
      "training loss: 2.889894485473633\n",
      "validation loss: 2.8737246990203857\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.875410795211792\n",
      "step 2\n",
      "training loss: 2.8868279457092285\n",
      "step 3\n",
      "training loss: 2.878176689147949\n",
      "step 4\n",
      "training loss: 2.8061158657073975\n",
      "step 5\n",
      "training loss: 2.7679553031921387\n",
      "step 6\n",
      "training loss: 2.862135410308838\n",
      "step 7\n",
      "training loss: 2.8911473751068115\n",
      "step 8\n",
      "training loss: 2.8700733184814453\n",
      "step 9\n",
      "training loss: 2.8539485931396484\n",
      "step 10\n",
      "training loss: 2.862426519393921\n",
      "validation loss: 2.9396426677703857\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.876246929168701\n",
      "step 12\n",
      "training loss: 2.8762075901031494\n",
      "step 13\n",
      "training loss: 2.8816630840301514\n",
      "step 14\n",
      "training loss: 2.868361473083496\n",
      "step 15\n",
      "training loss: 2.8818838596343994\n",
      "step 16\n",
      "training loss: 2.8780012130737305\n",
      "step 17\n",
      "training loss: 2.8410894870758057\n",
      "step 18\n",
      "training loss: 2.848820924758911\n",
      "step 19\n",
      "training loss: 2.8605270385742188\n",
      "step 20\n",
      "training loss: 2.8637099266052246\n",
      "validation loss: 2.871721029281616\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8725168704986572\n",
      "step 22\n",
      "training loss: 2.8691532611846924\n",
      "step 23\n",
      "training loss: 2.87402606010437\n",
      "step 24\n",
      "training loss: 2.8773398399353027\n",
      "step 25\n",
      "training loss: 2.872135877609253\n",
      "step 26\n",
      "training loss: 2.8868532180786133\n",
      "step 27\n",
      "training loss: 2.8674209117889404\n",
      "step 28\n",
      "training loss: 2.8753464221954346\n",
      "step 29\n",
      "training loss: 2.884342670440674\n",
      "step 30\n",
      "training loss: 2.8788466453552246\n",
      "validation loss: 2.8634767532348633\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8777003288269043\n",
      "step 32\n",
      "training loss: 2.877467393875122\n",
      "step 33\n",
      "training loss: 2.861521005630493\n",
      "step 34\n",
      "training loss: 2.8430566787719727\n",
      "step 35\n",
      "training loss: 2.8668360710144043\n",
      "step 36\n",
      "training loss: 2.876880645751953\n",
      "step 37\n",
      "training loss: 2.8404946327209473\n",
      "step 38\n",
      "training loss: 2.864288568496704\n",
      "step 39\n",
      "training loss: 2.868689775466919\n",
      "step 40\n",
      "training loss: 2.8547158241271973\n",
      "validation loss: 2.826004981994629\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.88130521774292\n",
      "step 42\n",
      "training loss: 2.8565258979797363\n",
      "step 43\n",
      "training loss: 2.8747031688690186\n",
      "step 44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.856233596801758\n",
      "step 45\n",
      "training loss: 2.8732645511627197\n",
      "step 46\n",
      "training loss: 2.824481964111328\n",
      "step 47\n",
      "training loss: 2.8841967582702637\n",
      "step 48\n",
      "training loss: 2.850383758544922\n",
      "step 49\n",
      "training loss: 2.8838253021240234\n",
      "step 50\n",
      "training loss: 2.8676812648773193\n",
      "validation loss: 2.834606647491455\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.847787618637085\n",
      "step 52\n",
      "training loss: 2.8441193103790283\n",
      "step 53\n",
      "training loss: 2.867253303527832\n",
      "step 54\n",
      "training loss: 2.8560309410095215\n",
      "step 55\n",
      "training loss: 2.8503499031066895\n",
      "step 56\n",
      "training loss: 2.8826160430908203\n",
      "step 57\n",
      "training loss: 2.8705854415893555\n",
      "step 58\n",
      "training loss: 2.8608906269073486\n",
      "step 59\n",
      "training loss: 2.8466429710388184\n",
      "step 60\n",
      "training loss: 2.8607470989227295\n",
      "validation loss: 2.8330957889556885\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.860153913497925\n",
      "step 62\n",
      "training loss: 2.836853265762329\n",
      "step 63\n",
      "training loss: 2.8523199558258057\n",
      "step 64\n",
      "training loss: 2.852707862854004\n",
      "step 65\n",
      "training loss: 2.846552848815918\n",
      "step 66\n",
      "training loss: 2.837871551513672\n",
      "step 67\n",
      "training loss: 2.876438856124878\n",
      "step 68\n",
      "training loss: 2.8503167629241943\n",
      "step 69\n",
      "training loss: 2.884549617767334\n",
      "step 70\n",
      "training loss: 2.862412929534912\n",
      "validation loss: 2.8574867248535156\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8405492305755615\n",
      "step 72\n",
      "training loss: 2.8792667388916016\n",
      "step 73\n",
      "training loss: 2.8568427562713623\n",
      "step 74\n",
      "training loss: 2.8637280464172363\n",
      "step 75\n",
      "training loss: 2.8676812648773193\n",
      "step 76\n",
      "training loss: 2.835984468460083\n",
      "step 77\n",
      "training loss: 2.8815553188323975\n",
      "step 78\n",
      "training loss: 2.8582589626312256\n",
      "step 79\n",
      "training loss: 2.8777825832366943\n",
      "step 80\n",
      "training loss: 2.8739757537841797\n",
      "validation loss: 2.868481159210205\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.894979953765869\n",
      "step 82\n",
      "training loss: 2.879272937774658\n",
      "step 83\n",
      "training loss: 2.840118169784546\n",
      "step 84\n",
      "training loss: 2.8752105236053467\n",
      "step 85\n",
      "training loss: 2.860137701034546\n",
      "step 86\n",
      "training loss: 2.851285696029663\n",
      "step 87\n",
      "training loss: 2.8593974113464355\n",
      "step 88\n",
      "training loss: 2.859891176223755\n",
      "step 89\n",
      "training loss: 2.8796868324279785\n",
      "step 90\n",
      "training loss: 2.865236282348633\n",
      "validation loss: 2.8643219470977783\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.881166934967041\n",
      "step 92\n",
      "training loss: 2.8801729679107666\n",
      "step 93\n",
      "training loss: 2.8800323009490967\n",
      "step 94\n",
      "training loss: 2.8539750576019287\n",
      "step 95\n",
      "training loss: 2.869035243988037\n",
      "step 96\n",
      "training loss: 2.866702079772949\n",
      "step 97\n",
      "training loss: 2.857799530029297\n",
      "step 98\n",
      "training loss: 2.869262456893921\n",
      "step 99\n",
      "training loss: 2.846544027328491\n",
      "step 100\n",
      "training loss: 2.8710525035858154\n",
      "validation loss: 2.8661019802093506\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8748233318328857\n",
      "step 102\n",
      "training loss: 2.8218166828155518\n",
      "step 103\n",
      "training loss: 2.860724687576294\n",
      "step 104\n",
      "training loss: 2.850752353668213\n",
      "step 105\n",
      "training loss: 2.868368625640869\n",
      "step 106\n",
      "training loss: 2.8752615451812744\n",
      "step 107\n",
      "training loss: 2.8493967056274414\n",
      "step 108\n",
      "training loss: 2.8700764179229736\n",
      "step 109\n",
      "training loss: 2.8252182006835938\n",
      "step 110\n",
      "training loss: 2.8561644554138184\n",
      "validation loss: 2.868251323699951\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8772926330566406\n",
      "step 112\n",
      "training loss: 2.8683087825775146\n",
      "step 113\n",
      "training loss: 2.860799551010132\n",
      "step 114\n",
      "training loss: 2.8525116443634033\n",
      "step 115\n",
      "training loss: 2.870438814163208\n",
      "step 116\n",
      "training loss: 2.853238105773926\n",
      "step 117\n",
      "training loss: 2.8661458492279053\n",
      "step 118\n",
      "training loss: 2.874897003173828\n",
      "step 119\n",
      "training loss: 2.862443208694458\n",
      "step 120\n",
      "training loss: 2.8609843254089355\n",
      "validation loss: 2.8470373153686523\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.860741376876831\n",
      "step 122\n",
      "training loss: 2.840878963470459\n",
      "step 123\n",
      "training loss: 2.8428428173065186\n",
      "step 124\n",
      "training loss: 2.834113836288452\n",
      "step 125\n",
      "training loss: 2.868053913116455\n",
      "step 126\n",
      "training loss: 2.8752856254577637\n",
      "step 127\n",
      "training loss: 2.8651459217071533\n",
      "step 128\n",
      "training loss: 2.8617427349090576\n",
      "step 129\n",
      "training loss: 2.8553380966186523\n",
      "step 130\n",
      "training loss: 2.8735463619232178\n",
      "validation loss: 2.849036693572998\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8934543132781982\n",
      "step 132\n",
      "training loss: 2.812044143676758\n",
      "step 133\n",
      "training loss: 2.861445903778076\n",
      "step 134\n",
      "training loss: 2.8558096885681152\n",
      "step 135\n",
      "training loss: 2.879220724105835\n",
      "step 136\n",
      "training loss: 2.8582377433776855\n",
      "step 137\n",
      "training loss: 2.8808090686798096\n",
      "step 138\n",
      "training loss: 2.855731248855591\n",
      "step 139\n",
      "training loss: 2.8411660194396973\n",
      "step 140\n",
      "training loss: 2.855997085571289\n",
      "validation loss: 2.8336915969848633\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8677947521209717\n",
      "step 142\n",
      "training loss: 2.8502378463745117\n",
      "step 143\n",
      "training loss: 2.8806302547454834\n",
      "step 144\n",
      "training loss: 2.864650011062622\n",
      "step 145\n",
      "training loss: 2.830320119857788\n",
      "step 146\n",
      "training loss: 2.8540897369384766\n",
      "step 147\n",
      "training loss: 2.8570938110351562\n",
      "step 148\n",
      "training loss: 2.857048749923706\n",
      "step 149\n",
      "training loss: 2.8688859939575195\n",
      "step 150\n",
      "training loss: 2.867333173751831\n",
      "validation loss: 2.8405637741088867\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8687617778778076\n",
      "step 152\n",
      "training loss: 2.822770357131958\n",
      "step 153\n",
      "training loss: 2.8654773235321045\n",
      "step 154\n",
      "training loss: 2.858994483947754\n",
      "step 155\n",
      "training loss: 2.86771821975708\n",
      "step 156\n",
      "training loss: 2.8659629821777344\n",
      "step 157\n",
      "training loss: 2.8772428035736084\n",
      "step 158\n",
      "training loss: 2.874067544937134\n",
      "step 159\n",
      "training loss: 2.8545873165130615\n",
      "step 160\n",
      "training loss: 2.880377769470215\n",
      "validation loss: 2.8694570064544678\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.875521183013916\n",
      "step 162\n",
      "training loss: 2.865394115447998\n",
      "step 163\n",
      "training loss: 2.86116099357605\n",
      "step 164\n",
      "training loss: 2.8718526363372803\n",
      "step 165\n",
      "training loss: 2.860538959503174\n",
      "step 166\n",
      "training loss: 2.860769510269165\n",
      "step 167\n",
      "training loss: 2.841635227203369\n",
      "step 168\n",
      "training loss: 2.828253746032715\n",
      "step 169\n",
      "training loss: 2.8729746341705322\n",
      "step 170\n",
      "training loss: 2.850216865539551\n",
      "validation loss: 2.8800206184387207\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8495535850524902\n",
      "step 172\n",
      "training loss: 2.864743947982788\n",
      "step 173\n",
      "training loss: 2.8960068225860596\n",
      "step 174\n",
      "training loss: 2.8452086448669434\n",
      "step 175\n",
      "training loss: 2.8718409538269043\n",
      "step 176\n",
      "training loss: 2.8650460243225098\n",
      "step 177\n",
      "training loss: 2.8772261142730713\n",
      "step 178\n",
      "training loss: 2.8518621921539307\n",
      "step 179\n",
      "training loss: 2.8617022037506104\n",
      "step 180\n",
      "training loss: 2.837125539779663\n",
      "validation loss: 2.8325729370117188\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8522887229919434\n",
      "step 182\n",
      "training loss: 2.8592875003814697\n",
      "step 183\n",
      "training loss: 2.8468782901763916\n",
      "step 184\n",
      "training loss: 2.8877599239349365\n",
      "step 185\n",
      "training loss: 2.862889051437378\n",
      "step 186\n",
      "training loss: 2.8487141132354736\n",
      "step 187\n",
      "training loss: 2.8677449226379395\n",
      "step 188\n",
      "training loss: 2.868081569671631\n",
      "step 189\n",
      "training loss: 2.8553383350372314\n",
      "step 190\n",
      "training loss: 2.8482394218444824\n",
      "validation loss: 2.8520498275756836\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8326101303100586\n",
      "step 192\n",
      "training loss: 2.819948196411133\n",
      "step 193\n",
      "training loss: 2.878275156021118\n",
      "step 194\n",
      "training loss: 2.8538732528686523\n",
      "step 195\n",
      "training loss: 2.8831253051757812\n",
      "step 196\n",
      "training loss: 2.857605218887329\n",
      "step 197\n",
      "training loss: 2.8683242797851562\n",
      "step 198\n",
      "training loss: 2.8846240043640137\n",
      "step 199\n",
      "training loss: 2.8592915534973145\n",
      "step 200\n",
      "training loss: 2.858142375946045\n",
      "validation loss: 2.8863883018493652\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.869114398956299\n",
      "step 202\n",
      "training loss: 2.84649658203125\n",
      "step 203\n",
      "training loss: 2.8583905696868896\n",
      "step 204\n",
      "training loss: 2.849196195602417\n",
      "step 205\n",
      "training loss: 2.8536434173583984\n",
      "step 206\n",
      "training loss: 2.843571186065674\n",
      "step 207\n",
      "training loss: 2.8612093925476074\n",
      "step 208\n",
      "training loss: 2.8637611865997314\n",
      "step 209\n",
      "training loss: 2.8482539653778076\n",
      "step 210\n",
      "training loss: 2.8746862411499023\n",
      "validation loss: 2.8812220096588135\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8545279502868652\n",
      "step 212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8651907444000244\n",
      "step 213\n",
      "training loss: 2.8599910736083984\n",
      "step 214\n",
      "training loss: 2.8672592639923096\n",
      "step 215\n",
      "training loss: 2.791482448577881\n",
      "step 216\n",
      "training loss: 2.8531887531280518\n",
      "step 217\n",
      "training loss: 2.8362646102905273\n",
      "step 218\n",
      "training loss: 2.8474106788635254\n",
      "step 219\n",
      "training loss: 2.8704140186309814\n",
      "step 220\n",
      "training loss: 2.874957799911499\n",
      "validation loss: 2.8739750385284424\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8658671379089355\n",
      "step 222\n",
      "training loss: 2.863823413848877\n",
      "step 223\n",
      "training loss: 2.8625080585479736\n",
      "step 224\n",
      "training loss: 2.8518190383911133\n",
      "step 225\n",
      "training loss: 2.8579909801483154\n",
      "step 226\n",
      "training loss: 2.8713927268981934\n",
      "step 227\n",
      "training loss: 2.8876900672912598\n",
      "step 228\n",
      "training loss: 2.8594391345977783\n",
      "step 229\n",
      "training loss: 2.847069025039673\n",
      "step 230\n",
      "training loss: 2.8497612476348877\n",
      "validation loss: 2.9273736476898193\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.849602222442627\n",
      "step 232\n",
      "training loss: 2.8619251251220703\n",
      "step 233\n",
      "training loss: 2.8672516345977783\n",
      "step 234\n",
      "training loss: 2.8599376678466797\n",
      "step 235\n",
      "training loss: 2.8704752922058105\n",
      "step 236\n",
      "training loss: 2.87599515914917\n",
      "step 237\n",
      "training loss: 2.849334716796875\n",
      "step 238\n",
      "training loss: 2.8749518394470215\n",
      "step 239\n",
      "training loss: 2.8795807361602783\n",
      "step 240\n",
      "training loss: 2.847360849380493\n",
      "validation loss: 2.880178213119507\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.849853992462158\n",
      "step 242\n",
      "training loss: 2.8719356060028076\n",
      "step 243\n",
      "training loss: 2.8655736446380615\n",
      "step 244\n",
      "training loss: 2.866363525390625\n",
      "step 245\n",
      "training loss: 2.8665707111358643\n",
      "step 246\n",
      "training loss: 2.871150255203247\n",
      "step 247\n",
      "training loss: 2.8627631664276123\n",
      "step 248\n",
      "training loss: 2.8668744564056396\n",
      "step 249\n",
      "training loss: 2.8631908893585205\n",
      "step 250\n",
      "training loss: 2.863978385925293\n",
      "validation loss: 2.8475987911224365\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8575150966644287\n",
      "step 252\n",
      "training loss: 2.8244998455047607\n",
      "step 253\n",
      "training loss: 2.8541409969329834\n",
      "step 254\n",
      "training loss: 2.8706367015838623\n",
      "step 255\n",
      "training loss: 2.8473572731018066\n",
      "step 256\n",
      "training loss: 2.855215072631836\n",
      "step 257\n",
      "training loss: 2.8885409832000732\n",
      "step 258\n",
      "training loss: 2.86618709564209\n",
      "step 259\n",
      "training loss: 2.882413864135742\n",
      "step 260\n",
      "training loss: 2.8817458152770996\n",
      "validation loss: 2.8589346408843994\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8711090087890625\n",
      "step 262\n",
      "training loss: 2.852100372314453\n",
      "step 263\n",
      "training loss: 2.877598762512207\n",
      "step 264\n",
      "training loss: 2.8782148361206055\n",
      "step 265\n",
      "training loss: 2.8695809841156006\n",
      "step 266\n",
      "training loss: 2.8680341243743896\n",
      "step 267\n",
      "training loss: 2.879727602005005\n",
      "step 268\n",
      "training loss: 2.888110399246216\n",
      "step 269\n",
      "training loss: 2.81086802482605\n",
      "step 270\n",
      "training loss: 2.851818323135376\n",
      "validation loss: 2.898728370666504\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.814455032348633\n",
      "----------3.0 min per epoch----------\n",
      "epoch 7\n",
      "step 0\n",
      "training loss: 2.851297616958618\n",
      "validation loss: 2.822956085205078\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.883127450942993\n",
      "step 2\n",
      "training loss: 2.8772873878479004\n",
      "step 3\n",
      "training loss: 2.889603853225708\n",
      "step 4\n",
      "training loss: 2.8804328441619873\n",
      "step 5\n",
      "training loss: 2.807008981704712\n",
      "step 6\n",
      "training loss: 2.753387689590454\n",
      "step 7\n",
      "training loss: 2.8740439414978027\n",
      "step 8\n",
      "training loss: 2.8821935653686523\n",
      "step 9\n",
      "training loss: 2.8690431118011475\n",
      "step 10\n",
      "training loss: 2.8579328060150146\n",
      "validation loss: 2.8623898029327393\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.861786365509033\n",
      "step 12\n",
      "training loss: 2.8775794506073\n",
      "step 13\n",
      "training loss: 2.8795197010040283\n",
      "step 14\n",
      "training loss: 2.8809266090393066\n",
      "step 15\n",
      "training loss: 2.8675427436828613\n",
      "step 16\n",
      "training loss: 2.882263422012329\n",
      "step 17\n",
      "training loss: 2.8816075325012207\n",
      "step 18\n",
      "training loss: 2.8402984142303467\n",
      "step 19\n",
      "training loss: 2.848677396774292\n",
      "step 20\n",
      "training loss: 2.8607113361358643\n",
      "validation loss: 2.8727364540100098\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8644285202026367\n",
      "step 22\n",
      "training loss: 2.876521587371826\n",
      "step 23\n",
      "training loss: 2.868044137954712\n",
      "step 24\n",
      "training loss: 2.8747928142547607\n",
      "step 25\n",
      "training loss: 2.878831386566162\n",
      "step 26\n",
      "training loss: 2.8709731101989746\n",
      "step 27\n",
      "training loss: 2.887312173843384\n",
      "step 28\n",
      "training loss: 2.866722822189331\n",
      "step 29\n",
      "training loss: 2.875897169113159\n",
      "step 30\n",
      "training loss: 2.8854293823242188\n",
      "validation loss: 2.8863766193389893\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8800010681152344\n",
      "step 32\n",
      "training loss: 2.8776803016662598\n",
      "step 33\n",
      "training loss: 2.877654790878296\n",
      "step 34\n",
      "training loss: 2.8619484901428223\n",
      "step 35\n",
      "training loss: 2.8449857234954834\n",
      "step 36\n",
      "training loss: 2.867713451385498\n",
      "step 37\n",
      "training loss: 2.874603271484375\n",
      "step 38\n",
      "training loss: 2.8411715030670166\n",
      "step 39\n",
      "training loss: 2.863029956817627\n",
      "step 40\n",
      "training loss: 2.868220567703247\n",
      "validation loss: 2.9212043285369873\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.852487325668335\n",
      "step 42\n",
      "training loss: 2.8815841674804688\n",
      "step 43\n",
      "training loss: 2.855593681335449\n",
      "step 44\n",
      "training loss: 2.8732078075408936\n",
      "step 45\n",
      "training loss: 2.856353759765625\n",
      "step 46\n",
      "training loss: 2.8735556602478027\n",
      "step 47\n",
      "training loss: 2.826172351837158\n",
      "step 48\n",
      "training loss: 2.8857669830322266\n",
      "step 49\n",
      "training loss: 2.851828098297119\n",
      "step 50\n",
      "training loss: 2.8836822509765625\n",
      "validation loss: 2.869354248046875\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.869349718093872\n",
      "step 52\n",
      "training loss: 2.851186752319336\n",
      "step 53\n",
      "training loss: 2.84574556350708\n",
      "step 54\n",
      "training loss: 2.8671767711639404\n",
      "step 55\n",
      "training loss: 2.855998992919922\n",
      "step 56\n",
      "training loss: 2.849532127380371\n",
      "step 57\n",
      "training loss: 2.8825361728668213\n",
      "step 58\n",
      "training loss: 2.871335983276367\n",
      "step 59\n",
      "training loss: 2.862633466720581\n",
      "step 60\n",
      "training loss: 2.847501277923584\n",
      "validation loss: 2.8647923469543457\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8621695041656494\n",
      "step 62\n",
      "training loss: 2.8595945835113525\n",
      "step 63\n",
      "training loss: 2.8401641845703125\n",
      "step 64\n",
      "training loss: 2.8519339561462402\n",
      "step 65\n",
      "training loss: 2.8511574268341064\n",
      "step 66\n",
      "training loss: 2.845808267593384\n",
      "step 67\n",
      "training loss: 2.8382439613342285\n",
      "step 68\n",
      "training loss: 2.8758931159973145\n",
      "step 69\n",
      "training loss: 2.8514750003814697\n",
      "step 70\n",
      "training loss: 2.8814117908477783\n",
      "validation loss: 2.8282413482666016\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.862373113632202\n",
      "step 72\n",
      "training loss: 2.8414711952209473\n",
      "step 73\n",
      "training loss: 2.8770573139190674\n",
      "step 74\n",
      "training loss: 2.858037233352661\n",
      "step 75\n",
      "training loss: 2.8666744232177734\n",
      "step 76\n",
      "training loss: 2.8654427528381348\n",
      "step 77\n",
      "training loss: 2.8364946842193604\n",
      "step 78\n",
      "training loss: 2.878047466278076\n",
      "step 79\n",
      "training loss: 2.8621010780334473\n",
      "step 80\n",
      "training loss: 2.8781120777130127\n",
      "validation loss: 2.833930015563965\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.875714063644409\n",
      "step 82\n",
      "training loss: 2.8956093788146973\n",
      "step 83\n",
      "training loss: 2.880413770675659\n",
      "step 84\n",
      "training loss: 2.8358712196350098\n",
      "step 85\n",
      "training loss: 2.8745267391204834\n",
      "step 86\n",
      "training loss: 2.8609747886657715\n",
      "step 87\n",
      "training loss: 2.8519818782806396\n",
      "step 88\n",
      "training loss: 2.8588383197784424\n",
      "step 89\n",
      "training loss: 2.8573875427246094\n",
      "step 90\n",
      "training loss: 2.8786308765411377\n",
      "validation loss: 2.8410086631774902\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8636300563812256\n",
      "step 92\n",
      "training loss: 2.8785505294799805\n",
      "step 93\n",
      "training loss: 2.879824161529541\n",
      "step 94\n",
      "training loss: 2.880782127380371\n",
      "step 95\n",
      "training loss: 2.853803873062134\n",
      "step 96\n",
      "training loss: 2.8707354068756104\n",
      "step 97\n",
      "training loss: 2.865767002105713\n",
      "step 98\n",
      "training loss: 2.858049154281616\n",
      "step 99\n",
      "training loss: 2.8696932792663574\n",
      "step 100\n",
      "training loss: 2.8458502292633057\n",
      "validation loss: 2.8586173057556152\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8715617656707764\n",
      "step 102\n",
      "training loss: 2.8767173290252686\n",
      "step 103\n",
      "training loss: 2.82025408744812\n",
      "step 104\n",
      "training loss: 2.8585479259490967\n",
      "step 105\n",
      "training loss: 2.852264165878296\n",
      "step 106\n",
      "training loss: 2.869154930114746\n",
      "step 107\n",
      "training loss: 2.8769052028656006\n",
      "step 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8481788635253906\n",
      "step 109\n",
      "training loss: 2.870295763015747\n",
      "step 110\n",
      "training loss: 2.8245749473571777\n",
      "validation loss: 2.8696444034576416\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8570139408111572\n",
      "step 112\n",
      "training loss: 2.8758742809295654\n",
      "step 113\n",
      "training loss: 2.8697152137756348\n",
      "step 114\n",
      "training loss: 2.8611152172088623\n",
      "step 115\n",
      "training loss: 2.8508901596069336\n",
      "step 116\n",
      "training loss: 2.8699662685394287\n",
      "step 117\n",
      "training loss: 2.8532872200012207\n",
      "step 118\n",
      "training loss: 2.865222215652466\n",
      "step 119\n",
      "training loss: 2.874776601791382\n",
      "step 120\n",
      "training loss: 2.863044500350952\n",
      "validation loss: 2.8613908290863037\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8603100776672363\n",
      "step 122\n",
      "training loss: 2.860532283782959\n",
      "step 123\n",
      "training loss: 2.838073492050171\n",
      "step 124\n",
      "training loss: 2.843129873275757\n",
      "step 125\n",
      "training loss: 2.8358259201049805\n",
      "step 126\n",
      "training loss: 2.8682703971862793\n",
      "step 127\n",
      "training loss: 2.8743183612823486\n",
      "step 128\n",
      "training loss: 2.8650481700897217\n",
      "step 129\n",
      "training loss: 2.8631696701049805\n",
      "step 130\n",
      "training loss: 2.851886034011841\n",
      "validation loss: 2.8640024662017822\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.876347064971924\n",
      "step 132\n",
      "training loss: 2.8944594860076904\n",
      "step 133\n",
      "training loss: 2.814091444015503\n",
      "step 134\n",
      "training loss: 2.86367130279541\n",
      "step 135\n",
      "training loss: 2.8552911281585693\n",
      "step 136\n",
      "training loss: 2.8791754245758057\n",
      "step 137\n",
      "training loss: 2.8575048446655273\n",
      "step 138\n",
      "training loss: 2.883098840713501\n",
      "step 139\n",
      "training loss: 2.8573005199432373\n",
      "step 140\n",
      "training loss: 2.8413093090057373\n",
      "validation loss: 2.870572090148926\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8558597564697266\n",
      "step 142\n",
      "training loss: 2.8671045303344727\n",
      "step 143\n",
      "training loss: 2.8515424728393555\n",
      "step 144\n",
      "training loss: 2.8786118030548096\n",
      "step 145\n",
      "training loss: 2.8651082515716553\n",
      "step 146\n",
      "training loss: 2.8317480087280273\n",
      "step 147\n",
      "training loss: 2.8527398109436035\n",
      "step 148\n",
      "training loss: 2.856250524520874\n",
      "step 149\n",
      "training loss: 2.859854221343994\n",
      "step 150\n",
      "training loss: 2.8702023029327393\n",
      "validation loss: 2.8465383052825928\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8687005043029785\n",
      "step 152\n",
      "training loss: 2.8684380054473877\n",
      "step 153\n",
      "training loss: 2.826552152633667\n",
      "step 154\n",
      "training loss: 2.865865707397461\n",
      "step 155\n",
      "training loss: 2.8606626987457275\n",
      "step 156\n",
      "training loss: 2.870778799057007\n",
      "step 157\n",
      "training loss: 2.8654582500457764\n",
      "step 158\n",
      "training loss: 2.8769097328186035\n",
      "step 159\n",
      "training loss: 2.8743443489074707\n",
      "step 160\n",
      "training loss: 2.859100580215454\n",
      "validation loss: 2.8494393825531006\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8808536529541016\n",
      "step 162\n",
      "training loss: 2.8775556087493896\n",
      "step 163\n",
      "training loss: 2.8658928871154785\n",
      "step 164\n",
      "training loss: 2.8616867065429688\n",
      "step 165\n",
      "training loss: 2.8715403079986572\n",
      "step 166\n",
      "training loss: 2.8631627559661865\n",
      "step 167\n",
      "training loss: 2.8623998165130615\n",
      "step 168\n",
      "training loss: 2.8410329818725586\n",
      "step 169\n",
      "training loss: 2.82517409324646\n",
      "step 170\n",
      "training loss: 2.872100830078125\n",
      "validation loss: 2.834850311279297\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.853206157684326\n",
      "step 172\n",
      "training loss: 2.8511409759521484\n",
      "step 173\n",
      "training loss: 2.8669960498809814\n",
      "step 174\n",
      "training loss: 2.8964340686798096\n",
      "step 175\n",
      "training loss: 2.8437564373016357\n",
      "step 176\n",
      "training loss: 2.8743059635162354\n",
      "step 177\n",
      "training loss: 2.86417293548584\n",
      "step 178\n",
      "training loss: 2.874347686767578\n",
      "step 179\n",
      "training loss: 2.849918842315674\n",
      "step 180\n",
      "training loss: 2.8641397953033447\n",
      "validation loss: 2.846684455871582\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.837529420852661\n",
      "step 182\n",
      "training loss: 2.8542027473449707\n",
      "step 183\n",
      "training loss: 2.8586533069610596\n",
      "step 184\n",
      "training loss: 2.847525119781494\n",
      "step 185\n",
      "training loss: 2.886645793914795\n",
      "step 186\n",
      "training loss: 2.8629257678985596\n",
      "step 187\n",
      "training loss: 2.849536657333374\n",
      "step 188\n",
      "training loss: 2.8678641319274902\n",
      "step 189\n",
      "training loss: 2.870181083679199\n",
      "step 190\n",
      "training loss: 2.857208728790283\n",
      "validation loss: 2.8691580295562744\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8490984439849854\n",
      "step 192\n",
      "training loss: 2.8334507942199707\n",
      "step 193\n",
      "training loss: 2.8175570964813232\n",
      "step 194\n",
      "training loss: 2.8799569606781006\n",
      "step 195\n",
      "training loss: 2.8543684482574463\n",
      "step 196\n",
      "training loss: 2.882671356201172\n",
      "step 197\n",
      "training loss: 2.8595402240753174\n",
      "step 198\n",
      "training loss: 2.868067979812622\n",
      "step 199\n",
      "training loss: 2.8826258182525635\n",
      "step 200\n",
      "training loss: 2.859398603439331\n",
      "validation loss: 2.8767237663269043\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.860300302505493\n",
      "step 202\n",
      "training loss: 2.8686327934265137\n",
      "step 203\n",
      "training loss: 2.8460896015167236\n",
      "step 204\n",
      "training loss: 2.8569276332855225\n",
      "step 205\n",
      "training loss: 2.846531629562378\n",
      "step 206\n",
      "training loss: 2.852750301361084\n",
      "step 207\n",
      "training loss: 2.84321665763855\n",
      "step 208\n",
      "training loss: 2.8620822429656982\n",
      "step 209\n",
      "training loss: 2.864558696746826\n",
      "step 210\n",
      "training loss: 2.847182035446167\n",
      "validation loss: 2.8321852684020996\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8750529289245605\n",
      "step 212\n",
      "training loss: 2.854182481765747\n",
      "step 213\n",
      "training loss: 2.8647828102111816\n",
      "step 214\n",
      "training loss: 2.8605828285217285\n",
      "step 215\n",
      "training loss: 2.8681366443634033\n",
      "step 216\n",
      "training loss: 2.792112350463867\n",
      "step 217\n",
      "training loss: 2.852576971054077\n",
      "step 218\n",
      "training loss: 2.83754563331604\n",
      "step 219\n",
      "training loss: 2.847520589828491\n",
      "step 220\n",
      "training loss: 2.8712875843048096\n",
      "validation loss: 2.854510545730591\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8739938735961914\n",
      "step 222\n",
      "training loss: 2.8651609420776367\n",
      "step 223\n",
      "training loss: 2.8657636642456055\n",
      "step 224\n",
      "training loss: 2.8632993698120117\n",
      "step 225\n",
      "training loss: 2.850989818572998\n",
      "step 226\n",
      "training loss: 2.855987787246704\n",
      "step 227\n",
      "training loss: 2.8677706718444824\n",
      "step 228\n",
      "training loss: 2.8885436058044434\n",
      "step 229\n",
      "training loss: 2.859589099884033\n",
      "step 230\n",
      "training loss: 2.847494125366211\n",
      "validation loss: 2.888707160949707\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8504085540771484\n",
      "step 232\n",
      "training loss: 2.8504891395568848\n",
      "step 233\n",
      "training loss: 2.8623573780059814\n",
      "step 234\n",
      "training loss: 2.867560863494873\n",
      "step 235\n",
      "training loss: 2.8592050075531006\n",
      "step 236\n",
      "training loss: 2.868795156478882\n",
      "step 237\n",
      "training loss: 2.873272657394409\n",
      "step 238\n",
      "training loss: 2.850100517272949\n",
      "step 239\n",
      "training loss: 2.8760292530059814\n",
      "step 240\n",
      "training loss: 2.8790953159332275\n",
      "validation loss: 2.8779449462890625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8454110622406006\n",
      "step 242\n",
      "training loss: 2.8477835655212402\n",
      "step 243\n",
      "training loss: 2.872452735900879\n",
      "step 244\n",
      "training loss: 2.866448163986206\n",
      "step 245\n",
      "training loss: 2.8629374504089355\n",
      "step 246\n",
      "training loss: 2.869760751724243\n",
      "step 247\n",
      "training loss: 2.8702125549316406\n",
      "step 248\n",
      "training loss: 2.8610880374908447\n",
      "step 249\n",
      "training loss: 2.8653194904327393\n",
      "step 250\n",
      "training loss: 2.864394426345825\n",
      "validation loss: 2.87315034866333\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.86269474029541\n",
      "step 252\n",
      "training loss: 2.8617303371429443\n",
      "step 253\n",
      "training loss: 2.8249459266662598\n",
      "step 254\n",
      "training loss: 2.8559820652008057\n",
      "step 255\n",
      "training loss: 2.869569778442383\n",
      "step 256\n",
      "training loss: 2.8484108448028564\n",
      "step 257\n",
      "training loss: 2.861570358276367\n",
      "step 258\n",
      "training loss: 2.8901772499084473\n",
      "step 259\n",
      "training loss: 2.86836576461792\n",
      "step 260\n",
      "training loss: 2.881934642791748\n",
      "validation loss: 2.9370813369750977\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8825674057006836\n",
      "step 262\n",
      "training loss: 2.871110677719116\n",
      "step 263\n",
      "training loss: 2.8530304431915283\n",
      "step 264\n",
      "training loss: 2.8783609867095947\n",
      "step 265\n",
      "training loss: 2.8788692951202393\n",
      "step 266\n",
      "training loss: 2.8702149391174316\n",
      "step 267\n",
      "training loss: 2.8656036853790283\n",
      "step 268\n",
      "training loss: 2.880173683166504\n",
      "step 269\n",
      "training loss: 2.8910012245178223\n",
      "step 270\n",
      "training loss: 2.809213638305664\n",
      "validation loss: 2.892936944961548\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8499903678894043\n",
      "----------3.0 min per epoch----------\n",
      "epoch 8\n",
      "step 0\n",
      "training loss: 2.818636178970337\n",
      "validation loss: 2.8386635780334473\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8527441024780273\n",
      "step 2\n",
      "training loss: 2.882432699203491\n",
      "step 3\n",
      "training loss: 2.8778421878814697\n",
      "step 4\n",
      "training loss: 2.881880521774292\n",
      "step 5\n",
      "training loss: 2.8768980503082275\n",
      "step 6\n",
      "training loss: 2.79657244682312\n",
      "step 7\n",
      "training loss: 2.75679874420166\n",
      "step 8\n",
      "training loss: 2.8699567317962646\n",
      "step 9\n",
      "training loss: 2.895054578781128\n",
      "step 10\n",
      "training loss: 2.874445676803589\n",
      "validation loss: 2.8564181327819824\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8510634899139404\n",
      "step 12\n",
      "training loss: 2.859750270843506\n",
      "step 13\n",
      "training loss: 2.878849506378174\n",
      "step 14\n",
      "training loss: 2.8781044483184814\n",
      "step 15\n",
      "training loss: 2.881213903427124\n",
      "step 16\n",
      "training loss: 2.864969253540039\n",
      "step 17\n",
      "training loss: 2.8825511932373047\n",
      "step 18\n",
      "training loss: 2.8808043003082275\n",
      "step 19\n",
      "training loss: 2.838575601577759\n",
      "step 20\n",
      "training loss: 2.8515563011169434\n",
      "validation loss: 2.9123528003692627\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.859881639480591\n",
      "step 22\n",
      "training loss: 2.8659861087799072\n",
      "step 23\n",
      "training loss: 2.872947931289673\n",
      "step 24\n",
      "training loss: 2.870694875717163\n",
      "step 25\n",
      "training loss: 2.8758437633514404\n",
      "step 26\n",
      "training loss: 2.874850273132324\n",
      "step 27\n",
      "training loss: 2.8700549602508545\n",
      "step 28\n",
      "training loss: 2.8877463340759277\n",
      "step 29\n",
      "training loss: 2.8667633533477783\n",
      "step 30\n",
      "training loss: 2.873105764389038\n",
      "validation loss: 2.8379878997802734\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8854098320007324\n",
      "step 32\n",
      "training loss: 2.880155563354492\n",
      "step 33\n",
      "training loss: 2.8767077922821045\n",
      "step 34\n",
      "training loss: 2.8750486373901367\n",
      "step 35\n",
      "training loss: 2.8588738441467285\n",
      "step 36\n",
      "training loss: 2.8427631855010986\n",
      "step 37\n",
      "training loss: 2.8701021671295166\n",
      "step 38\n",
      "training loss: 2.8779988288879395\n",
      "step 39\n",
      "training loss: 2.8417751789093018\n",
      "step 40\n",
      "training loss: 2.863199472427368\n",
      "validation loss: 2.862445116043091\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8710694313049316\n",
      "step 42\n",
      "training loss: 2.855072498321533\n",
      "step 43\n",
      "training loss: 2.8836426734924316\n",
      "step 44\n",
      "training loss: 2.85762095451355\n",
      "step 45\n",
      "training loss: 2.873352527618408\n",
      "step 46\n",
      "training loss: 2.8566782474517822\n",
      "step 47\n",
      "training loss: 2.872894287109375\n",
      "step 48\n",
      "training loss: 2.8250014781951904\n",
      "step 49\n",
      "training loss: 2.8846359252929688\n",
      "step 50\n",
      "training loss: 2.8527536392211914\n",
      "validation loss: 2.8665215969085693\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8828225135803223\n",
      "step 52\n",
      "training loss: 2.8699941635131836\n",
      "step 53\n",
      "training loss: 2.8478145599365234\n",
      "step 54\n",
      "training loss: 2.8458962440490723\n",
      "step 55\n",
      "training loss: 2.8676705360412598\n",
      "step 56\n",
      "training loss: 2.8562686443328857\n",
      "step 57\n",
      "training loss: 2.850029945373535\n",
      "step 58\n",
      "training loss: 2.8861663341522217\n",
      "step 59\n",
      "training loss: 2.8689446449279785\n",
      "step 60\n",
      "training loss: 2.862212657928467\n",
      "validation loss: 2.8787052631378174\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8459389209747314\n",
      "step 62\n",
      "training loss: 2.8608486652374268\n",
      "step 63\n",
      "training loss: 2.861048460006714\n",
      "step 64\n",
      "training loss: 2.8380026817321777\n",
      "step 65\n",
      "training loss: 2.8510355949401855\n",
      "step 66\n",
      "training loss: 2.8527731895446777\n",
      "step 67\n",
      "training loss: 2.8465256690979004\n",
      "step 68\n",
      "training loss: 2.840369701385498\n",
      "step 69\n",
      "training loss: 2.8758692741394043\n",
      "step 70\n",
      "training loss: 2.852287530899048\n",
      "validation loss: 2.9157278537750244\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8835878372192383\n",
      "step 72\n",
      "training loss: 2.861330986022949\n",
      "step 73\n",
      "training loss: 2.840144395828247\n",
      "step 74\n",
      "training loss: 2.877993106842041\n",
      "step 75\n",
      "training loss: 2.857299327850342\n",
      "step 76\n",
      "training loss: 2.8641271591186523\n",
      "step 77\n",
      "training loss: 2.86661434173584\n",
      "step 78\n",
      "training loss: 2.8382773399353027\n",
      "step 79\n",
      "training loss: 2.880628824234009\n",
      "step 80\n",
      "training loss: 2.8581390380859375\n",
      "validation loss: 2.8661019802093506\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8777360916137695\n",
      "step 82\n",
      "training loss: 2.8727550506591797\n",
      "step 83\n",
      "training loss: 2.8958818912506104\n",
      "step 84\n",
      "training loss: 2.8793530464172363\n",
      "step 85\n",
      "training loss: 2.8391435146331787\n",
      "step 86\n",
      "training loss: 2.8749148845672607\n",
      "step 87\n",
      "training loss: 2.861741542816162\n",
      "step 88\n",
      "training loss: 2.852085590362549\n",
      "step 89\n",
      "training loss: 2.8611464500427246\n",
      "step 90\n",
      "training loss: 2.857529401779175\n",
      "validation loss: 2.8641202449798584\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.878594160079956\n",
      "step 92\n",
      "training loss: 2.864943742752075\n",
      "step 93\n",
      "training loss: 2.8800745010375977\n",
      "step 94\n",
      "training loss: 2.880018711090088\n",
      "step 95\n",
      "training loss: 2.8802688121795654\n",
      "step 96\n",
      "training loss: 2.8584494590759277\n",
      "step 97\n",
      "training loss: 2.8694474697113037\n",
      "step 98\n",
      "training loss: 2.8674185276031494\n",
      "step 99\n",
      "training loss: 2.858419895172119\n",
      "step 100\n",
      "training loss: 2.869093894958496\n",
      "validation loss: 2.826500654220581\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8461921215057373\n",
      "step 102\n",
      "training loss: 2.8718512058258057\n",
      "step 103\n",
      "training loss: 2.8750226497650146\n",
      "step 104\n",
      "training loss: 2.81898832321167\n",
      "step 105\n",
      "training loss: 2.858452796936035\n",
      "step 106\n",
      "training loss: 2.854593276977539\n",
      "step 107\n",
      "training loss: 2.867928981781006\n",
      "step 108\n",
      "training loss: 2.874939441680908\n",
      "step 109\n",
      "training loss: 2.847583770751953\n",
      "step 110\n",
      "training loss: 2.870128870010376\n",
      "validation loss: 2.83419132232666\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8259832859039307\n",
      "step 112\n",
      "training loss: 2.8565895557403564\n",
      "step 113\n",
      "training loss: 2.876005172729492\n",
      "step 114\n",
      "training loss: 2.8670971393585205\n",
      "step 115\n",
      "training loss: 2.8598711490631104\n",
      "step 116\n",
      "training loss: 2.852198600769043\n",
      "step 117\n",
      "training loss: 2.869614362716675\n",
      "step 118\n",
      "training loss: 2.854672908782959\n",
      "step 119\n",
      "training loss: 2.862837076187134\n",
      "step 120\n",
      "training loss: 2.8726320266723633\n",
      "validation loss: 2.8385696411132812\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.863348960876465\n",
      "step 122\n",
      "training loss: 2.858505964279175\n",
      "step 123\n",
      "training loss: 2.861585855484009\n",
      "step 124\n",
      "training loss: 2.8399698734283447\n",
      "step 125\n",
      "training loss: 2.8419861793518066\n",
      "step 126\n",
      "training loss: 2.8347432613372803\n",
      "step 127\n",
      "training loss: 2.868860960006714\n",
      "step 128\n",
      "training loss: 2.872211217880249\n",
      "step 129\n",
      "training loss: 2.8624255657196045\n",
      "step 130\n",
      "training loss: 2.85971999168396\n",
      "validation loss: 2.8589794635772705\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.854464530944824\n",
      "step 132\n",
      "training loss: 2.8755593299865723\n",
      "step 133\n",
      "training loss: 2.8941805362701416\n",
      "step 134\n",
      "training loss: 2.810722827911377\n",
      "step 135\n",
      "training loss: 2.8624067306518555\n",
      "step 136\n",
      "training loss: 2.853635311126709\n",
      "step 137\n",
      "training loss: 2.880417823791504\n",
      "step 138\n",
      "training loss: 2.859539747238159\n",
      "step 139\n",
      "training loss: 2.881580114364624\n",
      "step 140\n",
      "training loss: 2.8554086685180664\n",
      "validation loss: 2.8699049949645996\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.840695381164551\n",
      "step 142\n",
      "training loss: 2.8573760986328125\n",
      "step 143\n",
      "training loss: 2.868718385696411\n",
      "step 144\n",
      "training loss: 2.8516433238983154\n",
      "step 145\n",
      "training loss: 2.8786494731903076\n",
      "step 146\n",
      "training loss: 2.867640495300293\n",
      "step 147\n",
      "training loss: 2.827357292175293\n",
      "step 148\n",
      "training loss: 2.8536221981048584\n",
      "step 149\n",
      "training loss: 2.8541736602783203\n",
      "step 150\n",
      "training loss: 2.859386444091797\n",
      "validation loss: 2.862259864807129\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8705835342407227\n",
      "step 152\n",
      "training loss: 2.865683078765869\n",
      "step 153\n",
      "training loss: 2.8695731163024902\n",
      "step 154\n",
      "training loss: 2.81937313079834\n",
      "step 155\n",
      "training loss: 2.8655216693878174\n",
      "step 156\n",
      "training loss: 2.856732130050659\n",
      "step 157\n",
      "training loss: 2.866849184036255\n",
      "step 158\n",
      "training loss: 2.8663992881774902\n",
      "step 159\n",
      "training loss: 2.875734806060791\n",
      "step 160\n",
      "training loss: 2.873499631881714\n",
      "validation loss: 2.86331844329834\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8538448810577393\n",
      "step 162\n",
      "training loss: 2.8819711208343506\n",
      "step 163\n",
      "training loss: 2.8753416538238525\n",
      "step 164\n",
      "training loss: 2.8656809329986572\n",
      "step 165\n",
      "training loss: 2.8601675033569336\n",
      "step 166\n",
      "training loss: 2.871919870376587\n",
      "step 167\n",
      "training loss: 2.8600757122039795\n",
      "step 168\n",
      "training loss: 2.8610453605651855\n",
      "step 169\n",
      "training loss: 2.842045783996582\n",
      "step 170\n",
      "training loss: 2.82541561126709\n",
      "validation loss: 2.868864059448242\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.869741916656494\n",
      "step 172\n",
      "training loss: 2.8514418601989746\n",
      "step 173\n",
      "training loss: 2.85280704498291\n",
      "step 174\n",
      "training loss: 2.8669188022613525\n",
      "step 175\n",
      "training loss: 2.8974404335021973\n",
      "step 176\n",
      "training loss: 2.845162868499756\n",
      "step 177\n",
      "training loss: 2.872020959854126\n",
      "step 178\n",
      "training loss: 2.862950086593628\n",
      "step 179\n",
      "training loss: 2.8736045360565186\n",
      "step 180\n",
      "training loss: 2.8498663902282715\n",
      "validation loss: 2.848552703857422\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8623735904693604\n",
      "step 182\n",
      "training loss: 2.836585521697998\n",
      "step 183\n",
      "training loss: 2.8546528816223145\n",
      "step 184\n",
      "training loss: 2.8597521781921387\n",
      "step 185\n",
      "training loss: 2.8470163345336914\n",
      "step 186\n",
      "training loss: 2.8849472999572754\n",
      "step 187\n",
      "training loss: 2.8607661724090576\n",
      "step 188\n",
      "training loss: 2.854680299758911\n",
      "step 189\n",
      "training loss: 2.8697071075439453\n",
      "step 190\n",
      "training loss: 2.867969274520874\n",
      "validation loss: 2.8473927974700928\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8563685417175293\n",
      "step 192\n",
      "training loss: 2.8448286056518555\n",
      "step 193\n",
      "training loss: 2.833562135696411\n",
      "step 194\n",
      "training loss: 2.8189618587493896\n",
      "step 195\n",
      "training loss: 2.879352331161499\n",
      "step 196\n",
      "training loss: 2.853752374649048\n",
      "step 197\n",
      "training loss: 2.880242109298706\n",
      "step 198\n",
      "training loss: 2.8573601245880127\n",
      "step 199\n",
      "training loss: 2.8681459426879883\n",
      "step 200\n",
      "training loss: 2.8807342052459717\n",
      "validation loss: 2.834623336791992\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.858560085296631\n",
      "step 202\n",
      "training loss: 2.855442523956299\n",
      "step 203\n",
      "training loss: 2.8673505783081055\n",
      "step 204\n",
      "training loss: 2.8428096771240234\n",
      "step 205\n",
      "training loss: 2.8540878295898438\n",
      "step 206\n",
      "training loss: 2.847560405731201\n",
      "step 207\n",
      "training loss: 2.854116678237915\n",
      "step 208\n",
      "training loss: 2.8408243656158447\n",
      "step 209\n",
      "training loss: 2.8630881309509277\n",
      "step 210\n",
      "training loss: 2.8623642921447754\n",
      "validation loss: 2.8379476070404053\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.847822427749634\n",
      "step 212\n",
      "training loss: 2.8721625804901123\n",
      "step 213\n",
      "training loss: 2.854027271270752\n",
      "step 214\n",
      "training loss: 2.8642659187316895\n",
      "step 215\n",
      "training loss: 2.86152982711792\n",
      "step 216\n",
      "training loss: 2.8672780990600586\n",
      "step 217\n",
      "training loss: 2.7936015129089355\n",
      "step 218\n",
      "training loss: 2.850149393081665\n",
      "step 219\n",
      "training loss: 2.837416887283325\n",
      "step 220\n",
      "training loss: 2.8475711345672607\n",
      "validation loss: 2.872889995574951\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8730406761169434\n",
      "step 222\n",
      "training loss: 2.8740875720977783\n",
      "step 223\n",
      "training loss: 2.8647422790527344\n",
      "step 224\n",
      "training loss: 2.8641204833984375\n",
      "step 225\n",
      "training loss: 2.8615074157714844\n",
      "step 226\n",
      "training loss: 2.8502113819122314\n",
      "step 227\n",
      "training loss: 2.8559134006500244\n",
      "step 228\n",
      "training loss: 2.870877265930176\n",
      "step 229\n",
      "training loss: 2.8884897232055664\n",
      "step 230\n",
      "training loss: 2.8579254150390625\n",
      "validation loss: 2.8790488243103027\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8485422134399414\n",
      "step 232\n",
      "training loss: 2.849339246749878\n",
      "step 233\n",
      "training loss: 2.8488407135009766\n",
      "step 234\n",
      "training loss: 2.8613479137420654\n",
      "step 235\n",
      "training loss: 2.8663947582244873\n",
      "step 236\n",
      "training loss: 2.8568122386932373\n",
      "step 237\n",
      "training loss: 2.8684895038604736\n",
      "step 238\n",
      "training loss: 2.87522554397583\n",
      "step 239\n",
      "training loss: 2.8498387336730957\n",
      "step 240\n",
      "training loss: 2.874107599258423\n",
      "validation loss: 2.832379102706909\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.878138780593872\n",
      "step 242\n",
      "training loss: 2.844419479370117\n",
      "step 243\n",
      "training loss: 2.848531723022461\n",
      "step 244\n",
      "training loss: 2.872490882873535\n",
      "step 245\n",
      "training loss: 2.8655567169189453\n",
      "step 246\n",
      "training loss: 2.866088628768921\n",
      "step 247\n",
      "training loss: 2.866783857345581\n",
      "step 248\n",
      "training loss: 2.8710649013519287\n",
      "step 249\n",
      "training loss: 2.860507011413574\n",
      "step 250\n",
      "training loss: 2.8663947582244873\n",
      "validation loss: 2.84501576423645\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8645405769348145\n",
      "step 252\n",
      "training loss: 2.8642890453338623\n",
      "step 253\n",
      "training loss: 2.8611342906951904\n",
      "step 254\n",
      "training loss: 2.8239035606384277\n",
      "step 255\n",
      "training loss: 2.851208448410034\n",
      "step 256\n",
      "training loss: 2.8697612285614014\n",
      "step 257\n",
      "training loss: 2.851456642150879\n",
      "step 258\n",
      "training loss: 2.8581461906433105\n",
      "step 259\n",
      "training loss: 2.889575242996216\n",
      "step 260\n",
      "training loss: 2.8661205768585205\n",
      "validation loss: 2.8848962783813477\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.881303071975708\n",
      "step 262\n",
      "training loss: 2.881532669067383\n",
      "step 263\n",
      "training loss: 2.8718271255493164\n",
      "step 264\n",
      "training loss: 2.8494820594787598\n",
      "step 265\n",
      "training loss: 2.879117965698242\n",
      "step 266\n",
      "training loss: 2.882490634918213\n",
      "step 267\n",
      "training loss: 2.870452642440796\n",
      "step 268\n",
      "training loss: 2.8681235313415527\n",
      "step 269\n",
      "training loss: 2.878787040710449\n",
      "step 270\n",
      "training loss: 2.8866260051727295\n",
      "validation loss: 2.8854172229766846\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.810309886932373\n",
      "----------3.0 min per epoch----------\n",
      "epoch 9\n",
      "step 0\n",
      "training loss: 2.8473892211914062\n",
      "validation loss: 2.874629020690918\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.817086935043335\n",
      "step 2\n",
      "training loss: 2.852137804031372\n",
      "step 3\n",
      "training loss: 2.8880560398101807\n",
      "step 4\n",
      "training loss: 2.880635976791382\n",
      "step 5\n",
      "training loss: 2.8869717121124268\n",
      "step 6\n",
      "training loss: 2.8785879611968994\n",
      "step 7\n",
      "training loss: 2.805551528930664\n",
      "step 8\n",
      "training loss: 2.755918264389038\n",
      "step 9\n",
      "training loss: 2.8566067218780518\n",
      "step 10\n",
      "training loss: 2.8856518268585205\n",
      "validation loss: 2.9341468811035156\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.873297929763794\n",
      "step 12\n",
      "training loss: 2.8518927097320557\n",
      "step 13\n",
      "training loss: 2.860184669494629\n",
      "step 14\n",
      "training loss: 2.8779914379119873\n",
      "step 15\n",
      "training loss: 2.875955581665039\n",
      "step 16\n",
      "training loss: 2.8819265365600586\n",
      "step 17\n",
      "training loss: 2.863649606704712\n",
      "step 18\n",
      "training loss: 2.880664825439453\n",
      "step 19\n",
      "training loss: 2.8785510063171387\n",
      "step 20\n",
      "training loss: 2.838139295578003\n",
      "validation loss: 2.8915815353393555\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8480982780456543\n",
      "step 22\n",
      "training loss: 2.8596580028533936\n",
      "step 23\n",
      "training loss: 2.8635900020599365\n",
      "step 24\n",
      "training loss: 2.8752431869506836\n",
      "step 25\n",
      "training loss: 2.86637544631958\n",
      "step 26\n",
      "training loss: 2.87542986869812\n",
      "step 27\n",
      "training loss: 2.876063108444214\n",
      "step 28\n",
      "training loss: 2.8720948696136475\n",
      "step 29\n",
      "training loss: 2.887585401535034\n",
      "step 30\n",
      "training loss: 2.865220308303833\n",
      "validation loss: 2.844831943511963\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8761634826660156\n",
      "step 32\n",
      "training loss: 2.8858261108398438\n",
      "step 33\n",
      "training loss: 2.880070686340332\n",
      "step 34\n",
      "training loss: 2.875420570373535\n",
      "step 35\n",
      "training loss: 2.877305030822754\n",
      "step 36\n",
      "training loss: 2.8584249019622803\n",
      "step 37\n",
      "training loss: 2.842391014099121\n",
      "step 38\n",
      "training loss: 2.8656599521636963\n",
      "step 39\n",
      "training loss: 2.8753340244293213\n",
      "step 40\n",
      "training loss: 2.8390591144561768\n",
      "validation loss: 2.865041732788086\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.865431308746338\n",
      "step 42\n",
      "training loss: 2.8697054386138916\n",
      "step 43\n",
      "training loss: 2.85162353515625\n",
      "step 44\n",
      "training loss: 2.8835179805755615\n",
      "step 45\n",
      "training loss: 2.8553988933563232\n",
      "step 46\n",
      "training loss: 2.875004768371582\n",
      "step 47\n",
      "training loss: 2.8559956550598145\n",
      "step 48\n",
      "training loss: 2.8714029788970947\n",
      "step 49\n",
      "training loss: 2.822988271713257\n",
      "step 50\n",
      "training loss: 2.884840726852417\n",
      "validation loss: 2.9015369415283203\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8507578372955322\n",
      "step 52\n",
      "training loss: 2.884803295135498\n",
      "step 53\n",
      "training loss: 2.867264986038208\n",
      "step 54\n",
      "training loss: 2.848703145980835\n",
      "step 55\n",
      "training loss: 2.8445656299591064\n",
      "step 56\n",
      "training loss: 2.8660809993743896\n",
      "step 57\n",
      "training loss: 2.855936050415039\n",
      "step 58\n",
      "training loss: 2.8504347801208496\n",
      "step 59\n",
      "training loss: 2.8823163509368896\n",
      "step 60\n",
      "training loss: 2.8693253993988037\n",
      "validation loss: 2.8290278911590576\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8589489459991455\n",
      "step 62\n",
      "training loss: 2.8436617851257324\n",
      "step 63\n",
      "training loss: 2.8603174686431885\n",
      "step 64\n",
      "training loss: 2.859365940093994\n",
      "step 65\n",
      "training loss: 2.8362011909484863\n",
      "step 66\n",
      "training loss: 2.8506948947906494\n",
      "step 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.852001905441284\n",
      "step 68\n",
      "training loss: 2.8444371223449707\n",
      "step 69\n",
      "training loss: 2.836027145385742\n",
      "step 70\n",
      "training loss: 2.871952533721924\n",
      "validation loss: 2.8596479892730713\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8482277393341064\n",
      "step 72\n",
      "training loss: 2.8825759887695312\n",
      "step 73\n",
      "training loss: 2.864739179611206\n",
      "step 74\n",
      "training loss: 2.837594747543335\n",
      "step 75\n",
      "training loss: 2.878777265548706\n",
      "step 76\n",
      "training loss: 2.8537778854370117\n",
      "step 77\n",
      "training loss: 2.8631112575531006\n",
      "step 78\n",
      "training loss: 2.8662643432617188\n",
      "step 79\n",
      "training loss: 2.8382551670074463\n",
      "step 80\n",
      "training loss: 2.8768370151519775\n",
      "validation loss: 2.8667256832122803\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.859714984893799\n",
      "step 82\n",
      "training loss: 2.879004955291748\n",
      "step 83\n",
      "training loss: 2.872551918029785\n",
      "step 84\n",
      "training loss: 2.8936753273010254\n",
      "step 85\n",
      "training loss: 2.8803749084472656\n",
      "step 86\n",
      "training loss: 2.836221218109131\n",
      "step 87\n",
      "training loss: 2.876047134399414\n",
      "step 88\n",
      "training loss: 2.8597214221954346\n",
      "step 89\n",
      "training loss: 2.8480241298675537\n",
      "step 90\n",
      "training loss: 2.8572630882263184\n",
      "validation loss: 2.8745360374450684\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.856254816055298\n",
      "step 92\n",
      "training loss: 2.876455783843994\n",
      "step 93\n",
      "training loss: 2.8615565299987793\n",
      "step 94\n",
      "training loss: 2.8795228004455566\n",
      "step 95\n",
      "training loss: 2.8797736167907715\n",
      "step 96\n",
      "training loss: 2.8805699348449707\n",
      "step 97\n",
      "training loss: 2.855761766433716\n",
      "step 98\n",
      "training loss: 2.8659815788269043\n",
      "step 99\n",
      "training loss: 2.864474296569824\n",
      "step 100\n",
      "training loss: 2.85916805267334\n",
      "validation loss: 2.9154014587402344\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8676917552948\n",
      "step 102\n",
      "training loss: 2.8421313762664795\n",
      "step 103\n",
      "training loss: 2.86965274810791\n",
      "step 104\n",
      "training loss: 2.8726892471313477\n",
      "step 105\n",
      "training loss: 2.81794810295105\n",
      "step 106\n",
      "training loss: 2.8583285808563232\n",
      "step 107\n",
      "training loss: 2.849363327026367\n",
      "step 108\n",
      "training loss: 2.8640356063842773\n",
      "step 109\n",
      "training loss: 2.872891902923584\n",
      "step 110\n",
      "training loss: 2.8474085330963135\n",
      "validation loss: 2.867541790008545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8680520057678223\n",
      "step 112\n",
      "training loss: 2.822556972503662\n",
      "step 113\n",
      "training loss: 2.8537545204162598\n",
      "step 114\n",
      "training loss: 2.873800039291382\n",
      "step 115\n",
      "training loss: 2.867746114730835\n",
      "step 116\n",
      "training loss: 2.8602237701416016\n",
      "step 117\n",
      "training loss: 2.849200487136841\n",
      "step 118\n",
      "training loss: 2.870882511138916\n",
      "step 119\n",
      "training loss: 2.8534433841705322\n",
      "step 120\n",
      "training loss: 2.8630001544952393\n",
      "validation loss: 2.864305257797241\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8724639415740967\n",
      "step 122\n",
      "training loss: 2.861678123474121\n",
      "step 123\n",
      "training loss: 2.8579773902893066\n",
      "step 124\n",
      "training loss: 2.8596608638763428\n",
      "step 125\n",
      "training loss: 2.8377063274383545\n",
      "step 126\n",
      "training loss: 2.844052314758301\n",
      "step 127\n",
      "training loss: 2.8334035873413086\n",
      "step 128\n",
      "training loss: 2.8692843914031982\n",
      "step 129\n",
      "training loss: 2.8696887493133545\n",
      "step 130\n",
      "training loss: 2.8627052307128906\n",
      "validation loss: 2.8255295753479004\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.862222671508789\n",
      "step 132\n",
      "training loss: 2.851557731628418\n",
      "step 133\n",
      "training loss: 2.875997543334961\n",
      "step 134\n",
      "training loss: 2.8918564319610596\n",
      "step 135\n",
      "training loss: 2.8107333183288574\n",
      "step 136\n",
      "training loss: 2.863367795944214\n",
      "step 137\n",
      "training loss: 2.8527772426605225\n",
      "step 138\n",
      "training loss: 2.878540277481079\n",
      "step 139\n",
      "training loss: 2.860053300857544\n",
      "step 140\n",
      "training loss: 2.8793630599975586\n",
      "validation loss: 2.834678888320923\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8547050952911377\n",
      "step 142\n",
      "training loss: 2.8411693572998047\n",
      "step 143\n",
      "training loss: 2.8573176860809326\n",
      "step 144\n",
      "training loss: 2.866201400756836\n",
      "step 145\n",
      "training loss: 2.8500640392303467\n",
      "step 146\n",
      "training loss: 2.877487897872925\n",
      "step 147\n",
      "training loss: 2.8642287254333496\n",
      "step 148\n",
      "training loss: 2.8266351222991943\n",
      "step 149\n",
      "training loss: 2.8528945446014404\n",
      "step 150\n",
      "training loss: 2.854034423828125\n",
      "validation loss: 2.8336355686187744\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.856764316558838\n",
      "step 152\n",
      "training loss: 2.867405414581299\n",
      "step 153\n",
      "training loss: 2.866990327835083\n",
      "step 154\n",
      "training loss: 2.8695666790008545\n",
      "step 155\n",
      "training loss: 2.8190524578094482\n",
      "step 156\n",
      "training loss: 2.8655192852020264\n",
      "step 157\n",
      "training loss: 2.8579204082489014\n",
      "step 158\n",
      "training loss: 2.8669252395629883\n",
      "step 159\n",
      "training loss: 2.865110158920288\n",
      "step 160\n",
      "training loss: 2.8758621215820312\n",
      "validation loss: 2.8586435317993164\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.870577335357666\n",
      "step 162\n",
      "training loss: 2.8535001277923584\n",
      "step 163\n",
      "training loss: 2.880218267440796\n",
      "step 164\n",
      "training loss: 2.8759610652923584\n",
      "step 165\n",
      "training loss: 2.8655567169189453\n",
      "step 166\n",
      "training loss: 2.8600778579711914\n",
      "step 167\n",
      "training loss: 2.8696980476379395\n",
      "step 168\n",
      "training loss: 2.8599801063537598\n",
      "step 169\n",
      "training loss: 2.8618903160095215\n",
      "step 170\n",
      "training loss: 2.8402719497680664\n",
      "validation loss: 2.869720697402954\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.822838544845581\n",
      "step 172\n",
      "training loss: 2.8708412647247314\n",
      "step 173\n",
      "training loss: 2.851813554763794\n",
      "step 174\n",
      "training loss: 2.849794387817383\n",
      "step 175\n",
      "training loss: 2.8625879287719727\n",
      "step 176\n",
      "training loss: 2.8957197666168213\n",
      "step 177\n",
      "training loss: 2.846163511276245\n",
      "step 178\n",
      "training loss: 2.8719711303710938\n",
      "step 179\n",
      "training loss: 2.8648674488067627\n",
      "step 180\n",
      "training loss: 2.87261962890625\n",
      "validation loss: 2.8636810779571533\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8487637042999268\n",
      "step 182\n",
      "training loss: 2.861220121383667\n",
      "step 183\n",
      "training loss: 2.834585189819336\n",
      "step 184\n",
      "training loss: 2.8516812324523926\n",
      "step 185\n",
      "training loss: 2.860402822494507\n",
      "step 186\n",
      "training loss: 2.8463377952575684\n",
      "step 187\n",
      "training loss: 2.8847591876983643\n",
      "step 188\n",
      "training loss: 2.8595824241638184\n",
      "step 189\n",
      "training loss: 2.853294849395752\n",
      "step 190\n",
      "training loss: 2.8673501014709473\n",
      "validation loss: 2.860175371170044\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8669230937957764\n",
      "step 192\n",
      "training loss: 2.8557310104370117\n",
      "step 193\n",
      "training loss: 2.845769166946411\n",
      "step 194\n",
      "training loss: 2.8318428993225098\n",
      "step 195\n",
      "training loss: 2.8164210319519043\n",
      "step 196\n",
      "training loss: 2.8765854835510254\n",
      "step 197\n",
      "training loss: 2.8531153202056885\n",
      "step 198\n",
      "training loss: 2.882861375808716\n",
      "step 199\n",
      "training loss: 2.856865882873535\n",
      "step 200\n",
      "training loss: 2.8698983192443848\n",
      "validation loss: 2.8689258098602295\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.880162000656128\n",
      "step 202\n",
      "training loss: 2.8596010208129883\n",
      "step 203\n",
      "training loss: 2.8558714389801025\n",
      "step 204\n",
      "training loss: 2.868393659591675\n",
      "step 205\n",
      "training loss: 2.844010353088379\n",
      "step 206\n",
      "training loss: 2.854461431503296\n",
      "step 207\n",
      "training loss: 2.8460962772369385\n",
      "step 208\n",
      "training loss: 2.852367401123047\n",
      "step 209\n",
      "training loss: 2.8410608768463135\n",
      "step 210\n",
      "training loss: 2.8617138862609863\n",
      "validation loss: 2.843641996383667\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.863224983215332\n",
      "step 212\n",
      "training loss: 2.846992254257202\n",
      "step 213\n",
      "training loss: 2.8747308254241943\n",
      "step 214\n",
      "training loss: 2.855926513671875\n",
      "step 215\n",
      "training loss: 2.8632051944732666\n",
      "step 216\n",
      "training loss: 2.857354164123535\n",
      "step 217\n",
      "training loss: 2.8657431602478027\n",
      "step 218\n",
      "training loss: 2.791567802429199\n",
      "step 219\n",
      "training loss: 2.8512990474700928\n",
      "step 220\n",
      "training loss: 2.834305763244629\n",
      "validation loss: 2.844393730163574\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.847567081451416\n",
      "step 222\n",
      "training loss: 2.871577739715576\n",
      "step 223\n",
      "training loss: 2.8737666606903076\n",
      "step 224\n",
      "training loss: 2.863093614578247\n",
      "step 225\n",
      "training loss: 2.864842176437378\n",
      "step 226\n",
      "training loss: 2.86183500289917\n",
      "step 227\n",
      "training loss: 2.848961591720581\n",
      "step 228\n",
      "training loss: 2.8586039543151855\n",
      "step 229\n",
      "training loss: 2.8694510459899902\n",
      "step 230\n",
      "training loss: 2.887639045715332\n",
      "validation loss: 2.833037853240967\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8578388690948486\n",
      "step 232\n",
      "training loss: 2.846597909927368\n",
      "step 233\n",
      "training loss: 2.849137306213379\n",
      "step 234\n",
      "training loss: 2.84679913520813\n",
      "step 235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8629300594329834\n",
      "step 236\n",
      "training loss: 2.8649022579193115\n",
      "step 237\n",
      "training loss: 2.8566067218780518\n",
      "step 238\n",
      "training loss: 2.8680737018585205\n",
      "step 239\n",
      "training loss: 2.874706983566284\n",
      "step 240\n",
      "training loss: 2.8496358394622803\n",
      "validation loss: 2.8376033306121826\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.873753309249878\n",
      "step 242\n",
      "training loss: 2.878024101257324\n",
      "step 243\n",
      "training loss: 2.84675669670105\n",
      "step 244\n",
      "training loss: 2.8485522270202637\n",
      "step 245\n",
      "training loss: 2.8714654445648193\n",
      "step 246\n",
      "training loss: 2.866328477859497\n",
      "step 247\n",
      "training loss: 2.8646693229675293\n",
      "step 248\n",
      "training loss: 2.8645901679992676\n",
      "step 249\n",
      "training loss: 2.869697332382202\n",
      "step 250\n",
      "training loss: 2.8590924739837646\n",
      "validation loss: 2.8697052001953125\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.864380359649658\n",
      "step 252\n",
      "training loss: 2.8633921146392822\n",
      "step 253\n",
      "training loss: 2.859363317489624\n",
      "step 254\n",
      "training loss: 2.8584234714508057\n",
      "step 255\n",
      "training loss: 2.823789596557617\n",
      "step 256\n",
      "training loss: 2.8516387939453125\n",
      "step 257\n",
      "training loss: 2.8683524131774902\n",
      "step 258\n",
      "training loss: 2.8486862182617188\n",
      "step 259\n",
      "training loss: 2.857086658477783\n",
      "step 260\n",
      "training loss: 2.889446258544922\n",
      "validation loss: 2.879171371459961\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8661861419677734\n",
      "step 262\n",
      "training loss: 2.881897211074829\n",
      "step 263\n",
      "training loss: 2.8792881965637207\n",
      "step 264\n",
      "training loss: 2.870479106903076\n",
      "step 265\n",
      "training loss: 2.852848529815674\n",
      "step 266\n",
      "training loss: 2.8764498233795166\n",
      "step 267\n",
      "training loss: 2.879006862640381\n",
      "step 268\n",
      "training loss: 2.869792938232422\n",
      "step 269\n",
      "training loss: 2.8655736446380615\n",
      "step 270\n",
      "training loss: 2.8782687187194824\n",
      "validation loss: 2.8425371646881104\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.886993408203125\n",
      "----------3.0 min per epoch----------\n",
      "epoch 10\n",
      "step 0\n",
      "training loss: 2.8101797103881836\n",
      "validation loss: 2.8596081733703613\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.84989857673645\n",
      "step 2\n",
      "training loss: 2.8150634765625\n",
      "step 3\n",
      "training loss: 2.8529632091522217\n",
      "step 4\n",
      "training loss: 2.8832154273986816\n",
      "step 5\n",
      "training loss: 2.875347852706909\n",
      "step 6\n",
      "training loss: 2.8827693462371826\n",
      "step 7\n",
      "training loss: 2.877420425415039\n",
      "step 8\n",
      "training loss: 2.7993085384368896\n",
      "step 9\n",
      "training loss: 2.757181406021118\n",
      "step 10\n",
      "training loss: 2.8606481552124023\n",
      "validation loss: 2.887594699859619\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8873119354248047\n",
      "step 12\n",
      "training loss: 2.8749444484710693\n",
      "step 13\n",
      "training loss: 2.8516838550567627\n",
      "step 14\n",
      "training loss: 2.857786178588867\n",
      "step 15\n",
      "training loss: 2.872274160385132\n",
      "step 16\n",
      "training loss: 2.87730073928833\n",
      "step 17\n",
      "training loss: 2.8810973167419434\n",
      "step 18\n",
      "training loss: 2.8634846210479736\n",
      "step 19\n",
      "training loss: 2.8819472789764404\n",
      "step 20\n",
      "training loss: 2.8783392906188965\n",
      "validation loss: 2.8872408866882324\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8373849391937256\n",
      "step 22\n",
      "training loss: 2.846235513687134\n",
      "step 23\n",
      "training loss: 2.858795166015625\n",
      "step 24\n",
      "training loss: 2.8599603176116943\n",
      "step 25\n",
      "training loss: 2.8743438720703125\n",
      "step 26\n",
      "training loss: 2.8678698539733887\n",
      "step 27\n",
      "training loss: 2.873737096786499\n",
      "step 28\n",
      "training loss: 2.8762543201446533\n",
      "step 29\n",
      "training loss: 2.86885142326355\n",
      "step 30\n",
      "training loss: 2.8871099948883057\n",
      "validation loss: 2.8778152465820312\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.863905191421509\n",
      "step 32\n",
      "training loss: 2.876866102218628\n",
      "step 33\n",
      "training loss: 2.8859739303588867\n",
      "step 34\n",
      "training loss: 2.879842519760132\n",
      "step 35\n",
      "training loss: 2.8742055892944336\n",
      "step 36\n",
      "training loss: 2.8743534088134766\n",
      "step 37\n",
      "training loss: 2.8585221767425537\n",
      "step 38\n",
      "training loss: 2.839519739151001\n",
      "step 39\n",
      "training loss: 2.8651676177978516\n",
      "step 40\n",
      "training loss: 2.8749587535858154\n",
      "validation loss: 2.9343113899230957\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8405096530914307\n",
      "step 42\n",
      "training loss: 2.8638925552368164\n",
      "step 43\n",
      "training loss: 2.8647544384002686\n",
      "step 44\n",
      "training loss: 2.8520147800445557\n",
      "step 45\n",
      "training loss: 2.8822011947631836\n",
      "step 46\n",
      "training loss: 2.856106996536255\n",
      "step 47\n",
      "training loss: 2.873774290084839\n",
      "step 48\n",
      "training loss: 2.855862855911255\n",
      "step 49\n",
      "training loss: 2.869074821472168\n",
      "step 50\n",
      "training loss: 2.824495792388916\n",
      "validation loss: 2.8820157051086426\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8836886882781982\n",
      "step 52\n",
      "training loss: 2.846315622329712\n",
      "step 53\n",
      "training loss: 2.8843936920166016\n",
      "step 54\n",
      "training loss: 2.8670132160186768\n",
      "step 55\n",
      "training loss: 2.8459200859069824\n",
      "step 56\n",
      "training loss: 2.8440802097320557\n",
      "step 57\n",
      "training loss: 2.8656649589538574\n",
      "step 58\n",
      "training loss: 2.853113889694214\n",
      "step 59\n",
      "training loss: 2.849855899810791\n",
      "step 60\n",
      "training loss: 2.883680820465088\n",
      "validation loss: 2.850729465484619\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.86871337890625\n",
      "step 62\n",
      "training loss: 2.8587334156036377\n",
      "step 63\n",
      "training loss: 2.8427155017852783\n",
      "step 64\n",
      "training loss: 2.8591127395629883\n",
      "step 65\n",
      "training loss: 2.8589048385620117\n",
      "step 66\n",
      "training loss: 2.8348028659820557\n",
      "step 67\n",
      "training loss: 2.8503198623657227\n",
      "step 68\n",
      "training loss: 2.8510115146636963\n",
      "step 69\n",
      "training loss: 2.845390558242798\n",
      "step 70\n",
      "training loss: 2.8402671813964844\n",
      "validation loss: 2.863722562789917\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.872748851776123\n",
      "step 72\n",
      "training loss: 2.846060276031494\n",
      "step 73\n",
      "training loss: 2.8794639110565186\n",
      "step 74\n",
      "training loss: 2.862426519393921\n",
      "step 75\n",
      "training loss: 2.838958978652954\n",
      "step 76\n",
      "training loss: 2.877267599105835\n",
      "step 77\n",
      "training loss: 2.854602575302124\n",
      "step 78\n",
      "training loss: 2.8647279739379883\n",
      "step 79\n",
      "training loss: 2.8655149936676025\n",
      "step 80\n",
      "training loss: 2.8354218006134033\n",
      "validation loss: 2.8923628330230713\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.876791477203369\n",
      "step 82\n",
      "training loss: 2.8593170642852783\n",
      "step 83\n",
      "training loss: 2.876650333404541\n",
      "step 84\n",
      "training loss: 2.871864080429077\n",
      "step 85\n",
      "training loss: 2.8956618309020996\n",
      "step 86\n",
      "training loss: 2.8804104328155518\n",
      "step 87\n",
      "training loss: 2.836484670639038\n",
      "step 88\n",
      "training loss: 2.874619245529175\n",
      "step 89\n",
      "training loss: 2.859490394592285\n",
      "step 90\n",
      "training loss: 2.848820209503174\n",
      "validation loss: 2.8318405151367188\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.858137607574463\n",
      "step 92\n",
      "training loss: 2.855140447616577\n",
      "step 93\n",
      "training loss: 2.877312183380127\n",
      "step 94\n",
      "training loss: 2.8615214824676514\n",
      "step 95\n",
      "training loss: 2.8791916370391846\n",
      "step 96\n",
      "training loss: 2.880467414855957\n",
      "step 97\n",
      "training loss: 2.879707098007202\n",
      "step 98\n",
      "training loss: 2.8561723232269287\n",
      "step 99\n",
      "training loss: 2.867058277130127\n",
      "step 100\n",
      "training loss: 2.8623769283294678\n",
      "validation loss: 2.862175941467285\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8581113815307617\n",
      "step 102\n",
      "training loss: 2.8673791885375977\n",
      "step 103\n",
      "training loss: 2.8456857204437256\n",
      "step 104\n",
      "training loss: 2.87011456489563\n",
      "step 105\n",
      "training loss: 2.873720407485962\n",
      "step 106\n",
      "training loss: 2.8157269954681396\n",
      "step 107\n",
      "training loss: 2.857264280319214\n",
      "step 108\n",
      "training loss: 2.85135817527771\n",
      "step 109\n",
      "training loss: 2.8647029399871826\n",
      "step 110\n",
      "training loss: 2.87268328666687\n",
      "validation loss: 2.8633346557617188\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.846813678741455\n",
      "step 112\n",
      "training loss: 2.865135908126831\n",
      "step 113\n",
      "training loss: 2.818713665008545\n",
      "step 114\n",
      "training loss: 2.853653907775879\n",
      "step 115\n",
      "training loss: 2.8760154247283936\n",
      "step 116\n",
      "training loss: 2.866685152053833\n",
      "step 117\n",
      "training loss: 2.860680103302002\n",
      "step 118\n",
      "training loss: 2.852083921432495\n",
      "step 119\n",
      "training loss: 2.868070602416992\n",
      "step 120\n",
      "training loss: 2.8532776832580566\n",
      "validation loss: 2.8754184246063232\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8620524406433105\n",
      "step 122\n",
      "training loss: 2.8734679222106934\n",
      "step 123\n",
      "training loss: 2.861494779586792\n",
      "step 124\n",
      "training loss: 2.8590567111968994\n",
      "step 125\n",
      "training loss: 2.8612515926361084\n",
      "step 126\n",
      "training loss: 2.8384623527526855\n",
      "step 127\n",
      "training loss: 2.8402864933013916\n",
      "step 128\n",
      "training loss: 2.833293914794922\n",
      "step 129\n",
      "training loss: 2.8697867393493652\n",
      "step 130\n",
      "training loss: 2.869103193283081\n",
      "validation loss: 2.908623695373535\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8626506328582764\n",
      "step 132\n",
      "training loss: 2.860990524291992\n",
      "step 133\n",
      "training loss: 2.8534603118896484\n",
      "step 134\n",
      "training loss: 2.872591733932495\n",
      "step 135\n",
      "training loss: 2.8930277824401855\n",
      "step 136\n",
      "training loss: 2.8121237754821777\n",
      "step 137\n",
      "training loss: 2.8629848957061768\n",
      "step 138\n",
      "training loss: 2.85371732711792\n",
      "step 139\n",
      "training loss: 2.880955696105957\n",
      "step 140\n",
      "training loss: 2.8599233627319336\n",
      "validation loss: 2.8680548667907715\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.88057541847229\n",
      "step 142\n",
      "training loss: 2.8524169921875\n",
      "step 143\n",
      "training loss: 2.8417654037475586\n",
      "step 144\n",
      "training loss: 2.8566908836364746\n",
      "step 145\n",
      "training loss: 2.8684089183807373\n",
      "step 146\n",
      "training loss: 2.8519372940063477\n",
      "step 147\n",
      "training loss: 2.880066394805908\n",
      "step 148\n",
      "training loss: 2.863818883895874\n",
      "step 149\n",
      "training loss: 2.826843500137329\n",
      "step 150\n",
      "training loss: 2.85137939453125\n",
      "validation loss: 2.8611972332000732\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8549070358276367\n",
      "step 152\n",
      "training loss: 2.8569581508636475\n",
      "step 153\n",
      "training loss: 2.872169256210327\n",
      "step 154\n",
      "training loss: 2.8660218715667725\n",
      "step 155\n",
      "training loss: 2.8681623935699463\n",
      "step 156\n",
      "training loss: 2.8181633949279785\n",
      "step 157\n",
      "training loss: 2.8642520904541016\n",
      "step 158\n",
      "training loss: 2.8569259643554688\n",
      "step 159\n",
      "training loss: 2.865692377090454\n",
      "step 160\n",
      "training loss: 2.864152193069458\n",
      "validation loss: 2.8234598636627197\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8749020099639893\n",
      "step 162\n",
      "training loss: 2.8739748001098633\n",
      "step 163\n",
      "training loss: 2.8533360958099365\n",
      "step 164\n",
      "training loss: 2.879899024963379\n",
      "step 165\n",
      "training loss: 2.876612424850464\n",
      "step 166\n",
      "training loss: 2.865133047103882\n",
      "step 167\n",
      "training loss: 2.860138177871704\n",
      "step 168\n",
      "training loss: 2.870140790939331\n",
      "step 169\n",
      "training loss: 2.859039306640625\n",
      "step 170\n",
      "training loss: 2.8614861965179443\n",
      "validation loss: 2.831990957260132\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.839851140975952\n",
      "step 172\n",
      "training loss: 2.8270528316497803\n",
      "step 173\n",
      "training loss: 2.8692243099212646\n",
      "step 174\n",
      "training loss: 2.8523471355438232\n",
      "step 175\n",
      "training loss: 2.8504364490509033\n",
      "step 176\n",
      "training loss: 2.863929271697998\n",
      "step 177\n",
      "training loss: 2.8952176570892334\n",
      "step 178\n",
      "training loss: 2.844999074935913\n",
      "step 179\n",
      "training loss: 2.871857166290283\n",
      "step 180\n",
      "training loss: 2.866668462753296\n",
      "validation loss: 2.8357722759246826\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8724498748779297\n",
      "step 182\n",
      "training loss: 2.850168466567993\n",
      "step 183\n",
      "training loss: 2.861207962036133\n",
      "step 184\n",
      "training loss: 2.8377065658569336\n",
      "step 185\n",
      "training loss: 2.8531198501586914\n",
      "step 186\n",
      "training loss: 2.8597893714904785\n",
      "step 187\n",
      "training loss: 2.8468782901763916\n",
      "step 188\n",
      "training loss: 2.888530969619751\n",
      "step 189\n",
      "training loss: 2.8605215549468994\n",
      "step 190\n",
      "training loss: 2.8525476455688477\n",
      "validation loss: 2.8579955101013184\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.868140459060669\n",
      "step 192\n",
      "training loss: 2.8665850162506104\n",
      "step 193\n",
      "training loss: 2.8554065227508545\n",
      "step 194\n",
      "training loss: 2.8455796241760254\n",
      "step 195\n",
      "training loss: 2.833298921585083\n",
      "step 196\n",
      "training loss: 2.8227012157440186\n",
      "step 197\n",
      "training loss: 2.8780362606048584\n",
      "step 198\n",
      "training loss: 2.854421615600586\n",
      "step 199\n",
      "training loss: 2.884235143661499\n",
      "step 200\n",
      "training loss: 2.8580775260925293\n",
      "validation loss: 2.8707234859466553\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8679275512695312\n",
      "step 202\n",
      "training loss: 2.882087469100952\n",
      "step 203\n",
      "training loss: 2.8581950664520264\n",
      "step 204\n",
      "training loss: 2.855501890182495\n",
      "step 205\n",
      "training loss: 2.8672587871551514\n",
      "step 206\n",
      "training loss: 2.843568801879883\n",
      "step 207\n",
      "training loss: 2.854581117630005\n",
      "step 208\n",
      "training loss: 2.8469398021698\n",
      "step 209\n",
      "training loss: 2.8517394065856934\n",
      "step 210\n",
      "training loss: 2.841343879699707\n",
      "validation loss: 2.8607444763183594\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.860823631286621\n",
      "step 212\n",
      "training loss: 2.8639535903930664\n",
      "step 213\n",
      "training loss: 2.8456521034240723\n",
      "step 214\n",
      "training loss: 2.8750360012054443\n",
      "step 215\n",
      "training loss: 2.8533549308776855\n",
      "step 216\n",
      "training loss: 2.862048625946045\n",
      "step 217\n",
      "training loss: 2.859391689300537\n",
      "step 218\n",
      "training loss: 2.8657991886138916\n",
      "step 219\n",
      "training loss: 2.7894248962402344\n",
      "step 220\n",
      "training loss: 2.851778507232666\n",
      "validation loss: 2.860517978668213\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8356759548187256\n",
      "step 222\n",
      "training loss: 2.8533308506011963\n",
      "step 223\n",
      "training loss: 2.872814178466797\n",
      "step 224\n",
      "training loss: 2.871433734893799\n",
      "step 225\n",
      "training loss: 2.864165782928467\n",
      "step 226\n",
      "training loss: 2.8622822761535645\n",
      "step 227\n",
      "training loss: 2.860562801361084\n",
      "step 228\n",
      "training loss: 2.84889817237854\n",
      "step 229\n",
      "training loss: 2.8550853729248047\n",
      "step 230\n",
      "training loss: 2.867147922515869\n",
      "validation loss: 2.862332820892334\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8859753608703613\n",
      "step 232\n",
      "training loss: 2.8576362133026123\n",
      "step 233\n",
      "training loss: 2.847620725631714\n",
      "step 234\n",
      "training loss: 2.8476688861846924\n",
      "step 235\n",
      "training loss: 2.847003698348999\n",
      "step 236\n",
      "training loss: 2.8612265586853027\n",
      "step 237\n",
      "training loss: 2.8623828887939453\n",
      "step 238\n",
      "training loss: 2.857909917831421\n",
      "step 239\n",
      "training loss: 2.8696370124816895\n",
      "step 240\n",
      "training loss: 2.872528076171875\n",
      "validation loss: 2.8422200679779053\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8476505279541016\n",
      "step 242\n",
      "training loss: 2.8748385906219482\n",
      "step 243\n",
      "training loss: 2.878464937210083\n",
      "step 244\n",
      "training loss: 2.8481242656707764\n",
      "step 245\n",
      "training loss: 2.8485326766967773\n",
      "step 246\n",
      "training loss: 2.869795083999634\n",
      "step 247\n",
      "training loss: 2.865180015563965\n",
      "step 248\n",
      "training loss: 2.8654754161834717\n",
      "step 249\n",
      "training loss: 2.86655330657959\n",
      "step 250\n",
      "training loss: 2.8705310821533203\n",
      "validation loss: 2.8455440998077393\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8593692779541016\n",
      "step 252\n",
      "training loss: 2.863966464996338\n",
      "step 253\n",
      "training loss: 2.864006280899048\n",
      "step 254\n",
      "training loss: 2.8610973358154297\n",
      "step 255\n",
      "training loss: 2.8573954105377197\n",
      "step 256\n",
      "training loss: 2.823822259902954\n",
      "step 257\n",
      "training loss: 2.851524829864502\n",
      "step 258\n",
      "training loss: 2.869112968444824\n",
      "step 259\n",
      "training loss: 2.84570050239563\n",
      "step 260\n",
      "training loss: 2.8530220985412598\n",
      "validation loss: 2.8319175243377686\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.88944411277771\n",
      "step 262\n",
      "training loss: 2.864678144454956\n",
      "step 263\n",
      "training loss: 2.882797956466675\n",
      "step 264\n",
      "training loss: 2.8824357986450195\n",
      "step 265\n",
      "training loss: 2.869710683822632\n",
      "step 266\n",
      "training loss: 2.853161096572876\n",
      "step 267\n",
      "training loss: 2.876290798187256\n",
      "step 268\n",
      "training loss: 2.879607677459717\n",
      "step 269\n",
      "training loss: 2.8691396713256836\n",
      "step 270\n",
      "training loss: 2.86486554145813\n",
      "validation loss: 2.8416566848754883\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.879911184310913\n",
      "----------3.0 min per epoch----------\n",
      "epoch 11\n",
      "step 0\n",
      "training loss: 2.8861734867095947\n",
      "validation loss: 2.879465341567993\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8087072372436523\n",
      "step 2\n",
      "training loss: 2.848623275756836\n",
      "step 3\n",
      "training loss: 2.8121848106384277\n",
      "step 4\n",
      "training loss: 2.8550078868865967\n",
      "step 5\n",
      "training loss: 2.8844149112701416\n",
      "step 6\n",
      "training loss: 2.877044439315796\n",
      "step 7\n",
      "training loss: 2.884955406188965\n",
      "step 8\n",
      "training loss: 2.876110076904297\n",
      "step 9\n",
      "training loss: 2.7960376739501953\n",
      "step 10\n",
      "training loss: 2.754674196243286\n",
      "validation loss: 2.886258602142334\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.863579273223877\n",
      "step 12\n",
      "training loss: 2.8844850063323975\n",
      "step 13\n",
      "training loss: 2.873281717300415\n",
      "step 14\n",
      "training loss: 2.8498260974884033\n",
      "step 15\n",
      "training loss: 2.8595023155212402\n",
      "step 16\n",
      "training loss: 2.874659538269043\n",
      "step 17\n",
      "training loss: 2.878227949142456\n",
      "step 18\n",
      "training loss: 2.8800055980682373\n",
      "step 19\n",
      "training loss: 2.864896774291992\n",
      "step 20\n",
      "training loss: 2.8826661109924316\n",
      "validation loss: 2.8410158157348633\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8773446083068848\n",
      "step 22\n",
      "training loss: 2.8369247913360596\n",
      "step 23\n",
      "training loss: 2.8442039489746094\n",
      "step 24\n",
      "training loss: 2.859069347381592\n",
      "step 25\n",
      "training loss: 2.860408306121826\n",
      "step 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8755149841308594\n",
      "step 27\n",
      "training loss: 2.8708384037017822\n",
      "step 28\n",
      "training loss: 2.874042272567749\n",
      "step 29\n",
      "training loss: 2.8753416538238525\n",
      "step 30\n",
      "training loss: 2.86834716796875\n",
      "validation loss: 2.8552892208099365\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8838744163513184\n",
      "step 32\n",
      "training loss: 2.867870569229126\n",
      "step 33\n",
      "training loss: 2.8760297298431396\n",
      "step 34\n",
      "training loss: 2.8864212036132812\n",
      "step 35\n",
      "training loss: 2.878674268722534\n",
      "step 36\n",
      "training loss: 2.874648332595825\n",
      "step 37\n",
      "training loss: 2.874525785446167\n",
      "step 38\n",
      "training loss: 2.861377716064453\n",
      "step 39\n",
      "training loss: 2.838832139968872\n",
      "step 40\n",
      "training loss: 2.8636553287506104\n",
      "validation loss: 2.8878321647644043\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.87392258644104\n",
      "step 42\n",
      "training loss: 2.8420042991638184\n",
      "step 43\n",
      "training loss: 2.8627748489379883\n",
      "step 44\n",
      "training loss: 2.8661162853240967\n",
      "step 45\n",
      "training loss: 2.852719783782959\n",
      "step 46\n",
      "training loss: 2.8826656341552734\n",
      "step 47\n",
      "training loss: 2.855017900466919\n",
      "step 48\n",
      "training loss: 2.873486280441284\n",
      "step 49\n",
      "training loss: 2.8562703132629395\n",
      "step 50\n",
      "training loss: 2.871962070465088\n",
      "validation loss: 2.878073215484619\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.824472665786743\n",
      "step 52\n",
      "training loss: 2.884171724319458\n",
      "step 53\n",
      "training loss: 2.845782518386841\n",
      "step 54\n",
      "training loss: 2.881834030151367\n",
      "step 55\n",
      "training loss: 2.8666415214538574\n",
      "step 56\n",
      "training loss: 2.84879994392395\n",
      "step 57\n",
      "training loss: 2.8429477214813232\n",
      "step 58\n",
      "training loss: 2.8652472496032715\n",
      "step 59\n",
      "training loss: 2.854469060897827\n",
      "step 60\n",
      "training loss: 2.848156452178955\n",
      "validation loss: 2.875972270965576\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8813140392303467\n",
      "step 62\n",
      "training loss: 2.868350028991699\n",
      "step 63\n",
      "training loss: 2.859668254852295\n",
      "step 64\n",
      "training loss: 2.845576286315918\n",
      "step 65\n",
      "training loss: 2.8606812953948975\n",
      "step 66\n",
      "training loss: 2.8607332706451416\n",
      "step 67\n",
      "training loss: 2.8335325717926025\n",
      "step 68\n",
      "training loss: 2.8511159420013428\n",
      "step 69\n",
      "training loss: 2.852863073348999\n",
      "step 70\n",
      "training loss: 2.844270706176758\n",
      "validation loss: 2.942244291305542\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8369932174682617\n",
      "step 72\n",
      "training loss: 2.8720767498016357\n",
      "step 73\n",
      "training loss: 2.8466551303863525\n",
      "step 74\n",
      "training loss: 2.8817362785339355\n",
      "step 75\n",
      "training loss: 2.861625909805298\n",
      "step 76\n",
      "training loss: 2.838127851486206\n",
      "step 77\n",
      "training loss: 2.877718210220337\n",
      "step 78\n",
      "training loss: 2.852731227874756\n",
      "step 79\n",
      "training loss: 2.862644910812378\n",
      "step 80\n",
      "training loss: 2.8675999641418457\n",
      "validation loss: 2.879425048828125\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8355462551116943\n",
      "step 82\n",
      "training loss: 2.877103805541992\n",
      "step 83\n",
      "training loss: 2.8614447116851807\n",
      "step 84\n",
      "training loss: 2.876533269882202\n",
      "step 85\n",
      "training loss: 2.8713016510009766\n",
      "step 86\n",
      "training loss: 2.8941640853881836\n",
      "step 87\n",
      "training loss: 2.878685712814331\n",
      "step 88\n",
      "training loss: 2.835359573364258\n",
      "step 89\n",
      "training loss: 2.876765489578247\n",
      "step 90\n",
      "training loss: 2.8563504219055176\n",
      "validation loss: 2.848008394241333\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8501827716827393\n",
      "step 92\n",
      "training loss: 2.857149600982666\n",
      "step 93\n",
      "training loss: 2.8563034534454346\n",
      "step 94\n",
      "training loss: 2.876926898956299\n",
      "step 95\n",
      "training loss: 2.8603389263153076\n",
      "step 96\n",
      "training loss: 2.8777060508728027\n",
      "step 97\n",
      "training loss: 2.8782153129577637\n",
      "step 98\n",
      "training loss: 2.8783955574035645\n",
      "step 99\n",
      "training loss: 2.8543365001678467\n",
      "step 100\n",
      "training loss: 2.8686017990112305\n",
      "validation loss: 2.8653833866119385\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8619277477264404\n",
      "step 102\n",
      "training loss: 2.8563780784606934\n",
      "step 103\n",
      "training loss: 2.868497133255005\n",
      "step 104\n",
      "training loss: 2.844555377960205\n",
      "step 105\n",
      "training loss: 2.869231939315796\n",
      "step 106\n",
      "training loss: 2.8752825260162354\n",
      "step 107\n",
      "training loss: 2.8158178329467773\n",
      "step 108\n",
      "training loss: 2.8580610752105713\n",
      "step 109\n",
      "training loss: 2.8509650230407715\n",
      "step 110\n",
      "training loss: 2.8648860454559326\n",
      "validation loss: 2.895444393157959\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8724052906036377\n",
      "step 112\n",
      "training loss: 2.8449316024780273\n",
      "step 113\n",
      "training loss: 2.864893913269043\n",
      "step 114\n",
      "training loss: 2.8185088634490967\n",
      "step 115\n",
      "training loss: 2.852889060974121\n",
      "step 116\n",
      "training loss: 2.874619483947754\n",
      "step 117\n",
      "training loss: 2.8663129806518555\n",
      "step 118\n",
      "training loss: 2.8601582050323486\n",
      "step 119\n",
      "training loss: 2.8523664474487305\n",
      "step 120\n",
      "training loss: 2.8703079223632812\n",
      "validation loss: 2.8285441398620605\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8536252975463867\n",
      "step 122\n",
      "training loss: 2.859774351119995\n",
      "step 123\n",
      "training loss: 2.8686070442199707\n",
      "step 124\n",
      "training loss: 2.860776662826538\n",
      "step 125\n",
      "training loss: 2.8594632148742676\n",
      "step 126\n",
      "training loss: 2.861447334289551\n",
      "step 127\n",
      "training loss: 2.8386361598968506\n",
      "step 128\n",
      "training loss: 2.8418679237365723\n",
      "step 129\n",
      "training loss: 2.833704710006714\n",
      "step 130\n",
      "training loss: 2.86773419380188\n",
      "validation loss: 2.857052803039551\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8710362911224365\n",
      "step 132\n",
      "training loss: 2.8632876873016357\n",
      "step 133\n",
      "training loss: 2.8622305393218994\n",
      "step 134\n",
      "training loss: 2.852813959121704\n",
      "step 135\n",
      "training loss: 2.8761608600616455\n",
      "step 136\n",
      "training loss: 2.892106771469116\n",
      "step 137\n",
      "training loss: 2.81156587600708\n",
      "step 138\n",
      "training loss: 2.8617234230041504\n",
      "step 139\n",
      "training loss: 2.853878974914551\n",
      "step 140\n",
      "training loss: 2.8771753311157227\n",
      "validation loss: 2.865234136581421\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8561015129089355\n",
      "step 142\n",
      "training loss: 2.8808462619781494\n",
      "step 143\n",
      "training loss: 2.8549821376800537\n",
      "step 144\n",
      "training loss: 2.841428279876709\n",
      "step 145\n",
      "training loss: 2.8547136783599854\n",
      "step 146\n",
      "training loss: 2.8650739192962646\n",
      "step 147\n",
      "training loss: 2.8510520458221436\n",
      "step 148\n",
      "training loss: 2.8777811527252197\n",
      "step 149\n",
      "training loss: 2.863461494445801\n",
      "step 150\n",
      "training loss: 2.8264272212982178\n",
      "validation loss: 2.871453046798706\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8517770767211914\n",
      "step 152\n",
      "training loss: 2.854569911956787\n",
      "step 153\n",
      "training loss: 2.855832815170288\n",
      "step 154\n",
      "training loss: 2.8686177730560303\n",
      "step 155\n",
      "training loss: 2.866781711578369\n",
      "step 156\n",
      "training loss: 2.8682942390441895\n",
      "step 157\n",
      "training loss: 2.8155055046081543\n",
      "step 158\n",
      "training loss: 2.8646929264068604\n",
      "step 159\n",
      "training loss: 2.8577098846435547\n",
      "step 160\n",
      "training loss: 2.8644042015075684\n",
      "validation loss: 2.9085028171539307\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8638975620269775\n",
      "step 162\n",
      "training loss: 2.876445770263672\n",
      "step 163\n",
      "training loss: 2.8716349601745605\n",
      "step 164\n",
      "training loss: 2.8531765937805176\n",
      "step 165\n",
      "training loss: 2.878967761993408\n",
      "step 166\n",
      "training loss: 2.8752849102020264\n",
      "step 167\n",
      "training loss: 2.864736318588257\n",
      "step 168\n",
      "training loss: 2.8596110343933105\n",
      "step 169\n",
      "training loss: 2.8698272705078125\n",
      "step 170\n",
      "training loss: 2.8603596687316895\n",
      "validation loss: 2.864586353302002\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.862218141555786\n",
      "step 172\n",
      "training loss: 2.842069149017334\n",
      "step 173\n",
      "training loss: 2.8232922554016113\n",
      "step 174\n",
      "training loss: 2.870518207550049\n",
      "step 175\n",
      "training loss: 2.8495945930480957\n",
      "step 176\n",
      "training loss: 2.8496084213256836\n",
      "step 177\n",
      "training loss: 2.8656227588653564\n",
      "step 178\n",
      "training loss: 2.8942387104034424\n",
      "step 179\n",
      "training loss: 2.8452465534210205\n",
      "step 180\n",
      "training loss: 2.8716371059417725\n",
      "validation loss: 2.8637654781341553\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8649826049804688\n",
      "step 182\n",
      "training loss: 2.871774435043335\n",
      "step 183\n",
      "training loss: 2.8481826782226562\n",
      "step 184\n",
      "training loss: 2.8603646755218506\n",
      "step 185\n",
      "training loss: 2.8351004123687744\n",
      "step 186\n",
      "training loss: 2.850529909133911\n",
      "step 187\n",
      "training loss: 2.8602964878082275\n",
      "step 188\n",
      "training loss: 2.844972610473633\n",
      "step 189\n",
      "training loss: 2.885345458984375\n",
      "step 190\n",
      "training loss: 2.8597779273986816\n",
      "validation loss: 2.8245041370391846\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.852814197540283\n",
      "step 192\n",
      "training loss: 2.866077423095703\n",
      "step 193\n",
      "training loss: 2.8647594451904297\n",
      "step 194\n",
      "training loss: 2.8534531593322754\n",
      "step 195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.842832088470459\n",
      "step 196\n",
      "training loss: 2.830279588699341\n",
      "step 197\n",
      "training loss: 2.8161003589630127\n",
      "step 198\n",
      "training loss: 2.877310037612915\n",
      "step 199\n",
      "training loss: 2.85365629196167\n",
      "step 200\n",
      "training loss: 2.8810698986053467\n",
      "validation loss: 2.8309249877929688\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8563992977142334\n",
      "step 202\n",
      "training loss: 2.8663947582244873\n",
      "step 203\n",
      "training loss: 2.8824520111083984\n",
      "step 204\n",
      "training loss: 2.8562707901000977\n",
      "step 205\n",
      "training loss: 2.8553335666656494\n",
      "step 206\n",
      "training loss: 2.8673157691955566\n",
      "step 207\n",
      "training loss: 2.8439972400665283\n",
      "step 208\n",
      "training loss: 2.8565878868103027\n",
      "step 209\n",
      "training loss: 2.8449647426605225\n",
      "step 210\n",
      "training loss: 2.8496835231781006\n",
      "validation loss: 2.834674835205078\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.841022491455078\n",
      "step 212\n",
      "training loss: 2.8595852851867676\n",
      "step 213\n",
      "training loss: 2.863034248352051\n",
      "step 214\n",
      "training loss: 2.8444812297821045\n",
      "step 215\n",
      "training loss: 2.8729772567749023\n",
      "step 216\n",
      "training loss: 2.8533992767333984\n",
      "step 217\n",
      "training loss: 2.8598995208740234\n",
      "step 218\n",
      "training loss: 2.8586843013763428\n",
      "step 219\n",
      "training loss: 2.8646042346954346\n",
      "step 220\n",
      "training loss: 2.7915802001953125\n",
      "validation loss: 2.858139991760254\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8508074283599854\n",
      "step 222\n",
      "training loss: 2.833078145980835\n",
      "step 223\n",
      "training loss: 2.8502347469329834\n",
      "step 224\n",
      "training loss: 2.87272047996521\n",
      "step 225\n",
      "training loss: 2.869706392288208\n",
      "step 226\n",
      "training loss: 2.862360715866089\n",
      "step 227\n",
      "training loss: 2.8617606163024902\n",
      "step 228\n",
      "training loss: 2.861830949783325\n",
      "step 229\n",
      "training loss: 2.8475921154022217\n",
      "step 230\n",
      "training loss: 2.855396270751953\n",
      "validation loss: 2.8682637214660645\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8674750328063965\n",
      "step 232\n",
      "training loss: 2.888441324234009\n",
      "step 233\n",
      "training loss: 2.8569931983947754\n",
      "step 234\n",
      "training loss: 2.847687244415283\n",
      "step 235\n",
      "training loss: 2.8492653369903564\n",
      "step 236\n",
      "training loss: 2.847536087036133\n",
      "step 237\n",
      "training loss: 2.862126111984253\n",
      "step 238\n",
      "training loss: 2.860539674758911\n",
      "step 239\n",
      "training loss: 2.8569297790527344\n",
      "step 240\n",
      "training loss: 2.8674144744873047\n",
      "validation loss: 2.861534357070923\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8715388774871826\n",
      "step 242\n",
      "training loss: 2.8489222526550293\n",
      "step 243\n",
      "training loss: 2.87459135055542\n",
      "step 244\n",
      "training loss: 2.876453399658203\n",
      "step 245\n",
      "training loss: 2.845778465270996\n",
      "step 246\n",
      "training loss: 2.845773935317993\n",
      "step 247\n",
      "training loss: 2.872183084487915\n",
      "step 248\n",
      "training loss: 2.8644943237304688\n",
      "step 249\n",
      "training loss: 2.8637399673461914\n",
      "step 250\n",
      "training loss: 2.8655965328216553\n",
      "validation loss: 2.860607862472534\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.869980573654175\n",
      "step 252\n",
      "training loss: 2.8592076301574707\n",
      "step 253\n",
      "training loss: 2.8635897636413574\n",
      "step 254\n",
      "training loss: 2.861340284347534\n",
      "step 255\n",
      "training loss: 2.861323356628418\n",
      "step 256\n",
      "training loss: 2.8592610359191895\n",
      "step 257\n",
      "training loss: 2.8215935230255127\n",
      "step 258\n",
      "training loss: 2.8557019233703613\n",
      "step 259\n",
      "training loss: 2.8704776763916016\n",
      "step 260\n",
      "training loss: 2.8449184894561768\n",
      "validation loss: 2.864546060562134\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.852113962173462\n",
      "step 262\n",
      "training loss: 2.885671377182007\n",
      "step 263\n",
      "training loss: 2.8624298572540283\n",
      "step 264\n",
      "training loss: 2.8797051906585693\n",
      "step 265\n",
      "training loss: 2.8813929557800293\n",
      "step 266\n",
      "training loss: 2.869011402130127\n",
      "step 267\n",
      "training loss: 2.849299669265747\n",
      "step 268\n",
      "training loss: 2.878188371658325\n",
      "step 269\n",
      "training loss: 2.877415657043457\n",
      "step 270\n",
      "training loss: 2.868966579437256\n",
      "validation loss: 2.8517770767211914\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.865553617477417\n",
      "----------3.0 min per epoch----------\n",
      "epoch 12\n",
      "step 0\n",
      "training loss: 2.8779518604278564\n",
      "validation loss: 2.849468469619751\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.886422872543335\n",
      "step 2\n",
      "training loss: 2.807870388031006\n",
      "step 3\n",
      "training loss: 2.847411632537842\n",
      "step 4\n",
      "training loss: 2.815613269805908\n",
      "step 5\n",
      "training loss: 2.8530287742614746\n",
      "step 6\n",
      "training loss: 2.881863832473755\n",
      "step 7\n",
      "training loss: 2.8759539127349854\n",
      "step 8\n",
      "training loss: 2.881584644317627\n",
      "step 9\n",
      "training loss: 2.8732497692108154\n",
      "step 10\n",
      "training loss: 2.797428846359253\n",
      "validation loss: 2.8302643299102783\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.7617783546447754\n",
      "step 12\n",
      "training loss: 2.858424425125122\n",
      "step 13\n",
      "training loss: 2.8846685886383057\n",
      "step 14\n",
      "training loss: 2.8696651458740234\n",
      "step 15\n",
      "training loss: 2.8514461517333984\n",
      "step 16\n",
      "training loss: 2.8578128814697266\n",
      "step 17\n",
      "training loss: 2.8728792667388916\n",
      "step 18\n",
      "training loss: 2.8781533241271973\n",
      "step 19\n",
      "training loss: 2.8803515434265137\n",
      "step 20\n",
      "training loss: 2.8621716499328613\n",
      "validation loss: 2.8426826000213623\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8816776275634766\n",
      "step 22\n",
      "training loss: 2.8778574466705322\n",
      "step 23\n",
      "training loss: 2.836585521697998\n",
      "step 24\n",
      "training loss: 2.8450927734375\n",
      "step 25\n",
      "training loss: 2.8578314781188965\n",
      "step 26\n",
      "training loss: 2.859961986541748\n",
      "step 27\n",
      "training loss: 2.8759958744049072\n",
      "step 28\n",
      "training loss: 2.868522882461548\n",
      "step 29\n",
      "training loss: 2.8747494220733643\n",
      "step 30\n",
      "training loss: 2.87439227104187\n",
      "validation loss: 2.8718268871307373\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.867668867111206\n",
      "step 32\n",
      "training loss: 2.8844707012176514\n",
      "step 33\n",
      "training loss: 2.866413116455078\n",
      "step 34\n",
      "training loss: 2.8752875328063965\n",
      "step 35\n",
      "training loss: 2.8833072185516357\n",
      "step 36\n",
      "training loss: 2.879591703414917\n",
      "step 37\n",
      "training loss: 2.8748316764831543\n",
      "step 38\n",
      "training loss: 2.8744277954101562\n",
      "step 39\n",
      "training loss: 2.8599278926849365\n",
      "step 40\n",
      "training loss: 2.8411786556243896\n",
      "validation loss: 2.878319025039673\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8648886680603027\n",
      "step 42\n",
      "training loss: 2.873831033706665\n",
      "step 43\n",
      "training loss: 2.8405680656433105\n",
      "step 44\n",
      "training loss: 2.8636980056762695\n",
      "step 45\n",
      "training loss: 2.8651058673858643\n",
      "step 46\n",
      "training loss: 2.8511312007904053\n",
      "step 47\n",
      "training loss: 2.8826217651367188\n",
      "step 48\n",
      "training loss: 2.854588031768799\n",
      "step 49\n",
      "training loss: 2.8749358654022217\n",
      "step 50\n",
      "training loss: 2.8557586669921875\n",
      "validation loss: 2.832631826400757\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8709001541137695\n",
      "step 52\n",
      "training loss: 2.8219387531280518\n",
      "step 53\n",
      "training loss: 2.885099172592163\n",
      "step 54\n",
      "training loss: 2.8473196029663086\n",
      "step 55\n",
      "training loss: 2.88128662109375\n",
      "step 56\n",
      "training loss: 2.866575002670288\n",
      "step 57\n",
      "training loss: 2.84938907623291\n",
      "step 58\n",
      "training loss: 2.841912269592285\n",
      "step 59\n",
      "training loss: 2.8657889366149902\n",
      "step 60\n",
      "training loss: 2.855300188064575\n",
      "validation loss: 2.8550193309783936\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.844944953918457\n",
      "step 62\n",
      "training loss: 2.882335662841797\n",
      "step 63\n",
      "training loss: 2.869431495666504\n",
      "step 64\n",
      "training loss: 2.860356330871582\n",
      "step 65\n",
      "training loss: 2.8446786403656006\n",
      "step 66\n",
      "training loss: 2.861288547515869\n",
      "step 67\n",
      "training loss: 2.858819007873535\n",
      "step 68\n",
      "training loss: 2.8325417041778564\n",
      "step 69\n",
      "training loss: 2.851640224456787\n",
      "step 70\n",
      "training loss: 2.8502559661865234\n",
      "validation loss: 2.884145736694336\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8448104858398438\n",
      "step 72\n",
      "training loss: 2.8358070850372314\n",
      "step 73\n",
      "training loss: 2.870781660079956\n",
      "step 74\n",
      "training loss: 2.8489437103271484\n",
      "step 75\n",
      "training loss: 2.8807878494262695\n",
      "step 76\n",
      "training loss: 2.86077618598938\n",
      "step 77\n",
      "training loss: 2.8397347927093506\n",
      "step 78\n",
      "training loss: 2.877382516860962\n",
      "step 79\n",
      "training loss: 2.8524539470672607\n",
      "step 80\n",
      "training loss: 2.8614871501922607\n",
      "validation loss: 2.875145673751831\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.863905429840088\n",
      "step 82\n",
      "training loss: 2.8350648880004883\n",
      "step 83\n",
      "training loss: 2.875460386276245\n",
      "step 84\n",
      "training loss: 2.85695219039917\n",
      "step 85\n",
      "training loss: 2.876349925994873\n",
      "step 86\n",
      "training loss: 2.872810125350952\n",
      "step 87\n",
      "training loss: 2.8936057090759277\n",
      "step 88\n",
      "training loss: 2.878192663192749\n",
      "step 89\n",
      "training loss: 2.833810806274414\n",
      "step 90\n",
      "training loss: 2.8742623329162598\n",
      "validation loss: 2.873603105545044\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.857875347137451\n",
      "step 92\n",
      "training loss: 2.851381778717041\n",
      "step 93\n",
      "training loss: 2.859490156173706\n",
      "step 94\n",
      "training loss: 2.859220266342163\n",
      "step 95\n",
      "training loss: 2.8752121925354004\n",
      "step 96\n",
      "training loss: 2.859644651412964\n",
      "step 97\n",
      "training loss: 2.8778839111328125\n",
      "step 98\n",
      "training loss: 2.8785717487335205\n",
      "step 99\n",
      "training loss: 2.8791401386260986\n",
      "step 100\n",
      "training loss: 2.854923725128174\n",
      "validation loss: 2.938058376312256\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.868964910507202\n",
      "step 102\n",
      "training loss: 2.8639326095581055\n",
      "step 103\n",
      "training loss: 2.856088876724243\n",
      "step 104\n",
      "training loss: 2.868021249771118\n",
      "step 105\n",
      "training loss: 2.8425943851470947\n",
      "step 106\n",
      "training loss: 2.8688154220581055\n",
      "step 107\n",
      "training loss: 2.8745336532592773\n",
      "step 108\n",
      "training loss: 2.8140268325805664\n",
      "step 109\n",
      "training loss: 2.8565478324890137\n",
      "step 110\n",
      "training loss: 2.8489272594451904\n",
      "validation loss: 2.8784568309783936\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.864469289779663\n",
      "step 112\n",
      "training loss: 2.8722550868988037\n",
      "step 113\n",
      "training loss: 2.844906806945801\n",
      "step 114\n",
      "training loss: 2.8648648262023926\n",
      "step 115\n",
      "training loss: 2.8193979263305664\n",
      "step 116\n",
      "training loss: 2.8535642623901367\n",
      "step 117\n",
      "training loss: 2.8763537406921387\n",
      "step 118\n",
      "training loss: 2.865520477294922\n",
      "step 119\n",
      "training loss: 2.860959768295288\n",
      "step 120\n",
      "training loss: 2.8496313095092773\n",
      "validation loss: 2.8523917198181152\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.869460105895996\n",
      "step 122\n",
      "training loss: 2.853374719619751\n",
      "step 123\n",
      "training loss: 2.860771894454956\n",
      "step 124\n",
      "training loss: 2.868274450302124\n",
      "step 125\n",
      "training loss: 2.859827995300293\n",
      "step 126\n",
      "training loss: 2.8586816787719727\n",
      "step 127\n",
      "training loss: 2.8598780632019043\n",
      "step 128\n",
      "training loss: 2.839214563369751\n",
      "step 129\n",
      "training loss: 2.8390917778015137\n",
      "step 130\n",
      "training loss: 2.8306610584259033\n",
      "validation loss: 2.8586299419403076\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.868119478225708\n",
      "step 132\n",
      "training loss: 2.868873357772827\n",
      "step 133\n",
      "training loss: 2.8623714447021484\n",
      "step 134\n",
      "training loss: 2.860827684402466\n",
      "step 135\n",
      "training loss: 2.853436231613159\n",
      "step 136\n",
      "training loss: 2.87431263923645\n",
      "step 137\n",
      "training loss: 2.8911874294281006\n",
      "step 138\n",
      "training loss: 2.812352180480957\n",
      "step 139\n",
      "training loss: 2.8619141578674316\n",
      "step 140\n",
      "training loss: 2.8531205654144287\n",
      "validation loss: 2.88612699508667\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8786306381225586\n",
      "step 142\n",
      "training loss: 2.8575618267059326\n",
      "step 143\n",
      "training loss: 2.8795032501220703\n",
      "step 144\n",
      "training loss: 2.8538336753845215\n",
      "step 145\n",
      "training loss: 2.8419418334960938\n",
      "step 146\n",
      "training loss: 2.854875087738037\n",
      "step 147\n",
      "training loss: 2.866103172302246\n",
      "step 148\n",
      "training loss: 2.8500816822052\n",
      "step 149\n",
      "training loss: 2.8776211738586426\n",
      "step 150\n",
      "training loss: 2.8636584281921387\n",
      "validation loss: 2.8363451957702637\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.82940673828125\n",
      "step 152\n",
      "training loss: 2.8527767658233643\n",
      "step 153\n",
      "training loss: 2.8544540405273438\n",
      "step 154\n",
      "training loss: 2.8582987785339355\n",
      "step 155\n",
      "training loss: 2.8703861236572266\n",
      "step 156\n",
      "training loss: 2.867246389389038\n",
      "step 157\n",
      "training loss: 2.868669033050537\n",
      "step 158\n",
      "training loss: 2.818634510040283\n",
      "step 159\n",
      "training loss: 2.865445613861084\n",
      "step 160\n",
      "training loss: 2.856508493423462\n",
      "validation loss: 2.8582637310028076\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.865318536758423\n",
      "step 162\n",
      "training loss: 2.8644022941589355\n",
      "step 163\n",
      "training loss: 2.875826835632324\n",
      "step 164\n",
      "training loss: 2.871687650680542\n",
      "step 165\n",
      "training loss: 2.8538742065429688\n",
      "step 166\n",
      "training loss: 2.880563974380493\n",
      "step 167\n",
      "training loss: 2.875922203063965\n",
      "step 168\n",
      "training loss: 2.863507032394409\n",
      "step 169\n",
      "training loss: 2.861027479171753\n",
      "step 170\n",
      "training loss: 2.8700006008148193\n",
      "validation loss: 2.866264820098877\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8603150844573975\n",
      "step 172\n",
      "training loss: 2.859161853790283\n",
      "step 173\n",
      "training loss: 2.84181547164917\n",
      "step 174\n",
      "training loss: 2.8227686882019043\n",
      "step 175\n",
      "training loss: 2.867565631866455\n",
      "step 176\n",
      "training loss: 2.852618455886841\n",
      "step 177\n",
      "training loss: 2.848869800567627\n",
      "step 178\n",
      "training loss: 2.8667469024658203\n",
      "step 179\n",
      "training loss: 2.8951306343078613\n",
      "step 180\n",
      "training loss: 2.8431830406188965\n",
      "validation loss: 2.870950222015381\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8717284202575684\n",
      "step 182\n",
      "training loss: 2.8619625568389893\n",
      "step 183\n",
      "training loss: 2.872760057449341\n",
      "step 184\n",
      "training loss: 2.850476026535034\n",
      "step 185\n",
      "training loss: 2.8601126670837402\n",
      "step 186\n",
      "training loss: 2.8361146450042725\n",
      "step 187\n",
      "training loss: 2.8540313243865967\n",
      "step 188\n",
      "training loss: 2.8605127334594727\n",
      "step 189\n",
      "training loss: 2.8451738357543945\n",
      "step 190\n",
      "training loss: 2.886507987976074\n",
      "validation loss: 2.9127821922302246\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.861870288848877\n",
      "step 192\n",
      "training loss: 2.8522238731384277\n",
      "step 193\n",
      "training loss: 2.863701581954956\n",
      "step 194\n",
      "training loss: 2.8650076389312744\n",
      "step 195\n",
      "training loss: 2.854092836380005\n",
      "step 196\n",
      "training loss: 2.8439440727233887\n",
      "step 197\n",
      "training loss: 2.8331902027130127\n",
      "step 198\n",
      "training loss: 2.817301034927368\n",
      "step 199\n",
      "training loss: 2.877082586288452\n",
      "step 200\n",
      "training loss: 2.854109287261963\n",
      "validation loss: 2.869701623916626\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8806302547454834\n",
      "step 202\n",
      "training loss: 2.8550736904144287\n",
      "step 203\n",
      "training loss: 2.865466833114624\n",
      "step 204\n",
      "training loss: 2.880030632019043\n",
      "step 205\n",
      "training loss: 2.8593010902404785\n",
      "step 206\n",
      "training loss: 2.856783628463745\n",
      "step 207\n",
      "training loss: 2.8675670623779297\n",
      "step 208\n",
      "training loss: 2.843958616256714\n",
      "step 209\n",
      "training loss: 2.854341983795166\n",
      "step 210\n",
      "training loss: 2.8500561714172363\n",
      "validation loss: 2.8613927364349365\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8514440059661865\n",
      "step 212\n",
      "training loss: 2.840038776397705\n",
      "step 213\n",
      "training loss: 2.86197566986084\n",
      "step 214\n",
      "training loss: 2.8634471893310547\n",
      "step 215\n",
      "training loss: 2.8456227779388428\n",
      "step 216\n",
      "training loss: 2.874392032623291\n",
      "step 217\n",
      "training loss: 2.8563497066497803\n",
      "step 218\n",
      "training loss: 2.8635520935058594\n",
      "step 219\n",
      "training loss: 2.8622100353240967\n",
      "step 220\n",
      "training loss: 2.868311882019043\n",
      "validation loss: 2.8271961212158203\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.7895615100860596\n",
      "step 222\n",
      "training loss: 2.8518600463867188\n",
      "step 223\n",
      "training loss: 2.8316335678100586\n",
      "step 224\n",
      "training loss: 2.855759859085083\n",
      "step 225\n",
      "training loss: 2.8686716556549072\n",
      "step 226\n",
      "training loss: 2.8720901012420654\n",
      "step 227\n",
      "training loss: 2.8631551265716553\n",
      "step 228\n",
      "training loss: 2.863502264022827\n",
      "step 229\n",
      "training loss: 2.8612313270568848\n",
      "step 230\n",
      "training loss: 2.848609209060669\n",
      "validation loss: 2.832878589630127\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8555307388305664\n",
      "step 232\n",
      "training loss: 2.8711295127868652\n",
      "step 233\n",
      "training loss: 2.8868775367736816\n",
      "step 234\n",
      "training loss: 2.8564212322235107\n",
      "step 235\n",
      "training loss: 2.847109794616699\n",
      "step 236\n",
      "training loss: 2.8495893478393555\n",
      "step 237\n",
      "training loss: 2.8454482555389404\n",
      "step 238\n",
      "training loss: 2.8634390830993652\n",
      "step 239\n",
      "training loss: 2.865100383758545\n",
      "step 240\n",
      "training loss: 2.860037088394165\n",
      "validation loss: 2.835624933242798\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8709545135498047\n",
      "step 242\n",
      "training loss: 2.874394416809082\n",
      "step 243\n",
      "training loss: 2.8464951515197754\n",
      "step 244\n",
      "training loss: 2.8734593391418457\n",
      "step 245\n",
      "training loss: 2.8771095275878906\n",
      "step 246\n",
      "training loss: 2.8448452949523926\n",
      "step 247\n",
      "training loss: 2.847522497177124\n",
      "step 248\n",
      "training loss: 2.8719429969787598\n",
      "step 249\n",
      "training loss: 2.8644301891326904\n",
      "step 250\n",
      "training loss: 2.863095283508301\n",
      "validation loss: 2.8581290245056152\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.865344524383545\n",
      "step 252\n",
      "training loss: 2.8700807094573975\n",
      "step 253\n",
      "training loss: 2.859753370285034\n",
      "step 254\n",
      "training loss: 2.8640310764312744\n",
      "step 255\n",
      "training loss: 2.8628015518188477\n",
      "step 256\n",
      "training loss: 2.863396644592285\n",
      "step 257\n",
      "training loss: 2.8591227531433105\n",
      "step 258\n",
      "training loss: 2.8223860263824463\n",
      "step 259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8544607162475586\n",
      "step 260\n",
      "training loss: 2.872130870819092\n",
      "validation loss: 2.8685691356658936\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.845947265625\n",
      "step 262\n",
      "training loss: 2.8565914630889893\n",
      "step 263\n",
      "training loss: 2.8863134384155273\n",
      "step 264\n",
      "training loss: 2.866023540496826\n",
      "step 265\n",
      "training loss: 2.87899112701416\n",
      "step 266\n",
      "training loss: 2.8795430660247803\n",
      "step 267\n",
      "training loss: 2.8684446811676025\n",
      "step 268\n",
      "training loss: 2.849461793899536\n",
      "step 269\n",
      "training loss: 2.87949538230896\n",
      "step 270\n",
      "training loss: 2.8793015480041504\n",
      "validation loss: 2.8730297088623047\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.871115207672119\n",
      "----------3.0 min per epoch----------\n",
      "epoch 13\n",
      "step 0\n",
      "training loss: 2.866582155227661\n",
      "validation loss: 2.8708558082580566\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.878298044204712\n",
      "step 2\n",
      "training loss: 2.8865487575531006\n",
      "step 3\n",
      "training loss: 2.810659646987915\n",
      "step 4\n",
      "training loss: 2.849714994430542\n",
      "step 5\n",
      "training loss: 2.8164665699005127\n",
      "step 6\n",
      "training loss: 2.8564653396606445\n",
      "step 7\n",
      "training loss: 2.8842735290527344\n",
      "step 8\n",
      "training loss: 2.8772904872894287\n",
      "step 9\n",
      "training loss: 2.883831024169922\n",
      "step 10\n",
      "training loss: 2.8731865882873535\n",
      "validation loss: 2.867976427078247\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8078153133392334\n",
      "step 12\n",
      "training loss: 2.7706782817840576\n",
      "step 13\n",
      "training loss: 2.854416847229004\n",
      "step 14\n",
      "training loss: 2.89408540725708\n",
      "step 15\n",
      "training loss: 2.8704514503479004\n",
      "step 16\n",
      "training loss: 2.851813793182373\n",
      "step 17\n",
      "training loss: 2.860074520111084\n",
      "step 18\n",
      "training loss: 2.8734006881713867\n",
      "step 19\n",
      "training loss: 2.8782522678375244\n",
      "step 20\n",
      "training loss: 2.8807170391082764\n",
      "validation loss: 2.853907823562622\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8641135692596436\n",
      "step 22\n",
      "training loss: 2.8813369274139404\n",
      "step 23\n",
      "training loss: 2.878725528717041\n",
      "step 24\n",
      "training loss: 2.8386926651000977\n",
      "step 25\n",
      "training loss: 2.8432016372680664\n",
      "step 26\n",
      "training loss: 2.857348918914795\n",
      "step 27\n",
      "training loss: 2.860874891281128\n",
      "step 28\n",
      "training loss: 2.87320876121521\n",
      "step 29\n",
      "training loss: 2.868412494659424\n",
      "step 30\n",
      "training loss: 2.8736133575439453\n",
      "validation loss: 2.854027271270752\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.87652325630188\n",
      "step 32\n",
      "training loss: 2.8686447143554688\n",
      "step 33\n",
      "training loss: 2.8838706016540527\n",
      "step 34\n",
      "training loss: 2.868469476699829\n",
      "step 35\n",
      "training loss: 2.8749022483825684\n",
      "step 36\n",
      "training loss: 2.8863675594329834\n",
      "step 37\n",
      "training loss: 2.8786795139312744\n",
      "step 38\n",
      "training loss: 2.875809669494629\n",
      "step 39\n",
      "training loss: 2.875570058822632\n",
      "step 40\n",
      "training loss: 2.86169171333313\n",
      "validation loss: 2.8392257690429688\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8416242599487305\n",
      "step 42\n",
      "training loss: 2.863072633743286\n",
      "step 43\n",
      "training loss: 2.8748316764831543\n",
      "step 44\n",
      "training loss: 2.8442718982696533\n",
      "step 45\n",
      "training loss: 2.861135482788086\n",
      "step 46\n",
      "training loss: 2.8621909618377686\n",
      "step 47\n",
      "training loss: 2.851672887802124\n",
      "step 48\n",
      "training loss: 2.882255792617798\n",
      "step 49\n",
      "training loss: 2.8541173934936523\n",
      "step 50\n",
      "training loss: 2.871582508087158\n",
      "validation loss: 2.843935966491699\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.85776424407959\n",
      "step 52\n",
      "training loss: 2.872338056564331\n",
      "step 53\n",
      "training loss: 2.8275325298309326\n",
      "step 54\n",
      "training loss: 2.8833749294281006\n",
      "step 55\n",
      "training loss: 2.8488597869873047\n",
      "step 56\n",
      "training loss: 2.8803296089172363\n",
      "step 57\n",
      "training loss: 2.8674604892730713\n",
      "step 58\n",
      "training loss: 2.847667694091797\n",
      "step 59\n",
      "training loss: 2.8449957370758057\n",
      "step 60\n",
      "training loss: 2.864461660385132\n",
      "validation loss: 2.8724076747894287\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.856794595718384\n",
      "step 62\n",
      "training loss: 2.844855546951294\n",
      "step 63\n",
      "training loss: 2.878824234008789\n",
      "step 64\n",
      "training loss: 2.867835521697998\n",
      "step 65\n",
      "training loss: 2.8601272106170654\n",
      "step 66\n",
      "training loss: 2.846471071243286\n",
      "step 67\n",
      "training loss: 2.860131025314331\n",
      "step 68\n",
      "training loss: 2.8591864109039307\n",
      "step 69\n",
      "training loss: 2.833721160888672\n",
      "step 70\n",
      "training loss: 2.8508708477020264\n",
      "validation loss: 2.8755452632904053\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.851062536239624\n",
      "step 72\n",
      "training loss: 2.8455216884613037\n",
      "step 73\n",
      "training loss: 2.8371386528015137\n",
      "step 74\n",
      "training loss: 2.871422529220581\n",
      "step 75\n",
      "training loss: 2.846191644668579\n",
      "step 76\n",
      "training loss: 2.880920171737671\n",
      "step 77\n",
      "training loss: 2.862785577774048\n",
      "step 78\n",
      "training loss: 2.838046073913574\n",
      "step 79\n",
      "training loss: 2.8766276836395264\n",
      "step 80\n",
      "training loss: 2.852522134780884\n",
      "validation loss: 2.8316051959991455\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8605329990386963\n",
      "step 82\n",
      "training loss: 2.8627235889434814\n",
      "step 83\n",
      "training loss: 2.835322380065918\n",
      "step 84\n",
      "training loss: 2.8770604133605957\n",
      "step 85\n",
      "training loss: 2.855478525161743\n",
      "step 86\n",
      "training loss: 2.8781068325042725\n",
      "step 87\n",
      "training loss: 2.8706893920898438\n",
      "step 88\n",
      "training loss: 2.8933026790618896\n",
      "step 89\n",
      "training loss: 2.879201889038086\n",
      "step 90\n",
      "training loss: 2.8361291885375977\n",
      "validation loss: 2.8484036922454834\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8733625411987305\n",
      "step 92\n",
      "training loss: 2.858884334564209\n",
      "step 93\n",
      "training loss: 2.8487765789031982\n",
      "step 94\n",
      "training loss: 2.8572800159454346\n",
      "step 95\n",
      "training loss: 2.8575479984283447\n",
      "step 96\n",
      "training loss: 2.8757333755493164\n",
      "step 97\n",
      "training loss: 2.8609023094177246\n",
      "step 98\n",
      "training loss: 2.8773202896118164\n",
      "step 99\n",
      "training loss: 2.8789029121398926\n",
      "step 100\n",
      "training loss: 2.8796513080596924\n",
      "validation loss: 2.8840603828430176\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.854440450668335\n",
      "step 102\n",
      "training loss: 2.8677825927734375\n",
      "step 103\n",
      "training loss: 2.864319086074829\n",
      "step 104\n",
      "training loss: 2.8579771518707275\n",
      "step 105\n",
      "training loss: 2.868279457092285\n",
      "step 106\n",
      "training loss: 2.845017671585083\n",
      "step 107\n",
      "training loss: 2.8678321838378906\n",
      "step 108\n",
      "training loss: 2.8729002475738525\n",
      "step 109\n",
      "training loss: 2.8176639080047607\n",
      "step 110\n",
      "training loss: 2.857069730758667\n",
      "validation loss: 2.879503011703491\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.849759101867676\n",
      "step 112\n",
      "training loss: 2.865415573120117\n",
      "step 113\n",
      "training loss: 2.8724288940429688\n",
      "step 114\n",
      "training loss: 2.8443517684936523\n",
      "step 115\n",
      "training loss: 2.867516279220581\n",
      "step 116\n",
      "training loss: 2.8184146881103516\n",
      "step 117\n",
      "training loss: 2.853881359100342\n",
      "step 118\n",
      "training loss: 2.8764960765838623\n",
      "step 119\n",
      "training loss: 2.8654158115386963\n",
      "step 120\n",
      "training loss: 2.8610880374908447\n",
      "validation loss: 2.8682467937469482\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.853177547454834\n",
      "step 122\n",
      "training loss: 2.8658695220947266\n",
      "step 123\n",
      "training loss: 2.853222370147705\n",
      "step 124\n",
      "training loss: 2.8609066009521484\n",
      "step 125\n",
      "training loss: 2.8691818714141846\n",
      "step 126\n",
      "training loss: 2.8618762493133545\n",
      "step 127\n",
      "training loss: 2.856968641281128\n",
      "step 128\n",
      "training loss: 2.859321117401123\n",
      "step 129\n",
      "training loss: 2.8386809825897217\n",
      "step 130\n",
      "training loss: 2.842097520828247\n",
      "validation loss: 2.92942214012146\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8330447673797607\n",
      "step 132\n",
      "training loss: 2.868849039077759\n",
      "step 133\n",
      "training loss: 2.8688249588012695\n",
      "step 134\n",
      "training loss: 2.861877679824829\n",
      "step 135\n",
      "training loss: 2.8608453273773193\n",
      "step 136\n",
      "training loss: 2.85408353805542\n",
      "step 137\n",
      "training loss: 2.874220132827759\n",
      "step 138\n",
      "training loss: 2.8927643299102783\n",
      "step 139\n",
      "training loss: 2.811854600906372\n",
      "step 140\n",
      "training loss: 2.861863136291504\n",
      "validation loss: 2.8738279342651367\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.854473114013672\n",
      "step 142\n",
      "training loss: 2.8780696392059326\n",
      "step 143\n",
      "training loss: 2.858133316040039\n",
      "step 144\n",
      "training loss: 2.8810999393463135\n",
      "step 145\n",
      "training loss: 2.853191375732422\n",
      "step 146\n",
      "training loss: 2.845170259475708\n",
      "step 147\n",
      "training loss: 2.853881359100342\n",
      "step 148\n",
      "training loss: 2.8652281761169434\n",
      "step 149\n",
      "training loss: 2.8504648208618164\n",
      "step 150\n",
      "training loss: 2.877937078475952\n",
      "validation loss: 2.8574435710906982\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.861253261566162\n",
      "step 152\n",
      "training loss: 2.825587749481201\n",
      "step 153\n",
      "training loss: 2.8524887561798096\n",
      "step 154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8542678356170654\n",
      "step 155\n",
      "training loss: 2.8581063747406006\n",
      "step 156\n",
      "training loss: 2.8692169189453125\n",
      "step 157\n",
      "training loss: 2.8662171363830566\n",
      "step 158\n",
      "training loss: 2.867429256439209\n",
      "step 159\n",
      "training loss: 2.8178155422210693\n",
      "step 160\n",
      "training loss: 2.8645615577697754\n",
      "validation loss: 2.8619675636291504\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8576571941375732\n",
      "step 162\n",
      "training loss: 2.8670501708984375\n",
      "step 163\n",
      "training loss: 2.86389422416687\n",
      "step 164\n",
      "training loss: 2.875312328338623\n",
      "step 165\n",
      "training loss: 2.8724050521850586\n",
      "step 166\n",
      "training loss: 2.853909492492676\n",
      "step 167\n",
      "training loss: 2.879732608795166\n",
      "step 168\n",
      "training loss: 2.875821352005005\n",
      "step 169\n",
      "training loss: 2.8654541969299316\n",
      "step 170\n",
      "training loss: 2.862208843231201\n",
      "validation loss: 2.887432098388672\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8700215816497803\n",
      "step 172\n",
      "training loss: 2.859670877456665\n",
      "step 173\n",
      "training loss: 2.859388589859009\n",
      "step 174\n",
      "training loss: 2.8430354595184326\n",
      "step 175\n",
      "training loss: 2.823068618774414\n",
      "step 176\n",
      "training loss: 2.8696956634521484\n",
      "step 177\n",
      "training loss: 2.8488311767578125\n",
      "step 178\n",
      "training loss: 2.8474643230438232\n",
      "step 179\n",
      "training loss: 2.8654446601867676\n",
      "step 180\n",
      "training loss: 2.8941290378570557\n",
      "validation loss: 2.836852550506592\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8453404903411865\n",
      "step 182\n",
      "training loss: 2.8735973834991455\n",
      "step 183\n",
      "training loss: 2.860820770263672\n",
      "step 184\n",
      "training loss: 2.8703131675720215\n",
      "step 185\n",
      "training loss: 2.848090171813965\n",
      "step 186\n",
      "training loss: 2.861790895462036\n",
      "step 187\n",
      "training loss: 2.8379814624786377\n",
      "step 188\n",
      "training loss: 2.8556249141693115\n",
      "step 189\n",
      "training loss: 2.8608858585357666\n",
      "step 190\n",
      "training loss: 2.8447492122650146\n",
      "validation loss: 2.8616087436676025\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8870749473571777\n",
      "step 192\n",
      "training loss: 2.8600850105285645\n",
      "step 193\n",
      "training loss: 2.8513011932373047\n",
      "step 194\n",
      "training loss: 2.865086555480957\n",
      "step 195\n",
      "training loss: 2.865434408187866\n",
      "step 196\n",
      "training loss: 2.8539488315582275\n",
      "step 197\n",
      "training loss: 2.846006155014038\n",
      "step 198\n",
      "training loss: 2.831984519958496\n",
      "step 199\n",
      "training loss: 2.8170061111450195\n",
      "step 200\n",
      "training loss: 2.878617525100708\n",
      "validation loss: 2.8668437004089355\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8573312759399414\n",
      "step 202\n",
      "training loss: 2.8801379203796387\n",
      "step 203\n",
      "training loss: 2.855457305908203\n",
      "step 204\n",
      "training loss: 2.8668038845062256\n",
      "step 205\n",
      "training loss: 2.881962537765503\n",
      "step 206\n",
      "training loss: 2.8585314750671387\n",
      "step 207\n",
      "training loss: 2.857374668121338\n",
      "step 208\n",
      "training loss: 2.869163990020752\n",
      "step 209\n",
      "training loss: 2.845327615737915\n",
      "step 210\n",
      "training loss: 2.8550477027893066\n",
      "validation loss: 2.8746631145477295\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8476598262786865\n",
      "step 212\n",
      "training loss: 2.8512213230133057\n",
      "step 213\n",
      "training loss: 2.8420567512512207\n",
      "step 214\n",
      "training loss: 2.8614251613616943\n",
      "step 215\n",
      "training loss: 2.864067792892456\n",
      "step 216\n",
      "training loss: 2.8462295532226562\n",
      "step 217\n",
      "training loss: 2.874018430709839\n",
      "step 218\n",
      "training loss: 2.8537044525146484\n",
      "step 219\n",
      "training loss: 2.862619161605835\n",
      "step 220\n",
      "training loss: 2.8606765270233154\n",
      "validation loss: 2.9028308391571045\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.867431879043579\n",
      "step 222\n",
      "training loss: 2.79207444190979\n",
      "step 223\n",
      "training loss: 2.850886821746826\n",
      "step 224\n",
      "training loss: 2.8348021507263184\n",
      "step 225\n",
      "training loss: 2.8549017906188965\n",
      "step 226\n",
      "training loss: 2.8709330558776855\n",
      "step 227\n",
      "training loss: 2.8736112117767334\n",
      "step 228\n",
      "training loss: 2.864285945892334\n",
      "step 229\n",
      "training loss: 2.861398220062256\n",
      "step 230\n",
      "training loss: 2.861095666885376\n",
      "validation loss: 2.8683536052703857\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8481497764587402\n",
      "step 232\n",
      "training loss: 2.8559515476226807\n",
      "step 233\n",
      "training loss: 2.870915412902832\n",
      "step 234\n",
      "training loss: 2.887613296508789\n",
      "step 235\n",
      "training loss: 2.8550825119018555\n",
      "step 236\n",
      "training loss: 2.8473904132843018\n",
      "step 237\n",
      "training loss: 2.847646951675415\n",
      "step 238\n",
      "training loss: 2.8464109897613525\n",
      "step 239\n",
      "training loss: 2.861854314804077\n",
      "step 240\n",
      "training loss: 2.8633134365081787\n",
      "validation loss: 2.8623080253601074\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8564600944519043\n",
      "step 242\n",
      "training loss: 2.866605043411255\n",
      "step 243\n",
      "training loss: 2.8729116916656494\n",
      "step 244\n",
      "training loss: 2.849522113800049\n",
      "step 245\n",
      "training loss: 2.874652147293091\n",
      "step 246\n",
      "training loss: 2.877650499343872\n",
      "step 247\n",
      "training loss: 2.8441553115844727\n",
      "step 248\n",
      "training loss: 2.8497238159179688\n",
      "step 249\n",
      "training loss: 2.8699073791503906\n",
      "step 250\n",
      "training loss: 2.8655917644500732\n",
      "validation loss: 2.8272533416748047\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.86544132232666\n",
      "step 252\n",
      "training loss: 2.867281436920166\n",
      "step 253\n",
      "training loss: 2.8678414821624756\n",
      "step 254\n",
      "training loss: 2.8573827743530273\n",
      "step 255\n",
      "training loss: 2.867614269256592\n",
      "step 256\n",
      "training loss: 2.862166166305542\n",
      "step 257\n",
      "training loss: 2.86138653755188\n",
      "step 258\n",
      "training loss: 2.8589062690734863\n",
      "step 259\n",
      "training loss: 2.824450969696045\n",
      "step 260\n",
      "training loss: 2.8518753051757812\n",
      "validation loss: 2.835340738296509\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8732690811157227\n",
      "step 262\n",
      "training loss: 2.847282886505127\n",
      "step 263\n",
      "training loss: 2.857715368270874\n",
      "step 264\n",
      "training loss: 2.8890738487243652\n",
      "step 265\n",
      "training loss: 2.864628314971924\n",
      "step 266\n",
      "training loss: 2.881255865097046\n",
      "step 267\n",
      "training loss: 2.8805437088012695\n",
      "step 268\n",
      "training loss: 2.869065761566162\n",
      "step 269\n",
      "training loss: 2.848102331161499\n",
      "step 270\n",
      "training loss: 2.877568244934082\n",
      "validation loss: 2.841305732727051\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.877936840057373\n",
      "----------3.0 min per epoch----------\n",
      "epoch 14\n",
      "step 0\n",
      "training loss: 2.8699843883514404\n",
      "validation loss: 2.8695197105407715\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8661081790924072\n",
      "step 2\n",
      "training loss: 2.8779613971710205\n",
      "step 3\n",
      "training loss: 2.885622978210449\n",
      "step 4\n",
      "training loss: 2.809626579284668\n",
      "step 5\n",
      "training loss: 2.851755380630493\n",
      "step 6\n",
      "training loss: 2.8179867267608643\n",
      "step 7\n",
      "training loss: 2.852531671524048\n",
      "step 8\n",
      "training loss: 2.8820033073425293\n",
      "step 9\n",
      "training loss: 2.875955581665039\n",
      "step 10\n",
      "training loss: 2.886014938354492\n",
      "validation loss: 2.867509126663208\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8714609146118164\n",
      "step 12\n",
      "training loss: 2.808645486831665\n",
      "step 13\n",
      "training loss: 2.773955821990967\n",
      "step 14\n",
      "training loss: 2.8612587451934814\n",
      "step 15\n",
      "training loss: 2.889000415802002\n",
      "step 16\n",
      "training loss: 2.8712317943573\n",
      "step 17\n",
      "training loss: 2.8530824184417725\n",
      "step 18\n",
      "training loss: 2.8603482246398926\n",
      "step 19\n",
      "training loss: 2.874561309814453\n",
      "step 20\n",
      "training loss: 2.8751914501190186\n",
      "validation loss: 2.867845058441162\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.880317211151123\n",
      "step 22\n",
      "training loss: 2.8720521926879883\n",
      "step 23\n",
      "training loss: 2.8802335262298584\n",
      "step 24\n",
      "training loss: 2.879840612411499\n",
      "step 25\n",
      "training loss: 2.837695598602295\n",
      "step 26\n",
      "training loss: 2.8512470722198486\n",
      "step 27\n",
      "training loss: 2.8568501472473145\n",
      "step 28\n",
      "training loss: 2.8609392642974854\n",
      "step 29\n",
      "training loss: 2.8732707500457764\n",
      "step 30\n",
      "training loss: 2.8684282302856445\n",
      "validation loss: 2.8638100624084473\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.873771905899048\n",
      "step 32\n",
      "training loss: 2.87577223777771\n",
      "step 33\n",
      "training loss: 2.869730234146118\n",
      "step 34\n",
      "training loss: 2.8842296600341797\n",
      "step 35\n",
      "training loss: 2.868891954421997\n",
      "step 36\n",
      "training loss: 2.8759212493896484\n",
      "step 37\n",
      "training loss: 2.8844499588012695\n",
      "step 38\n",
      "training loss: 2.878617286682129\n",
      "step 39\n",
      "training loss: 2.873331308364868\n",
      "step 40\n",
      "training loss: 2.87597393989563\n",
      "validation loss: 2.8656504154205322\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.85910701751709\n",
      "step 42\n",
      "training loss: 2.8413541316986084\n",
      "step 43\n",
      "training loss: 2.8624722957611084\n",
      "step 44\n",
      "training loss: 2.875586986541748\n",
      "step 45\n",
      "training loss: 2.844432830810547\n",
      "step 46\n",
      "training loss: 2.8622450828552246\n",
      "step 47\n",
      "training loss: 2.863866090774536\n",
      "step 48\n",
      "training loss: 2.8507235050201416\n",
      "step 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8823037147521973\n",
      "step 50\n",
      "training loss: 2.8554575443267822\n",
      "validation loss: 2.8451972007751465\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8716344833374023\n",
      "step 52\n",
      "training loss: 2.857590675354004\n",
      "step 53\n",
      "training loss: 2.871788501739502\n",
      "step 54\n",
      "training loss: 2.8247227668762207\n",
      "step 55\n",
      "training loss: 2.88234281539917\n",
      "step 56\n",
      "training loss: 2.8492934703826904\n",
      "step 57\n",
      "training loss: 2.882511615753174\n",
      "step 58\n",
      "training loss: 2.866164445877075\n",
      "step 59\n",
      "training loss: 2.847994804382324\n",
      "step 60\n",
      "training loss: 2.8424460887908936\n",
      "validation loss: 2.8515686988830566\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8654680252075195\n",
      "step 62\n",
      "training loss: 2.853900671005249\n",
      "step 63\n",
      "training loss: 2.8476603031158447\n",
      "step 64\n",
      "training loss: 2.8796327114105225\n",
      "step 65\n",
      "training loss: 2.868882179260254\n",
      "step 66\n",
      "training loss: 2.860546112060547\n",
      "step 67\n",
      "training loss: 2.8426637649536133\n",
      "step 68\n",
      "training loss: 2.858417510986328\n",
      "step 69\n",
      "training loss: 2.85929799079895\n",
      "step 70\n",
      "training loss: 2.8342695236206055\n",
      "validation loss: 2.8331356048583984\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.850755453109741\n",
      "step 72\n",
      "training loss: 2.850883722305298\n",
      "step 73\n",
      "training loss: 2.8463246822357178\n",
      "step 74\n",
      "training loss: 2.8379626274108887\n",
      "step 75\n",
      "training loss: 2.872330665588379\n",
      "step 76\n",
      "training loss: 2.845381259918213\n",
      "step 77\n",
      "training loss: 2.880340576171875\n",
      "step 78\n",
      "training loss: 2.859788656234741\n",
      "step 79\n",
      "training loss: 2.839858055114746\n",
      "step 80\n",
      "training loss: 2.8763656616210938\n",
      "validation loss: 2.8428337574005127\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.854452133178711\n",
      "step 82\n",
      "training loss: 2.8612804412841797\n",
      "step 83\n",
      "training loss: 2.866532325744629\n",
      "step 84\n",
      "training loss: 2.8332407474517822\n",
      "step 85\n",
      "training loss: 2.8753700256347656\n",
      "step 86\n",
      "training loss: 2.8559882640838623\n",
      "step 87\n",
      "training loss: 2.8763651847839355\n",
      "step 88\n",
      "training loss: 2.8726370334625244\n",
      "step 89\n",
      "training loss: 2.894740343093872\n",
      "step 90\n",
      "training loss: 2.879384994506836\n",
      "validation loss: 2.871497392654419\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.834872007369995\n",
      "step 92\n",
      "training loss: 2.8721182346343994\n",
      "step 93\n",
      "training loss: 2.857937812805176\n",
      "step 94\n",
      "training loss: 2.847700357437134\n",
      "step 95\n",
      "training loss: 2.856550455093384\n",
      "step 96\n",
      "training loss: 2.857485771179199\n",
      "step 97\n",
      "training loss: 2.8765103816986084\n",
      "step 98\n",
      "training loss: 2.860991954803467\n",
      "step 99\n",
      "training loss: 2.8784642219543457\n",
      "step 100\n",
      "training loss: 2.8784706592559814\n",
      "validation loss: 2.8753628730773926\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8795218467712402\n",
      "step 102\n",
      "training loss: 2.8577044010162354\n",
      "step 103\n",
      "training loss: 2.8685848712921143\n",
      "step 104\n",
      "training loss: 2.863518238067627\n",
      "step 105\n",
      "training loss: 2.858063220977783\n",
      "step 106\n",
      "training loss: 2.8672068119049072\n",
      "step 107\n",
      "training loss: 2.8449361324310303\n",
      "step 108\n",
      "training loss: 2.8691985607147217\n",
      "step 109\n",
      "training loss: 2.8732590675354004\n",
      "step 110\n",
      "training loss: 2.8175785541534424\n",
      "validation loss: 2.831747055053711\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8577260971069336\n",
      "step 112\n",
      "training loss: 2.850517988204956\n",
      "step 113\n",
      "training loss: 2.8666703701019287\n",
      "step 114\n",
      "training loss: 2.874420166015625\n",
      "step 115\n",
      "training loss: 2.8472251892089844\n",
      "step 116\n",
      "training loss: 2.867947578430176\n",
      "step 117\n",
      "training loss: 2.821070909500122\n",
      "step 118\n",
      "training loss: 2.851069927215576\n",
      "step 119\n",
      "training loss: 2.8748648166656494\n",
      "step 120\n",
      "training loss: 2.8687779903411865\n",
      "validation loss: 2.853076457977295\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8609023094177246\n",
      "step 122\n",
      "training loss: 2.8512799739837646\n",
      "step 123\n",
      "training loss: 2.865255117416382\n",
      "step 124\n",
      "training loss: 2.8530704975128174\n",
      "step 125\n",
      "training loss: 2.861262083053589\n",
      "step 126\n",
      "training loss: 2.8678171634674072\n",
      "step 127\n",
      "training loss: 2.8608016967773438\n",
      "step 128\n",
      "training loss: 2.8564083576202393\n",
      "step 129\n",
      "training loss: 2.859323263168335\n",
      "step 130\n",
      "training loss: 2.8373048305511475\n",
      "validation loss: 2.8776378631591797\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8408069610595703\n",
      "step 132\n",
      "training loss: 2.8318047523498535\n",
      "step 133\n",
      "training loss: 2.867912769317627\n",
      "step 134\n",
      "training loss: 2.871243476867676\n",
      "step 135\n",
      "training loss: 2.8636326789855957\n",
      "step 136\n",
      "training loss: 2.8585054874420166\n",
      "step 137\n",
      "training loss: 2.8519535064697266\n",
      "step 138\n",
      "training loss: 2.8711252212524414\n",
      "step 139\n",
      "training loss: 2.890536308288574\n",
      "step 140\n",
      "training loss: 2.813309907913208\n",
      "validation loss: 2.8797483444213867\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8600070476531982\n",
      "step 142\n",
      "training loss: 2.8521835803985596\n",
      "step 143\n",
      "training loss: 2.8752658367156982\n",
      "step 144\n",
      "training loss: 2.8582961559295654\n",
      "step 145\n",
      "training loss: 2.8802168369293213\n",
      "step 146\n",
      "training loss: 2.8526575565338135\n",
      "step 147\n",
      "training loss: 2.844473361968994\n",
      "step 148\n",
      "training loss: 2.853433609008789\n",
      "step 149\n",
      "training loss: 2.8685593605041504\n",
      "step 150\n",
      "training loss: 2.8513407707214355\n",
      "validation loss: 2.869352340698242\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8766684532165527\n",
      "step 152\n",
      "training loss: 2.862865924835205\n",
      "step 153\n",
      "training loss: 2.826101064682007\n",
      "step 154\n",
      "training loss: 2.851923942565918\n",
      "step 155\n",
      "training loss: 2.855330228805542\n",
      "step 156\n",
      "training loss: 2.8553388118743896\n",
      "step 157\n",
      "training loss: 2.870041847229004\n",
      "step 158\n",
      "training loss: 2.864626884460449\n",
      "step 159\n",
      "training loss: 2.8671915531158447\n",
      "step 160\n",
      "training loss: 2.815826892852783\n",
      "validation loss: 2.93538498878479\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.863933801651001\n",
      "step 162\n",
      "training loss: 2.857482671737671\n",
      "step 163\n",
      "training loss: 2.864424705505371\n",
      "step 164\n",
      "training loss: 2.8634281158447266\n",
      "step 165\n",
      "training loss: 2.873128890991211\n",
      "step 166\n",
      "training loss: 2.8697919845581055\n",
      "step 167\n",
      "training loss: 2.8537509441375732\n",
      "step 168\n",
      "training loss: 2.8806915283203125\n",
      "step 169\n",
      "training loss: 2.8768742084503174\n",
      "step 170\n",
      "training loss: 2.865485191345215\n",
      "validation loss: 2.8775267601013184\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8626081943511963\n",
      "step 172\n",
      "training loss: 2.869187593460083\n",
      "step 173\n",
      "training loss: 2.859180212020874\n",
      "step 174\n",
      "training loss: 2.8600869178771973\n",
      "step 175\n",
      "training loss: 2.8403477668762207\n",
      "step 176\n",
      "training loss: 2.822255849838257\n",
      "step 177\n",
      "training loss: 2.8709027767181396\n",
      "step 178\n",
      "training loss: 2.8498761653900146\n",
      "step 179\n",
      "training loss: 2.8492581844329834\n",
      "step 180\n",
      "training loss: 2.864710807800293\n",
      "validation loss: 2.8553524017333984\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8938021659851074\n",
      "step 182\n",
      "training loss: 2.846187114715576\n",
      "step 183\n",
      "training loss: 2.8727710247039795\n",
      "step 184\n",
      "training loss: 2.8656089305877686\n",
      "step 185\n",
      "training loss: 2.8716869354248047\n",
      "step 186\n",
      "training loss: 2.8505444526672363\n",
      "step 187\n",
      "training loss: 2.8588991165161133\n",
      "step 188\n",
      "training loss: 2.8378655910491943\n",
      "step 189\n",
      "training loss: 2.851858139038086\n",
      "step 190\n",
      "training loss: 2.8606741428375244\n",
      "validation loss: 2.8664822578430176\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.845639228820801\n",
      "step 192\n",
      "training loss: 2.8871989250183105\n",
      "step 193\n",
      "training loss: 2.8611345291137695\n",
      "step 194\n",
      "training loss: 2.8511760234832764\n",
      "step 195\n",
      "training loss: 2.863232374191284\n",
      "step 196\n",
      "training loss: 2.8670010566711426\n",
      "step 197\n",
      "training loss: 2.8563315868377686\n",
      "step 198\n",
      "training loss: 2.8432366847991943\n",
      "step 199\n",
      "training loss: 2.8315024375915527\n",
      "step 200\n",
      "training loss: 2.81760573387146\n",
      "validation loss: 2.9029080867767334\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.877528190612793\n",
      "step 202\n",
      "training loss: 2.8540730476379395\n",
      "step 203\n",
      "training loss: 2.8787360191345215\n",
      "step 204\n",
      "training loss: 2.8576390743255615\n",
      "step 205\n",
      "training loss: 2.865967035293579\n",
      "step 206\n",
      "training loss: 2.8826091289520264\n",
      "step 207\n",
      "training loss: 2.857968807220459\n",
      "step 208\n",
      "training loss: 2.8551621437072754\n",
      "step 209\n",
      "training loss: 2.869371175765991\n",
      "step 210\n",
      "training loss: 2.845987319946289\n",
      "validation loss: 2.8328449726104736\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8551151752471924\n",
      "step 212\n",
      "training loss: 2.849557876586914\n",
      "step 213\n",
      "training loss: 2.851149559020996\n",
      "step 214\n",
      "training loss: 2.8400890827178955\n",
      "step 215\n",
      "training loss: 2.8609116077423096\n",
      "step 216\n",
      "training loss: 2.8644416332244873\n",
      "step 217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.845919132232666\n",
      "step 218\n",
      "training loss: 2.8743677139282227\n",
      "step 219\n",
      "training loss: 2.853909492492676\n",
      "step 220\n",
      "training loss: 2.86250638961792\n",
      "validation loss: 2.858293294906616\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8596136569976807\n",
      "step 222\n",
      "training loss: 2.8676273822784424\n",
      "step 223\n",
      "training loss: 2.790422201156616\n",
      "step 224\n",
      "training loss: 2.8518595695495605\n",
      "step 225\n",
      "training loss: 2.833472728729248\n",
      "step 226\n",
      "training loss: 2.8488707542419434\n",
      "step 227\n",
      "training loss: 2.870349884033203\n",
      "step 228\n",
      "training loss: 2.8729500770568848\n",
      "step 229\n",
      "training loss: 2.862492084503174\n",
      "step 230\n",
      "training loss: 2.8614342212677\n",
      "validation loss: 2.8645408153533936\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.860565662384033\n",
      "step 232\n",
      "training loss: 2.8494560718536377\n",
      "step 233\n",
      "training loss: 2.8542370796203613\n",
      "step 234\n",
      "training loss: 2.868074893951416\n",
      "step 235\n",
      "training loss: 2.88627290725708\n",
      "step 236\n",
      "training loss: 2.8567233085632324\n",
      "step 237\n",
      "training loss: 2.845944404602051\n",
      "step 238\n",
      "training loss: 2.849086046218872\n",
      "step 239\n",
      "training loss: 2.845837116241455\n",
      "step 240\n",
      "training loss: 2.861788749694824\n",
      "validation loss: 2.8687644004821777\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8646044731140137\n",
      "step 242\n",
      "training loss: 2.8564586639404297\n",
      "step 243\n",
      "training loss: 2.868133306503296\n",
      "step 244\n",
      "training loss: 2.874879837036133\n",
      "step 245\n",
      "training loss: 2.8486015796661377\n",
      "step 246\n",
      "training loss: 2.874375343322754\n",
      "step 247\n",
      "training loss: 2.880014657974243\n",
      "step 248\n",
      "training loss: 2.847960948944092\n",
      "step 249\n",
      "training loss: 2.846273183822632\n",
      "step 250\n",
      "training loss: 2.870279312133789\n",
      "validation loss: 2.9083898067474365\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8646738529205322\n",
      "step 252\n",
      "training loss: 2.8640894889831543\n",
      "step 253\n",
      "training loss: 2.865663528442383\n",
      "step 254\n",
      "training loss: 2.8693692684173584\n",
      "step 255\n",
      "training loss: 2.859604835510254\n",
      "step 256\n",
      "training loss: 2.8670554161071777\n",
      "step 257\n",
      "training loss: 2.860947608947754\n",
      "step 258\n",
      "training loss: 2.8644587993621826\n",
      "step 259\n",
      "training loss: 2.8607659339904785\n",
      "step 260\n",
      "training loss: 2.823758363723755\n",
      "validation loss: 2.8665759563446045\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.85392689704895\n",
      "step 262\n",
      "training loss: 2.871725082397461\n",
      "step 263\n",
      "training loss: 2.8487014770507812\n",
      "step 264\n",
      "training loss: 2.856109619140625\n",
      "step 265\n",
      "training loss: 2.889873504638672\n",
      "step 266\n",
      "training loss: 2.8645920753479004\n",
      "step 267\n",
      "training loss: 2.880600690841675\n",
      "step 268\n",
      "training loss: 2.880009412765503\n",
      "step 269\n",
      "training loss: 2.8724491596221924\n",
      "step 270\n",
      "training loss: 2.8506100177764893\n",
      "validation loss: 2.871793746948242\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8781332969665527\n",
      "----------3.0 min per epoch----------\n",
      "epoch 15\n",
      "step 0\n",
      "training loss: 2.8766698837280273\n",
      "validation loss: 2.838261365890503\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8702380657196045\n",
      "step 2\n",
      "training loss: 2.864377975463867\n",
      "step 3\n",
      "training loss: 2.877714157104492\n",
      "step 4\n",
      "training loss: 2.8860137462615967\n",
      "step 5\n",
      "training loss: 2.8072588443756104\n",
      "step 6\n",
      "training loss: 2.8479554653167725\n",
      "step 7\n",
      "training loss: 2.8219656944274902\n",
      "step 8\n",
      "training loss: 2.8571441173553467\n",
      "step 9\n",
      "training loss: 2.879544734954834\n",
      "step 10\n",
      "training loss: 2.8776016235351562\n",
      "validation loss: 2.8424761295318604\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8852336406707764\n",
      "step 12\n",
      "training loss: 2.875030755996704\n",
      "step 13\n",
      "training loss: 2.8095901012420654\n",
      "step 14\n",
      "training loss: 2.7647221088409424\n",
      "step 15\n",
      "training loss: 2.8537209033966064\n",
      "step 16\n",
      "training loss: 2.891298770904541\n",
      "step 17\n",
      "training loss: 2.8778293132781982\n",
      "step 18\n",
      "training loss: 2.855001449584961\n",
      "step 19\n",
      "training loss: 2.860649824142456\n",
      "step 20\n",
      "training loss: 2.879193067550659\n",
      "validation loss: 2.8418047428131104\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8813788890838623\n",
      "step 22\n",
      "training loss: 2.8830246925354004\n",
      "step 23\n",
      "training loss: 2.8681700229644775\n",
      "step 24\n",
      "training loss: 2.8807952404022217\n",
      "step 25\n",
      "training loss: 2.8784968852996826\n",
      "step 26\n",
      "training loss: 2.8380839824676514\n",
      "step 27\n",
      "training loss: 2.8473756313323975\n",
      "step 28\n",
      "training loss: 2.857830286026001\n",
      "step 29\n",
      "training loss: 2.864722490310669\n",
      "step 30\n",
      "training loss: 2.8714756965637207\n",
      "validation loss: 2.863680124282837\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.86647629737854\n",
      "step 32\n",
      "training loss: 2.874173164367676\n",
      "step 33\n",
      "training loss: 2.874336004257202\n",
      "step 34\n",
      "training loss: 2.869096040725708\n",
      "step 35\n",
      "training loss: 2.8873138427734375\n",
      "step 36\n",
      "training loss: 2.8660950660705566\n",
      "step 37\n",
      "training loss: 2.8750112056732178\n",
      "step 38\n",
      "training loss: 2.882871150970459\n",
      "step 39\n",
      "training loss: 2.8765552043914795\n",
      "step 40\n",
      "training loss: 2.8731420040130615\n",
      "validation loss: 2.8741440773010254\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8769173622131348\n",
      "step 42\n",
      "training loss: 2.858738899230957\n",
      "step 43\n",
      "training loss: 2.839820146560669\n",
      "step 44\n",
      "training loss: 2.864121198654175\n",
      "step 45\n",
      "training loss: 2.8750364780426025\n",
      "step 46\n",
      "training loss: 2.8468589782714844\n",
      "step 47\n",
      "training loss: 2.8623316287994385\n",
      "step 48\n",
      "training loss: 2.861921548843384\n",
      "step 49\n",
      "training loss: 2.8519463539123535\n",
      "step 50\n",
      "training loss: 2.8814456462860107\n",
      "validation loss: 2.862626552581787\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8560678958892822\n",
      "step 52\n",
      "training loss: 2.8726494312286377\n",
      "step 53\n",
      "training loss: 2.8550446033477783\n",
      "step 54\n",
      "training loss: 2.870152235031128\n",
      "step 55\n",
      "training loss: 2.8250904083251953\n",
      "step 56\n",
      "training loss: 2.8841261863708496\n",
      "step 57\n",
      "training loss: 2.848938465118408\n",
      "step 58\n",
      "training loss: 2.8836309909820557\n",
      "step 59\n",
      "training loss: 2.866764783859253\n",
      "step 60\n",
      "training loss: 2.8498873710632324\n",
      "validation loss: 2.8594233989715576\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.843632221221924\n",
      "step 62\n",
      "training loss: 2.864772081375122\n",
      "step 63\n",
      "training loss: 2.855087995529175\n",
      "step 64\n",
      "training loss: 2.844947576522827\n",
      "step 65\n",
      "training loss: 2.8802907466888428\n",
      "step 66\n",
      "training loss: 2.8715951442718506\n",
      "step 67\n",
      "training loss: 2.8606717586517334\n",
      "step 68\n",
      "training loss: 2.843780994415283\n",
      "step 69\n",
      "training loss: 2.8590683937072754\n",
      "step 70\n",
      "training loss: 2.858877420425415\n",
      "validation loss: 2.865541934967041\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.834773540496826\n",
      "step 72\n",
      "training loss: 2.8505914211273193\n",
      "step 73\n",
      "training loss: 2.853466749191284\n",
      "step 74\n",
      "training loss: 2.8438491821289062\n",
      "step 75\n",
      "training loss: 2.83760666847229\n",
      "step 76\n",
      "training loss: 2.8732211589813232\n",
      "step 77\n",
      "training loss: 2.8484303951263428\n",
      "step 78\n",
      "training loss: 2.8808679580688477\n",
      "step 79\n",
      "training loss: 2.8604886531829834\n",
      "step 80\n",
      "training loss: 2.8385093212127686\n",
      "validation loss: 2.8464860916137695\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8782544136047363\n",
      "step 82\n",
      "training loss: 2.85062575340271\n",
      "step 83\n",
      "training loss: 2.862025260925293\n",
      "step 84\n",
      "training loss: 2.864164352416992\n",
      "step 85\n",
      "training loss: 2.8347198963165283\n",
      "step 86\n",
      "training loss: 2.876429319381714\n",
      "step 87\n",
      "training loss: 2.8559348583221436\n",
      "step 88\n",
      "training loss: 2.8762550354003906\n",
      "step 89\n",
      "training loss: 2.873000144958496\n",
      "step 90\n",
      "training loss: 2.895038604736328\n",
      "validation loss: 2.8488574028015137\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.878730297088623\n",
      "step 92\n",
      "training loss: 2.8371615409851074\n",
      "step 93\n",
      "training loss: 2.8717892169952393\n",
      "step 94\n",
      "training loss: 2.8589515686035156\n",
      "step 95\n",
      "training loss: 2.849522590637207\n",
      "step 96\n",
      "training loss: 2.8585262298583984\n",
      "step 97\n",
      "training loss: 2.856851816177368\n",
      "step 98\n",
      "training loss: 2.8751373291015625\n",
      "step 99\n",
      "training loss: 2.863839626312256\n",
      "step 100\n",
      "training loss: 2.8797409534454346\n",
      "validation loss: 2.831998109817505\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8777270317077637\n",
      "step 102\n",
      "training loss: 2.87906551361084\n",
      "step 103\n",
      "training loss: 2.852964162826538\n",
      "step 104\n",
      "training loss: 2.870657444000244\n",
      "step 105\n",
      "training loss: 2.864467144012451\n",
      "step 106\n",
      "training loss: 2.859084129333496\n",
      "step 107\n",
      "training loss: 2.8680248260498047\n",
      "step 108\n",
      "training loss: 2.8449411392211914\n",
      "step 109\n",
      "training loss: 2.8700435161590576\n",
      "step 110\n",
      "training loss: 2.8723857402801514\n",
      "validation loss: 2.8420257568359375\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.816331624984741\n",
      "step 112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.855452060699463\n",
      "step 113\n",
      "training loss: 2.8486270904541016\n",
      "step 114\n",
      "training loss: 2.8649497032165527\n",
      "step 115\n",
      "training loss: 2.8735320568084717\n",
      "step 116\n",
      "training loss: 2.847102165222168\n",
      "step 117\n",
      "training loss: 2.866683006286621\n",
      "step 118\n",
      "training loss: 2.8202483654022217\n",
      "step 119\n",
      "training loss: 2.8545844554901123\n",
      "step 120\n",
      "training loss: 2.875633716583252\n",
      "validation loss: 2.8695759773254395\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8672049045562744\n",
      "step 122\n",
      "training loss: 2.8581974506378174\n",
      "step 123\n",
      "training loss: 2.848811388015747\n",
      "step 124\n",
      "training loss: 2.8668713569641113\n",
      "step 125\n",
      "training loss: 2.8534302711486816\n",
      "step 126\n",
      "training loss: 2.8634235858917236\n",
      "step 127\n",
      "training loss: 2.867556571960449\n",
      "step 128\n",
      "training loss: 2.861978769302368\n",
      "step 129\n",
      "training loss: 2.85662841796875\n",
      "step 130\n",
      "training loss: 2.86128306388855\n",
      "validation loss: 2.875094175338745\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8378031253814697\n",
      "step 132\n",
      "training loss: 2.8422162532806396\n",
      "step 133\n",
      "training loss: 2.831294298171997\n",
      "step 134\n",
      "training loss: 2.8675150871276855\n",
      "step 135\n",
      "training loss: 2.870745897293091\n",
      "step 136\n",
      "training loss: 2.86160945892334\n",
      "step 137\n",
      "training loss: 2.859346389770508\n",
      "step 138\n",
      "training loss: 2.8531007766723633\n",
      "step 139\n",
      "training loss: 2.8705427646636963\n",
      "step 140\n",
      "training loss: 2.8933560848236084\n",
      "validation loss: 2.8323094844818115\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8146610260009766\n",
      "step 142\n",
      "training loss: 2.8600614070892334\n",
      "step 143\n",
      "training loss: 2.852196455001831\n",
      "step 144\n",
      "training loss: 2.874932050704956\n",
      "step 145\n",
      "training loss: 2.85996150970459\n",
      "step 146\n",
      "training loss: 2.880995750427246\n",
      "step 147\n",
      "training loss: 2.854586124420166\n",
      "step 148\n",
      "training loss: 2.841892957687378\n",
      "step 149\n",
      "training loss: 2.852353811264038\n",
      "step 150\n",
      "training loss: 2.866563081741333\n",
      "validation loss: 2.8523924350738525\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.85117506980896\n",
      "step 152\n",
      "training loss: 2.8785243034362793\n",
      "step 153\n",
      "training loss: 2.862621545791626\n",
      "step 154\n",
      "training loss: 2.8245725631713867\n",
      "step 155\n",
      "training loss: 2.853557586669922\n",
      "step 156\n",
      "training loss: 2.854180097579956\n",
      "step 157\n",
      "training loss: 2.858342409133911\n",
      "step 158\n",
      "training loss: 2.8701107501983643\n",
      "step 159\n",
      "training loss: 2.865900754928589\n",
      "step 160\n",
      "training loss: 2.8672571182250977\n",
      "validation loss: 2.8780057430267334\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.815469264984131\n",
      "step 162\n",
      "training loss: 2.863581418991089\n",
      "step 163\n",
      "training loss: 2.857658624649048\n",
      "step 164\n",
      "training loss: 2.8667590618133545\n",
      "step 165\n",
      "training loss: 2.8634307384490967\n",
      "step 166\n",
      "training loss: 2.8760221004486084\n",
      "step 167\n",
      "training loss: 2.8699917793273926\n",
      "step 168\n",
      "training loss: 2.854579210281372\n",
      "step 169\n",
      "training loss: 2.879668951034546\n",
      "step 170\n",
      "training loss: 2.8750557899475098\n",
      "validation loss: 2.877009153366089\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.864663600921631\n",
      "step 172\n",
      "training loss: 2.861492872238159\n",
      "step 173\n",
      "training loss: 2.8695898056030273\n",
      "step 174\n",
      "training loss: 2.859769582748413\n",
      "step 175\n",
      "training loss: 2.8592984676361084\n",
      "step 176\n",
      "training loss: 2.8389065265655518\n",
      "step 177\n",
      "training loss: 2.8232176303863525\n",
      "step 178\n",
      "training loss: 2.871896743774414\n",
      "step 179\n",
      "training loss: 2.850537061691284\n",
      "step 180\n",
      "training loss: 2.850428342819214\n",
      "validation loss: 2.872288465499878\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8603248596191406\n",
      "step 182\n",
      "training loss: 2.892432451248169\n",
      "step 183\n",
      "training loss: 2.8462283611297607\n",
      "step 184\n",
      "training loss: 2.8724637031555176\n",
      "step 185\n",
      "training loss: 2.8636980056762695\n",
      "step 186\n",
      "training loss: 2.870993137359619\n",
      "step 187\n",
      "training loss: 2.851024866104126\n",
      "step 188\n",
      "training loss: 2.8596200942993164\n",
      "step 189\n",
      "training loss: 2.8366756439208984\n",
      "step 190\n",
      "training loss: 2.8526010513305664\n",
      "validation loss: 2.939643383026123\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8574283123016357\n",
      "step 192\n",
      "training loss: 2.8460609912872314\n",
      "step 193\n",
      "training loss: 2.888157367706299\n",
      "step 194\n",
      "training loss: 2.859989643096924\n",
      "step 195\n",
      "training loss: 2.848942995071411\n",
      "step 196\n",
      "training loss: 2.8651905059814453\n",
      "step 197\n",
      "training loss: 2.865847587585449\n",
      "step 198\n",
      "training loss: 2.8534908294677734\n",
      "step 199\n",
      "training loss: 2.8430252075195312\n",
      "step 200\n",
      "training loss: 2.831115484237671\n",
      "validation loss: 2.8830718994140625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8180248737335205\n",
      "step 202\n",
      "training loss: 2.8776185512542725\n",
      "step 203\n",
      "training loss: 2.8529603481292725\n",
      "step 204\n",
      "training loss: 2.8794286251068115\n",
      "step 205\n",
      "training loss: 2.853931427001953\n",
      "step 206\n",
      "training loss: 2.863862991333008\n",
      "step 207\n",
      "training loss: 2.881622314453125\n",
      "step 208\n",
      "training loss: 2.8566813468933105\n",
      "step 209\n",
      "training loss: 2.8541793823242188\n",
      "step 210\n",
      "training loss: 2.868175983428955\n",
      "validation loss: 2.857618570327759\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8447155952453613\n",
      "step 212\n",
      "training loss: 2.8539726734161377\n",
      "step 213\n",
      "training loss: 2.847947120666504\n",
      "step 214\n",
      "training loss: 2.854949474334717\n",
      "step 215\n",
      "training loss: 2.840785026550293\n",
      "step 216\n",
      "training loss: 2.8581767082214355\n",
      "step 217\n",
      "training loss: 2.8648643493652344\n",
      "step 218\n",
      "training loss: 2.8456239700317383\n",
      "step 219\n",
      "training loss: 2.8753936290740967\n",
      "step 220\n",
      "training loss: 2.8522346019744873\n",
      "validation loss: 2.8606371879577637\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8615381717681885\n",
      "step 222\n",
      "training loss: 2.861915111541748\n",
      "step 223\n",
      "training loss: 2.8645341396331787\n",
      "step 224\n",
      "training loss: 2.7894792556762695\n",
      "step 225\n",
      "training loss: 2.849092721939087\n",
      "step 226\n",
      "training loss: 2.8332762718200684\n",
      "step 227\n",
      "training loss: 2.8503310680389404\n",
      "step 228\n",
      "training loss: 2.870389461517334\n",
      "step 229\n",
      "training loss: 2.871666669845581\n",
      "step 230\n",
      "training loss: 2.862525224685669\n",
      "validation loss: 2.8833556175231934\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.860323667526245\n",
      "step 232\n",
      "training loss: 2.859532117843628\n",
      "step 233\n",
      "training loss: 2.847367525100708\n",
      "step 234\n",
      "training loss: 2.8550994396209717\n",
      "step 235\n",
      "training loss: 2.8725078105926514\n",
      "step 236\n",
      "training loss: 2.884969711303711\n",
      "step 237\n",
      "training loss: 2.8560309410095215\n",
      "step 238\n",
      "training loss: 2.8460891246795654\n",
      "step 239\n",
      "training loss: 2.8477442264556885\n",
      "step 240\n",
      "training loss: 2.8479392528533936\n",
      "validation loss: 2.8320863246917725\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8610668182373047\n",
      "step 242\n",
      "training loss: 2.863673448562622\n",
      "step 243\n",
      "training loss: 2.8569233417510986\n",
      "step 244\n",
      "training loss: 2.865849018096924\n",
      "step 245\n",
      "training loss: 2.873924732208252\n",
      "step 246\n",
      "training loss: 2.8476531505584717\n",
      "step 247\n",
      "training loss: 2.8731513023376465\n",
      "step 248\n",
      "training loss: 2.8772597312927246\n",
      "step 249\n",
      "training loss: 2.846635580062866\n",
      "step 250\n",
      "training loss: 2.8475584983825684\n",
      "validation loss: 2.8564178943634033\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8712575435638428\n",
      "step 252\n",
      "training loss: 2.864903211593628\n",
      "step 253\n",
      "training loss: 2.865844964981079\n",
      "step 254\n",
      "training loss: 2.865600109100342\n",
      "step 255\n",
      "training loss: 2.8679184913635254\n",
      "step 256\n",
      "training loss: 2.858616590499878\n",
      "step 257\n",
      "training loss: 2.865473508834839\n",
      "step 258\n",
      "training loss: 2.861846446990967\n",
      "step 259\n",
      "training loss: 2.8615097999572754\n",
      "step 260\n",
      "training loss: 2.862311363220215\n",
      "validation loss: 2.8661558628082275\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.826277017593384\n",
      "step 262\n",
      "training loss: 2.8545477390289307\n",
      "step 263\n",
      "training loss: 2.8697681427001953\n",
      "step 264\n",
      "training loss: 2.8489482402801514\n",
      "step 265\n",
      "training loss: 2.858023166656494\n",
      "step 266\n",
      "training loss: 2.888233184814453\n",
      "step 267\n",
      "training loss: 2.863600254058838\n",
      "step 268\n",
      "training loss: 2.8788111209869385\n",
      "step 269\n",
      "training loss: 2.881492853164673\n",
      "step 270\n",
      "training loss: 2.86711049079895\n",
      "validation loss: 2.8770153522491455\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8508706092834473\n",
      "----------3.0 min per epoch----------\n",
      "epoch 16\n",
      "step 0\n",
      "training loss: 2.8758575916290283\n",
      "validation loss: 2.9065535068511963\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8781392574310303\n",
      "step 2\n",
      "training loss: 2.8690009117126465\n",
      "step 3\n",
      "training loss: 2.866154193878174\n",
      "step 4\n",
      "training loss: 2.8773746490478516\n",
      "step 5\n",
      "training loss: 2.884446859359741\n",
      "step 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.806103229522705\n",
      "step 7\n",
      "training loss: 2.8492369651794434\n",
      "step 8\n",
      "training loss: 2.8223278522491455\n",
      "step 9\n",
      "training loss: 2.8553740978240967\n",
      "step 10\n",
      "training loss: 2.880113363265991\n",
      "validation loss: 2.875910520553589\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.874023199081421\n",
      "step 12\n",
      "training loss: 2.882389545440674\n",
      "step 13\n",
      "training loss: 2.8715128898620605\n",
      "step 14\n",
      "training loss: 2.80954909324646\n",
      "step 15\n",
      "training loss: 2.7729477882385254\n",
      "step 16\n",
      "training loss: 2.8452413082122803\n",
      "step 17\n",
      "training loss: 2.88606595993042\n",
      "step 18\n",
      "training loss: 2.869929075241089\n",
      "step 19\n",
      "training loss: 2.8541808128356934\n",
      "step 20\n",
      "training loss: 2.8607077598571777\n",
      "validation loss: 2.86795711517334\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.875959873199463\n",
      "step 22\n",
      "training loss: 2.880720853805542\n",
      "step 23\n",
      "training loss: 2.8823304176330566\n",
      "step 24\n",
      "training loss: 2.8718206882476807\n",
      "step 25\n",
      "training loss: 2.8813514709472656\n",
      "step 26\n",
      "training loss: 2.880214214324951\n",
      "step 27\n",
      "training loss: 2.8371660709381104\n",
      "step 28\n",
      "training loss: 2.8454463481903076\n",
      "step 29\n",
      "training loss: 2.8560643196105957\n",
      "step 30\n",
      "training loss: 2.8605551719665527\n",
      "validation loss: 2.830716609954834\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.874687433242798\n",
      "step 32\n",
      "training loss: 2.868471622467041\n",
      "step 33\n",
      "training loss: 2.874272346496582\n",
      "step 34\n",
      "training loss: 2.8755767345428467\n",
      "step 35\n",
      "training loss: 2.869637966156006\n",
      "step 36\n",
      "training loss: 2.884911060333252\n",
      "step 37\n",
      "training loss: 2.8654325008392334\n",
      "step 38\n",
      "training loss: 2.8782429695129395\n",
      "step 39\n",
      "training loss: 2.8828065395355225\n",
      "step 40\n",
      "training loss: 2.8787081241607666\n",
      "validation loss: 2.836000442504883\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.873337507247925\n",
      "step 42\n",
      "training loss: 2.876812219619751\n",
      "step 43\n",
      "training loss: 2.8613474369049072\n",
      "step 44\n",
      "training loss: 2.8392951488494873\n",
      "step 45\n",
      "training loss: 2.8638734817504883\n",
      "step 46\n",
      "training loss: 2.8736913204193115\n",
      "step 47\n",
      "training loss: 2.8433072566986084\n",
      "step 48\n",
      "training loss: 2.8608038425445557\n",
      "step 49\n",
      "training loss: 2.8633437156677246\n",
      "step 50\n",
      "training loss: 2.8485729694366455\n",
      "validation loss: 2.8312535285949707\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.882155179977417\n",
      "step 52\n",
      "training loss: 2.855759620666504\n",
      "step 53\n",
      "training loss: 2.8723533153533936\n",
      "step 54\n",
      "training loss: 2.8543806076049805\n",
      "step 55\n",
      "training loss: 2.8710150718688965\n",
      "step 56\n",
      "training loss: 2.8232600688934326\n",
      "step 57\n",
      "training loss: 2.883955955505371\n",
      "step 58\n",
      "training loss: 2.848968982696533\n",
      "step 59\n",
      "training loss: 2.8809146881103516\n",
      "step 60\n",
      "training loss: 2.8667232990264893\n",
      "validation loss: 2.863581895828247\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8484530448913574\n",
      "step 62\n",
      "training loss: 2.8446884155273438\n",
      "step 63\n",
      "training loss: 2.863960027694702\n",
      "step 64\n",
      "training loss: 2.8525888919830322\n",
      "step 65\n",
      "training loss: 2.8465943336486816\n",
      "step 66\n",
      "training loss: 2.881309986114502\n",
      "step 67\n",
      "training loss: 2.8686773777008057\n",
      "step 68\n",
      "training loss: 2.860172748565674\n",
      "step 69\n",
      "training loss: 2.8444318771362305\n",
      "step 70\n",
      "training loss: 2.85844349861145\n",
      "validation loss: 2.8677749633789062\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8592770099639893\n",
      "step 72\n",
      "training loss: 2.835299015045166\n",
      "step 73\n",
      "training loss: 2.8515360355377197\n",
      "step 74\n",
      "training loss: 2.8504621982574463\n",
      "step 75\n",
      "training loss: 2.8426663875579834\n",
      "step 76\n",
      "training loss: 2.8376386165618896\n",
      "step 77\n",
      "training loss: 2.8713817596435547\n",
      "step 78\n",
      "training loss: 2.843752145767212\n",
      "step 79\n",
      "training loss: 2.8788552284240723\n",
      "step 80\n",
      "training loss: 2.860103130340576\n",
      "validation loss: 2.8620641231536865\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.836904525756836\n",
      "step 82\n",
      "training loss: 2.8745784759521484\n",
      "step 83\n",
      "training loss: 2.8520758152008057\n",
      "step 84\n",
      "training loss: 2.8596503734588623\n",
      "step 85\n",
      "training loss: 2.8618643283843994\n",
      "step 86\n",
      "training loss: 2.831951379776001\n",
      "step 87\n",
      "training loss: 2.8781542778015137\n",
      "step 88\n",
      "training loss: 2.8577773571014404\n",
      "step 89\n",
      "training loss: 2.879031181335449\n",
      "step 90\n",
      "training loss: 2.872457981109619\n",
      "validation loss: 2.8591582775115967\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8932909965515137\n",
      "step 92\n",
      "training loss: 2.878575325012207\n",
      "step 93\n",
      "training loss: 2.8342251777648926\n",
      "step 94\n",
      "training loss: 2.86987042427063\n",
      "step 95\n",
      "training loss: 2.8591349124908447\n",
      "step 96\n",
      "training loss: 2.8490705490112305\n",
      "step 97\n",
      "training loss: 2.8608622550964355\n",
      "step 98\n",
      "training loss: 2.856879949569702\n",
      "step 99\n",
      "training loss: 2.8766379356384277\n",
      "step 100\n",
      "training loss: 2.8618412017822266\n",
      "validation loss: 2.8662216663360596\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8771936893463135\n",
      "step 102\n",
      "training loss: 2.876518487930298\n",
      "step 103\n",
      "training loss: 2.8789663314819336\n",
      "step 104\n",
      "training loss: 2.857302188873291\n",
      "step 105\n",
      "training loss: 2.867969512939453\n",
      "step 106\n",
      "training loss: 2.862910509109497\n",
      "step 107\n",
      "training loss: 2.857064962387085\n",
      "step 108\n",
      "training loss: 2.868037700653076\n",
      "step 109\n",
      "training loss: 2.8452720642089844\n",
      "step 110\n",
      "training loss: 2.8681209087371826\n",
      "validation loss: 2.8443238735198975\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.873187303543091\n",
      "step 112\n",
      "training loss: 2.816823720932007\n",
      "step 113\n",
      "training loss: 2.8554811477661133\n",
      "step 114\n",
      "training loss: 2.8488316535949707\n",
      "step 115\n",
      "training loss: 2.8635129928588867\n",
      "step 116\n",
      "training loss: 2.874256134033203\n",
      "step 117\n",
      "training loss: 2.8455374240875244\n",
      "step 118\n",
      "training loss: 2.8659470081329346\n",
      "step 119\n",
      "training loss: 2.819132089614868\n",
      "step 120\n",
      "training loss: 2.851975917816162\n",
      "validation loss: 2.847026824951172\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.874377965927124\n",
      "step 122\n",
      "training loss: 2.867215633392334\n",
      "step 123\n",
      "training loss: 2.857677698135376\n",
      "step 124\n",
      "training loss: 2.850825548171997\n",
      "step 125\n",
      "training loss: 2.8643219470977783\n",
      "step 126\n",
      "training loss: 2.8512156009674072\n",
      "step 127\n",
      "training loss: 2.8618900775909424\n",
      "step 128\n",
      "training loss: 2.868730068206787\n",
      "step 129\n",
      "training loss: 2.864049196243286\n",
      "step 130\n",
      "training loss: 2.855665683746338\n",
      "validation loss: 2.8338425159454346\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8619167804718018\n",
      "step 132\n",
      "training loss: 2.834981918334961\n",
      "step 133\n",
      "training loss: 2.8423283100128174\n",
      "step 134\n",
      "training loss: 2.832901954650879\n",
      "step 135\n",
      "training loss: 2.868506669998169\n",
      "step 136\n",
      "training loss: 2.8710777759552\n",
      "step 137\n",
      "training loss: 2.862274408340454\n",
      "step 138\n",
      "training loss: 2.8594441413879395\n",
      "step 139\n",
      "training loss: 2.852036237716675\n",
      "step 140\n",
      "training loss: 2.872729539871216\n",
      "validation loss: 2.8385438919067383\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.891526699066162\n",
      "step 142\n",
      "training loss: 2.8109023571014404\n",
      "step 143\n",
      "training loss: 2.860708236694336\n",
      "step 144\n",
      "training loss: 2.85201358795166\n",
      "step 145\n",
      "training loss: 2.877293586730957\n",
      "step 146\n",
      "training loss: 2.859208822250366\n",
      "step 147\n",
      "training loss: 2.8798067569732666\n",
      "step 148\n",
      "training loss: 2.852691650390625\n",
      "step 149\n",
      "training loss: 2.840885639190674\n",
      "step 150\n",
      "training loss: 2.853753089904785\n",
      "validation loss: 2.8670654296875\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8668768405914307\n",
      "step 152\n",
      "training loss: 2.8500752449035645\n",
      "step 153\n",
      "training loss: 2.875171422958374\n",
      "step 154\n",
      "training loss: 2.8601930141448975\n",
      "step 155\n",
      "training loss: 2.825927734375\n",
      "step 156\n",
      "training loss: 2.8496029376983643\n",
      "step 157\n",
      "training loss: 2.855470657348633\n",
      "step 158\n",
      "training loss: 2.855288028717041\n",
      "step 159\n",
      "training loss: 2.869433879852295\n",
      "step 160\n",
      "training loss: 2.8649749755859375\n",
      "validation loss: 2.8740880489349365\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.867497444152832\n",
      "step 162\n",
      "training loss: 2.8146822452545166\n",
      "step 163\n",
      "training loss: 2.861680746078491\n",
      "step 164\n",
      "training loss: 2.8578567504882812\n",
      "step 165\n",
      "training loss: 2.862250804901123\n",
      "step 166\n",
      "training loss: 2.8638076782226562\n",
      "step 167\n",
      "training loss: 2.875372886657715\n",
      "step 168\n",
      "training loss: 2.870201587677002\n",
      "step 169\n",
      "training loss: 2.8517656326293945\n",
      "step 170\n",
      "training loss: 2.8797800540924072\n",
      "validation loss: 2.8315277099609375\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8751912117004395\n",
      "step 172\n",
      "training loss: 2.863820791244507\n",
      "step 173\n",
      "training loss: 2.859915256500244\n",
      "step 174\n",
      "training loss: 2.8687283992767334\n",
      "step 175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.860626220703125\n",
      "step 176\n",
      "training loss: 2.8600358963012695\n",
      "step 177\n",
      "training loss: 2.8378169536590576\n",
      "step 178\n",
      "training loss: 2.8218460083007812\n",
      "step 179\n",
      "training loss: 2.8699405193328857\n",
      "step 180\n",
      "training loss: 2.8500595092773438\n",
      "validation loss: 2.852081298828125\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.848493814468384\n",
      "step 182\n",
      "training loss: 2.8627536296844482\n",
      "step 183\n",
      "training loss: 2.8938913345336914\n",
      "step 184\n",
      "training loss: 2.846104860305786\n",
      "step 185\n",
      "training loss: 2.8687167167663574\n",
      "step 186\n",
      "training loss: 2.8619308471679688\n",
      "step 187\n",
      "training loss: 2.8691153526306152\n",
      "step 188\n",
      "training loss: 2.8495278358459473\n",
      "step 189\n",
      "training loss: 2.859523296356201\n",
      "step 190\n",
      "training loss: 2.836057662963867\n",
      "validation loss: 2.8806397914886475\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8520569801330566\n",
      "step 192\n",
      "training loss: 2.8578617572784424\n",
      "step 193\n",
      "training loss: 2.846163749694824\n",
      "step 194\n",
      "training loss: 2.8856661319732666\n",
      "step 195\n",
      "training loss: 2.859306812286377\n",
      "step 196\n",
      "training loss: 2.8497743606567383\n",
      "step 197\n",
      "training loss: 2.8629415035247803\n",
      "step 198\n",
      "training loss: 2.8655848503112793\n",
      "step 199\n",
      "training loss: 2.853745937347412\n",
      "step 200\n",
      "training loss: 2.841423749923706\n",
      "validation loss: 2.878275156021118\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.826700210571289\n",
      "step 202\n",
      "training loss: 2.8156824111938477\n",
      "step 203\n",
      "training loss: 2.8757991790771484\n",
      "step 204\n",
      "training loss: 2.8542439937591553\n",
      "step 205\n",
      "training loss: 2.8806302547454834\n",
      "step 206\n",
      "training loss: 2.855544090270996\n",
      "step 207\n",
      "training loss: 2.8646111488342285\n",
      "step 208\n",
      "training loss: 2.8793978691101074\n",
      "step 209\n",
      "training loss: 2.858787775039673\n",
      "step 210\n",
      "training loss: 2.854860305786133\n",
      "validation loss: 2.8647263050079346\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8680500984191895\n",
      "step 212\n",
      "training loss: 2.845642328262329\n",
      "step 213\n",
      "training loss: 2.8537747859954834\n",
      "step 214\n",
      "training loss: 2.8458251953125\n",
      "step 215\n",
      "training loss: 2.8509681224823\n",
      "step 216\n",
      "training loss: 2.83910870552063\n",
      "step 217\n",
      "training loss: 2.8604283332824707\n",
      "step 218\n",
      "training loss: 2.8626174926757812\n",
      "step 219\n",
      "training loss: 2.846515417098999\n",
      "step 220\n",
      "training loss: 2.873706102371216\n",
      "validation loss: 2.9319920539855957\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8505122661590576\n",
      "step 222\n",
      "training loss: 2.862612009048462\n",
      "step 223\n",
      "training loss: 2.8578829765319824\n",
      "step 224\n",
      "training loss: 2.8673648834228516\n",
      "step 225\n",
      "training loss: 2.7882091999053955\n",
      "step 226\n",
      "training loss: 2.850762128829956\n",
      "step 227\n",
      "training loss: 2.832728385925293\n",
      "step 228\n",
      "training loss: 2.8461012840270996\n",
      "step 229\n",
      "training loss: 2.868257761001587\n",
      "step 230\n",
      "training loss: 2.8714585304260254\n",
      "validation loss: 2.877925157546997\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.863780975341797\n",
      "step 232\n",
      "training loss: 2.86285138130188\n",
      "step 233\n",
      "training loss: 2.8602428436279297\n",
      "step 234\n",
      "training loss: 2.848764657974243\n",
      "step 235\n",
      "training loss: 2.855098247528076\n",
      "step 236\n",
      "training loss: 2.8709893226623535\n",
      "step 237\n",
      "training loss: 2.886535406112671\n",
      "step 238\n",
      "training loss: 2.8554346561431885\n",
      "step 239\n",
      "training loss: 2.8461802005767822\n",
      "step 240\n",
      "training loss: 2.8483784198760986\n",
      "validation loss: 2.8490383625030518\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.845919132232666\n",
      "step 242\n",
      "training loss: 2.861811876296997\n",
      "step 243\n",
      "training loss: 2.862475633621216\n",
      "step 244\n",
      "training loss: 2.856630802154541\n",
      "step 245\n",
      "training loss: 2.8657422065734863\n",
      "step 246\n",
      "training loss: 2.872596502304077\n",
      "step 247\n",
      "training loss: 2.844045877456665\n",
      "step 248\n",
      "training loss: 2.871657371520996\n",
      "step 249\n",
      "training loss: 2.878645181655884\n",
      "step 250\n",
      "training loss: 2.847811222076416\n",
      "validation loss: 2.8634936809539795\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8485207557678223\n",
      "step 252\n",
      "training loss: 2.8695240020751953\n",
      "step 253\n",
      "training loss: 2.8654518127441406\n",
      "step 254\n",
      "training loss: 2.865156888961792\n",
      "step 255\n",
      "training loss: 2.8639190196990967\n",
      "step 256\n",
      "training loss: 2.8662476539611816\n",
      "step 257\n",
      "training loss: 2.8573672771453857\n",
      "step 258\n",
      "training loss: 2.8635337352752686\n",
      "step 259\n",
      "training loss: 2.8612730503082275\n",
      "step 260\n",
      "training loss: 2.862043857574463\n",
      "validation loss: 2.8851499557495117\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8580095767974854\n",
      "step 262\n",
      "training loss: 2.8238606452941895\n",
      "step 263\n",
      "training loss: 2.852896213531494\n",
      "step 264\n",
      "training loss: 2.8713598251342773\n",
      "step 265\n",
      "training loss: 2.844994306564331\n",
      "step 266\n",
      "training loss: 2.853381633758545\n",
      "step 267\n",
      "training loss: 2.888772487640381\n",
      "step 268\n",
      "training loss: 2.8630895614624023\n",
      "step 269\n",
      "training loss: 2.876905918121338\n",
      "step 270\n",
      "training loss: 2.879744291305542\n",
      "validation loss: 2.828486442565918\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8690850734710693\n",
      "----------3.0 min per epoch----------\n",
      "epoch 17\n",
      "step 0\n",
      "training loss: 2.845155954360962\n",
      "validation loss: 2.864663600921631\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.874191999435425\n",
      "step 2\n",
      "training loss: 2.8743646144866943\n",
      "step 3\n",
      "training loss: 2.8687057495117188\n",
      "step 4\n",
      "training loss: 2.8653526306152344\n",
      "step 5\n",
      "training loss: 2.878387451171875\n",
      "step 6\n",
      "training loss: 2.885910987854004\n",
      "step 7\n",
      "training loss: 2.8083670139312744\n",
      "step 8\n",
      "training loss: 2.848315715789795\n",
      "step 9\n",
      "training loss: 2.8202483654022217\n",
      "step 10\n",
      "training loss: 2.8517961502075195\n",
      "validation loss: 2.863351345062256\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.876941442489624\n",
      "step 12\n",
      "training loss: 2.8758091926574707\n",
      "step 13\n",
      "training loss: 2.885993242263794\n",
      "step 14\n",
      "training loss: 2.875930070877075\n",
      "step 15\n",
      "training loss: 2.803765296936035\n",
      "step 16\n",
      "training loss: 2.767727851867676\n",
      "step 17\n",
      "training loss: 2.84633469581604\n",
      "step 18\n",
      "training loss: 2.8919456005096436\n",
      "step 19\n",
      "training loss: 2.8724002838134766\n",
      "step 20\n",
      "training loss: 2.8552584648132324\n",
      "validation loss: 2.8941736221313477\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8604345321655273\n",
      "step 22\n",
      "training loss: 2.8756465911865234\n",
      "step 23\n",
      "training loss: 2.87861967086792\n",
      "step 24\n",
      "training loss: 2.879791498184204\n",
      "step 25\n",
      "training loss: 2.865879774093628\n",
      "step 26\n",
      "training loss: 2.8819327354431152\n",
      "step 27\n",
      "training loss: 2.880549907684326\n",
      "step 28\n",
      "training loss: 2.8358774185180664\n",
      "step 29\n",
      "training loss: 2.845858335494995\n",
      "step 30\n",
      "training loss: 2.857379198074341\n",
      "validation loss: 2.9233510494232178\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8584506511688232\n",
      "step 32\n",
      "training loss: 2.8773610591888428\n",
      "step 33\n",
      "training loss: 2.871692180633545\n",
      "step 34\n",
      "training loss: 2.8734290599823\n",
      "step 35\n",
      "training loss: 2.8785996437072754\n",
      "step 36\n",
      "training loss: 2.8688769340515137\n",
      "step 37\n",
      "training loss: 2.8875250816345215\n",
      "step 38\n",
      "training loss: 2.8642656803131104\n",
      "step 39\n",
      "training loss: 2.876194953918457\n",
      "step 40\n",
      "training loss: 2.8853864669799805\n",
      "validation loss: 2.8684895038604736\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.879639148712158\n",
      "step 42\n",
      "training loss: 2.8743200302124023\n",
      "step 43\n",
      "training loss: 2.873769760131836\n",
      "step 44\n",
      "training loss: 2.8607072830200195\n",
      "step 45\n",
      "training loss: 2.840636730194092\n",
      "step 46\n",
      "training loss: 2.8626949787139893\n",
      "step 47\n",
      "training loss: 2.874314546585083\n",
      "step 48\n",
      "training loss: 2.844851016998291\n",
      "step 49\n",
      "training loss: 2.8615405559539795\n",
      "step 50\n",
      "training loss: 2.8639867305755615\n",
      "validation loss: 2.8636255264282227\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.851799964904785\n",
      "step 52\n",
      "training loss: 2.880398750305176\n",
      "step 53\n",
      "training loss: 2.854537010192871\n",
      "step 54\n",
      "training loss: 2.8738460540771484\n",
      "step 55\n",
      "training loss: 2.856281042098999\n",
      "step 56\n",
      "training loss: 2.871215581893921\n",
      "step 57\n",
      "training loss: 2.8237247467041016\n",
      "step 58\n",
      "training loss: 2.8837459087371826\n",
      "step 59\n",
      "training loss: 2.849968433380127\n",
      "step 60\n",
      "training loss: 2.883547306060791\n",
      "validation loss: 2.8318183422088623\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8651678562164307\n",
      "step 62\n",
      "training loss: 2.8481087684631348\n",
      "step 63\n",
      "training loss: 2.84190034866333\n",
      "step 64\n",
      "training loss: 2.865342378616333\n",
      "step 65\n",
      "training loss: 2.851227283477783\n",
      "step 66\n",
      "training loss: 2.8459184169769287\n",
      "step 67\n",
      "training loss: 2.879833221435547\n",
      "step 68\n",
      "training loss: 2.86765718460083\n",
      "step 69\n",
      "training loss: 2.860156297683716\n",
      "step 70\n",
      "training loss: 2.84419584274292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.8322200775146484\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.858602285385132\n",
      "step 72\n",
      "training loss: 2.859825372695923\n",
      "step 73\n",
      "training loss: 2.835875988006592\n",
      "step 74\n",
      "training loss: 2.8500208854675293\n",
      "step 75\n",
      "training loss: 2.852813720703125\n",
      "step 76\n",
      "training loss: 2.8463284969329834\n",
      "step 77\n",
      "training loss: 2.8352513313293457\n",
      "step 78\n",
      "training loss: 2.8715808391571045\n",
      "step 79\n",
      "training loss: 2.8439102172851562\n",
      "step 80\n",
      "training loss: 2.8782575130462646\n",
      "validation loss: 2.8366241455078125\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.863248586654663\n",
      "step 82\n",
      "training loss: 2.839256525039673\n",
      "step 83\n",
      "training loss: 2.8759968280792236\n",
      "step 84\n",
      "training loss: 2.8509767055511475\n",
      "step 85\n",
      "training loss: 2.861400842666626\n",
      "step 86\n",
      "training loss: 2.8657026290893555\n",
      "step 87\n",
      "training loss: 2.830519676208496\n",
      "step 88\n",
      "training loss: 2.875486373901367\n",
      "step 89\n",
      "training loss: 2.8549885749816895\n",
      "step 90\n",
      "training loss: 2.876049041748047\n",
      "validation loss: 2.8601696491241455\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8707454204559326\n",
      "step 92\n",
      "training loss: 2.8942298889160156\n",
      "step 93\n",
      "training loss: 2.8797004222869873\n",
      "step 94\n",
      "training loss: 2.835923671722412\n",
      "step 95\n",
      "training loss: 2.8700270652770996\n",
      "step 96\n",
      "training loss: 2.8583621978759766\n",
      "step 97\n",
      "training loss: 2.847452163696289\n",
      "step 98\n",
      "training loss: 2.858480453491211\n",
      "step 99\n",
      "training loss: 2.8567845821380615\n",
      "step 100\n",
      "training loss: 2.8775794506073\n",
      "validation loss: 2.8689370155334473\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8602454662323\n",
      "step 102\n",
      "training loss: 2.8770642280578613\n",
      "step 103\n",
      "training loss: 2.8766872882843018\n",
      "step 104\n",
      "training loss: 2.878615379333496\n",
      "step 105\n",
      "training loss: 2.855884552001953\n",
      "step 106\n",
      "training loss: 2.867155075073242\n",
      "step 107\n",
      "training loss: 2.863898515701294\n",
      "step 108\n",
      "training loss: 2.8559372425079346\n",
      "step 109\n",
      "training loss: 2.866816759109497\n",
      "step 110\n",
      "training loss: 2.842825412750244\n",
      "validation loss: 2.8628902435302734\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8682701587677\n",
      "step 112\n",
      "training loss: 2.8713905811309814\n",
      "step 113\n",
      "training loss: 2.8156676292419434\n",
      "step 114\n",
      "training loss: 2.855027675628662\n",
      "step 115\n",
      "training loss: 2.8477983474731445\n",
      "step 116\n",
      "training loss: 2.8653512001037598\n",
      "step 117\n",
      "training loss: 2.8720638751983643\n",
      "step 118\n",
      "training loss: 2.8443024158477783\n",
      "step 119\n",
      "training loss: 2.8658671379089355\n",
      "step 120\n",
      "training loss: 2.8193728923797607\n",
      "validation loss: 2.859753370285034\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8526597023010254\n",
      "step 122\n",
      "training loss: 2.8733673095703125\n",
      "step 123\n",
      "training loss: 2.8669631481170654\n",
      "step 124\n",
      "training loss: 2.858112335205078\n",
      "step 125\n",
      "training loss: 2.8454644680023193\n",
      "step 126\n",
      "training loss: 2.863926410675049\n",
      "step 127\n",
      "training loss: 2.8513681888580322\n",
      "step 128\n",
      "training loss: 2.86072039604187\n",
      "step 129\n",
      "training loss: 2.8661820888519287\n",
      "step 130\n",
      "training loss: 2.8613440990448\n",
      "validation loss: 2.86889910697937\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8567962646484375\n",
      "step 132\n",
      "training loss: 2.8601279258728027\n",
      "step 133\n",
      "training loss: 2.8376052379608154\n",
      "step 134\n",
      "training loss: 2.840940237045288\n",
      "step 135\n",
      "training loss: 2.8300070762634277\n",
      "step 136\n",
      "training loss: 2.8654417991638184\n",
      "step 137\n",
      "training loss: 2.8694913387298584\n",
      "step 138\n",
      "training loss: 2.8622193336486816\n",
      "step 139\n",
      "training loss: 2.8614730834960938\n",
      "step 140\n",
      "training loss: 2.853691339492798\n",
      "validation loss: 2.8453989028930664\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.870225191116333\n",
      "step 142\n",
      "training loss: 2.891063928604126\n",
      "step 143\n",
      "training loss: 2.8127121925354004\n",
      "step 144\n",
      "training loss: 2.8600339889526367\n",
      "step 145\n",
      "training loss: 2.8509252071380615\n",
      "step 146\n",
      "training loss: 2.8757054805755615\n",
      "step 147\n",
      "training loss: 2.8604142665863037\n",
      "step 148\n",
      "training loss: 2.880845308303833\n",
      "step 149\n",
      "training loss: 2.8541359901428223\n",
      "step 150\n",
      "training loss: 2.8430516719818115\n",
      "validation loss: 2.8486764430999756\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8544020652770996\n",
      "step 152\n",
      "training loss: 2.8658714294433594\n",
      "step 153\n",
      "training loss: 2.849942445755005\n",
      "step 154\n",
      "training loss: 2.876579761505127\n",
      "step 155\n",
      "training loss: 2.862333297729492\n",
      "step 156\n",
      "training loss: 2.8261499404907227\n",
      "step 157\n",
      "training loss: 2.852268934249878\n",
      "step 158\n",
      "training loss: 2.851402521133423\n",
      "step 159\n",
      "training loss: 2.855649948120117\n",
      "step 160\n",
      "training loss: 2.8668978214263916\n",
      "validation loss: 2.834012985229492\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.864964723587036\n",
      "step 162\n",
      "training loss: 2.8678698539733887\n",
      "step 163\n",
      "training loss: 2.8155534267425537\n",
      "step 164\n",
      "training loss: 2.8630363941192627\n",
      "step 165\n",
      "training loss: 2.8543808460235596\n",
      "step 166\n",
      "training loss: 2.86377215385437\n",
      "step 167\n",
      "training loss: 2.8624017238616943\n",
      "step 168\n",
      "training loss: 2.874657154083252\n",
      "step 169\n",
      "training loss: 2.868128776550293\n",
      "step 170\n",
      "training loss: 2.8502378463745117\n",
      "validation loss: 2.8406572341918945\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8779239654541016\n",
      "step 172\n",
      "training loss: 2.873809814453125\n",
      "step 173\n",
      "training loss: 2.8649072647094727\n",
      "step 174\n",
      "training loss: 2.859802484512329\n",
      "step 175\n",
      "training loss: 2.8678317070007324\n",
      "step 176\n",
      "training loss: 2.859039068222046\n",
      "step 177\n",
      "training loss: 2.8579866886138916\n",
      "step 178\n",
      "training loss: 2.839470148086548\n",
      "step 179\n",
      "training loss: 2.8223068714141846\n",
      "step 180\n",
      "training loss: 2.8709068298339844\n",
      "validation loss: 2.871823787689209\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8491623401641846\n",
      "step 182\n",
      "training loss: 2.8508546352386475\n",
      "step 183\n",
      "training loss: 2.864574670791626\n",
      "step 184\n",
      "training loss: 2.892920732498169\n",
      "step 185\n",
      "training loss: 2.8456923961639404\n",
      "step 186\n",
      "training loss: 2.873697280883789\n",
      "step 187\n",
      "training loss: 2.861898422241211\n",
      "step 188\n",
      "training loss: 2.8696625232696533\n",
      "step 189\n",
      "training loss: 2.8485190868377686\n",
      "step 190\n",
      "training loss: 2.8590433597564697\n",
      "validation loss: 2.873300075531006\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8358054161071777\n",
      "step 192\n",
      "training loss: 2.85127854347229\n",
      "step 193\n",
      "training loss: 2.8575823307037354\n",
      "step 194\n",
      "training loss: 2.8452682495117188\n",
      "step 195\n",
      "training loss: 2.885901689529419\n",
      "step 196\n",
      "training loss: 2.85929012298584\n",
      "step 197\n",
      "training loss: 2.853090286254883\n",
      "step 198\n",
      "training loss: 2.864950180053711\n",
      "step 199\n",
      "training loss: 2.8658595085144043\n",
      "step 200\n",
      "training loss: 2.85298490524292\n",
      "validation loss: 2.832044839859009\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8425421714782715\n",
      "step 202\n",
      "training loss: 2.8315374851226807\n",
      "step 203\n",
      "training loss: 2.8197500705718994\n",
      "step 204\n",
      "training loss: 2.8773694038391113\n",
      "step 205\n",
      "training loss: 2.8525004386901855\n",
      "step 206\n",
      "training loss: 2.878748893737793\n",
      "step 207\n",
      "training loss: 2.8520936965942383\n",
      "step 208\n",
      "training loss: 2.8646793365478516\n",
      "step 209\n",
      "training loss: 2.8810508251190186\n",
      "step 210\n",
      "training loss: 2.8577659130096436\n",
      "validation loss: 2.8502397537231445\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.85577392578125\n",
      "step 212\n",
      "training loss: 2.8679394721984863\n",
      "step 213\n",
      "training loss: 2.843965768814087\n",
      "step 214\n",
      "training loss: 2.8532166481018066\n",
      "step 215\n",
      "training loss: 2.8473904132843018\n",
      "step 216\n",
      "training loss: 2.851322889328003\n",
      "step 217\n",
      "training loss: 2.8398871421813965\n",
      "step 218\n",
      "training loss: 2.860373020172119\n",
      "step 219\n",
      "training loss: 2.8636128902435303\n",
      "step 220\n",
      "training loss: 2.8448963165283203\n",
      "validation loss: 2.884892225265503\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.875231981277466\n",
      "step 222\n",
      "training loss: 2.850264310836792\n",
      "step 223\n",
      "training loss: 2.8624231815338135\n",
      "step 224\n",
      "training loss: 2.8587546348571777\n",
      "step 225\n",
      "training loss: 2.8654072284698486\n",
      "step 226\n",
      "training loss: 2.787815570831299\n",
      "step 227\n",
      "training loss: 2.8492603302001953\n",
      "step 228\n",
      "training loss: 2.8320469856262207\n",
      "step 229\n",
      "training loss: 2.8486685752868652\n",
      "step 230\n",
      "training loss: 2.8692634105682373\n",
      "validation loss: 2.879075765609741\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8744678497314453\n",
      "step 232\n",
      "training loss: 2.8630318641662598\n",
      "step 233\n",
      "training loss: 2.862213134765625\n",
      "step 234\n",
      "training loss: 2.859595775604248\n",
      "step 235\n",
      "training loss: 2.8489222526550293\n",
      "step 236\n",
      "training loss: 2.855922222137451\n",
      "step 237\n",
      "training loss: 2.8715720176696777\n",
      "step 238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.888251781463623\n",
      "step 239\n",
      "training loss: 2.856370687484741\n",
      "step 240\n",
      "training loss: 2.844534158706665\n",
      "validation loss: 2.8719358444213867\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8497612476348877\n",
      "step 242\n",
      "training loss: 2.84538197517395\n",
      "step 243\n",
      "training loss: 2.8616607189178467\n",
      "step 244\n",
      "training loss: 2.862687349319458\n",
      "step 245\n",
      "training loss: 2.856766939163208\n",
      "step 246\n",
      "training loss: 2.8659145832061768\n",
      "step 247\n",
      "training loss: 2.874772548675537\n",
      "step 248\n",
      "training loss: 2.850306510925293\n",
      "step 249\n",
      "training loss: 2.8715009689331055\n",
      "step 250\n",
      "training loss: 2.87861704826355\n",
      "validation loss: 2.9394750595092773\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8467113971710205\n",
      "step 252\n",
      "training loss: 2.8485894203186035\n",
      "step 253\n",
      "training loss: 2.871323347091675\n",
      "step 254\n",
      "training loss: 2.866347551345825\n",
      "step 255\n",
      "training loss: 2.865304470062256\n",
      "step 256\n",
      "training loss: 2.864797830581665\n",
      "step 257\n",
      "training loss: 2.8671343326568604\n",
      "step 258\n",
      "training loss: 2.8564486503601074\n",
      "step 259\n",
      "training loss: 2.8637382984161377\n",
      "step 260\n",
      "training loss: 2.8619489669799805\n",
      "validation loss: 2.8769750595092773\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8627562522888184\n",
      "step 262\n",
      "training loss: 2.859353542327881\n",
      "step 263\n",
      "training loss: 2.8228089809417725\n",
      "step 264\n",
      "training loss: 2.851477861404419\n",
      "step 265\n",
      "training loss: 2.871502161026001\n",
      "step 266\n",
      "training loss: 2.845407724380493\n",
      "step 267\n",
      "training loss: 2.8514554500579834\n",
      "step 268\n",
      "training loss: 2.887671947479248\n",
      "step 269\n",
      "training loss: 2.863380193710327\n",
      "step 270\n",
      "training loss: 2.877976894378662\n",
      "validation loss: 2.8525993824005127\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.881319522857666\n",
      "----------3.0 min per epoch----------\n",
      "epoch 18\n",
      "step 0\n",
      "training loss: 2.8698298931121826\n",
      "validation loss: 2.8645102977752686\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8461947441101074\n",
      "step 2\n",
      "training loss: 2.8785128593444824\n",
      "step 3\n",
      "training loss: 2.879009485244751\n",
      "step 4\n",
      "training loss: 2.870124578475952\n",
      "step 5\n",
      "training loss: 2.8673858642578125\n",
      "step 6\n",
      "training loss: 2.8756792545318604\n",
      "step 7\n",
      "training loss: 2.886490821838379\n",
      "step 8\n",
      "training loss: 2.807609796524048\n",
      "step 9\n",
      "training loss: 2.8495004177093506\n",
      "step 10\n",
      "training loss: 2.8211567401885986\n",
      "validation loss: 2.8983168601989746\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.855238914489746\n",
      "step 12\n",
      "training loss: 2.880366325378418\n",
      "step 13\n",
      "training loss: 2.8769938945770264\n",
      "step 14\n",
      "training loss: 2.8842875957489014\n",
      "step 15\n",
      "training loss: 2.872041702270508\n",
      "step 16\n",
      "training loss: 2.8027279376983643\n",
      "step 17\n",
      "training loss: 2.771059513092041\n",
      "step 18\n",
      "training loss: 2.849497079849243\n",
      "step 19\n",
      "training loss: 2.8839797973632812\n",
      "step 20\n",
      "training loss: 2.869990348815918\n",
      "validation loss: 2.8216946125030518\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.855989933013916\n",
      "step 22\n",
      "training loss: 2.8605010509490967\n",
      "step 23\n",
      "training loss: 2.8753297328948975\n",
      "step 24\n",
      "training loss: 2.879154682159424\n",
      "step 25\n",
      "training loss: 2.882016181945801\n",
      "step 26\n",
      "training loss: 2.869433879852295\n",
      "step 27\n",
      "training loss: 2.881640911102295\n",
      "step 28\n",
      "training loss: 2.879755973815918\n",
      "step 29\n",
      "training loss: 2.835310220718384\n",
      "step 30\n",
      "training loss: 2.8427770137786865\n",
      "validation loss: 2.8657824993133545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.857180595397949\n",
      "step 32\n",
      "training loss: 2.8585386276245117\n",
      "step 33\n",
      "training loss: 2.879026412963867\n",
      "step 34\n",
      "training loss: 2.868420362472534\n",
      "step 35\n",
      "training loss: 2.8741400241851807\n",
      "step 36\n",
      "training loss: 2.877777576446533\n",
      "step 37\n",
      "training loss: 2.8691482543945312\n",
      "step 38\n",
      "training loss: 2.8855881690979004\n",
      "step 39\n",
      "training loss: 2.8633296489715576\n",
      "step 40\n",
      "training loss: 2.8756697177886963\n",
      "validation loss: 2.868914842605591\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.885385513305664\n",
      "step 42\n",
      "training loss: 2.8778676986694336\n",
      "step 43\n",
      "training loss: 2.874495506286621\n",
      "step 44\n",
      "training loss: 2.8767197132110596\n",
      "step 45\n",
      "training loss: 2.8629448413848877\n",
      "step 46\n",
      "training loss: 2.838404417037964\n",
      "step 47\n",
      "training loss: 2.864999771118164\n",
      "step 48\n",
      "training loss: 2.875774621963501\n",
      "step 49\n",
      "training loss: 2.846498489379883\n",
      "step 50\n",
      "training loss: 2.862868070602417\n",
      "validation loss: 2.8719727993011475\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.861116886138916\n",
      "step 52\n",
      "training loss: 2.8531765937805176\n",
      "step 53\n",
      "training loss: 2.880794048309326\n",
      "step 54\n",
      "training loss: 2.8557798862457275\n",
      "step 55\n",
      "training loss: 2.873067617416382\n",
      "step 56\n",
      "training loss: 2.8553757667541504\n",
      "step 57\n",
      "training loss: 2.872168779373169\n",
      "step 58\n",
      "training loss: 2.8267951011657715\n",
      "step 59\n",
      "training loss: 2.8835537433624268\n",
      "step 60\n",
      "training loss: 2.8498048782348633\n",
      "validation loss: 2.915241241455078\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8842806816101074\n",
      "step 62\n",
      "training loss: 2.8671212196350098\n",
      "step 63\n",
      "training loss: 2.8488452434539795\n",
      "step 64\n",
      "training loss: 2.843125104904175\n",
      "step 65\n",
      "training loss: 2.865809440612793\n",
      "step 66\n",
      "training loss: 2.852458953857422\n",
      "step 67\n",
      "training loss: 2.845278739929199\n",
      "step 68\n",
      "training loss: 2.881727695465088\n",
      "step 69\n",
      "training loss: 2.8668365478515625\n",
      "step 70\n",
      "training loss: 2.8601911067962646\n",
      "validation loss: 2.866921901702881\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.844841718673706\n",
      "step 72\n",
      "training loss: 2.8598928451538086\n",
      "step 73\n",
      "training loss: 2.859736680984497\n",
      "step 74\n",
      "training loss: 2.833447217941284\n",
      "step 75\n",
      "training loss: 2.850687265396118\n",
      "step 76\n",
      "training loss: 2.8507466316223145\n",
      "step 77\n",
      "training loss: 2.8441221714019775\n",
      "step 78\n",
      "training loss: 2.836650848388672\n",
      "step 79\n",
      "training loss: 2.8723130226135254\n",
      "step 80\n",
      "training loss: 2.8444271087646484\n",
      "validation loss: 2.860821485519409\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8793070316314697\n",
      "step 82\n",
      "training loss: 2.861463785171509\n",
      "step 83\n",
      "training loss: 2.8409674167633057\n",
      "step 84\n",
      "training loss: 2.875997304916382\n",
      "step 85\n",
      "training loss: 2.851977586746216\n",
      "step 86\n",
      "training loss: 2.8597536087036133\n",
      "step 87\n",
      "training loss: 2.863255262374878\n",
      "step 88\n",
      "training loss: 2.8311033248901367\n",
      "step 89\n",
      "training loss: 2.8766770362854004\n",
      "step 90\n",
      "training loss: 2.857377052307129\n",
      "validation loss: 2.8319931030273438\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.874652862548828\n",
      "step 92\n",
      "training loss: 2.872169256210327\n",
      "step 93\n",
      "training loss: 2.8943870067596436\n",
      "step 94\n",
      "training loss: 2.879211187362671\n",
      "step 95\n",
      "training loss: 2.834850311279297\n",
      "step 96\n",
      "training loss: 2.8718018531799316\n",
      "step 97\n",
      "training loss: 2.860736846923828\n",
      "step 98\n",
      "training loss: 2.848397731781006\n",
      "step 99\n",
      "training loss: 2.857858180999756\n",
      "step 100\n",
      "training loss: 2.861142635345459\n",
      "validation loss: 2.8314898014068604\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8790440559387207\n",
      "step 102\n",
      "training loss: 2.8623533248901367\n",
      "step 103\n",
      "training loss: 2.877840757369995\n",
      "step 104\n",
      "training loss: 2.8746392726898193\n",
      "step 105\n",
      "training loss: 2.8796799182891846\n",
      "step 106\n",
      "training loss: 2.855802059173584\n",
      "step 107\n",
      "training loss: 2.869528293609619\n",
      "step 108\n",
      "training loss: 2.8656044006347656\n",
      "step 109\n",
      "training loss: 2.854736089706421\n",
      "step 110\n",
      "training loss: 2.868773937225342\n",
      "validation loss: 2.8351402282714844\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.845855474472046\n",
      "step 112\n",
      "training loss: 2.869384765625\n",
      "step 113\n",
      "training loss: 2.8733694553375244\n",
      "step 114\n",
      "training loss: 2.816643238067627\n",
      "step 115\n",
      "training loss: 2.8551745414733887\n",
      "step 116\n",
      "training loss: 2.8493895530700684\n",
      "step 117\n",
      "training loss: 2.86637544631958\n",
      "step 118\n",
      "training loss: 2.87381911277771\n",
      "step 119\n",
      "training loss: 2.843658924102783\n",
      "step 120\n",
      "training loss: 2.867398500442505\n",
      "validation loss: 2.8587160110473633\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.816880941390991\n",
      "step 122\n",
      "training loss: 2.8533215522766113\n",
      "step 123\n",
      "training loss: 2.8740546703338623\n",
      "step 124\n",
      "training loss: 2.8654778003692627\n",
      "step 125\n",
      "training loss: 2.8590941429138184\n",
      "step 126\n",
      "training loss: 2.848278760910034\n",
      "step 127\n",
      "training loss: 2.8659534454345703\n",
      "step 128\n",
      "training loss: 2.852968692779541\n",
      "step 129\n",
      "training loss: 2.8628652095794678\n",
      "step 130\n",
      "training loss: 2.8670239448547363\n",
      "validation loss: 2.8685107231140137\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8617067337036133\n",
      "step 132\n",
      "training loss: 2.85823917388916\n",
      "step 133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.861111879348755\n",
      "step 134\n",
      "training loss: 2.836315631866455\n",
      "step 135\n",
      "training loss: 2.840609073638916\n",
      "step 136\n",
      "training loss: 2.833108425140381\n",
      "step 137\n",
      "training loss: 2.868177890777588\n",
      "step 138\n",
      "training loss: 2.868908166885376\n",
      "step 139\n",
      "training loss: 2.8613975048065186\n",
      "step 140\n",
      "training loss: 2.8635551929473877\n",
      "validation loss: 2.8595051765441895\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8519933223724365\n",
      "step 142\n",
      "training loss: 2.8691182136535645\n",
      "step 143\n",
      "training loss: 2.8897714614868164\n",
      "step 144\n",
      "training loss: 2.8078410625457764\n",
      "step 145\n",
      "training loss: 2.860473871231079\n",
      "step 146\n",
      "training loss: 2.8504443168640137\n",
      "step 147\n",
      "training loss: 2.874675750732422\n",
      "step 148\n",
      "training loss: 2.8581480979919434\n",
      "step 149\n",
      "training loss: 2.8806533813476562\n",
      "step 150\n",
      "training loss: 2.855102062225342\n",
      "validation loss: 2.8574795722961426\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.841458797454834\n",
      "step 152\n",
      "training loss: 2.853210687637329\n",
      "step 153\n",
      "training loss: 2.865593671798706\n",
      "step 154\n",
      "training loss: 2.8489842414855957\n",
      "step 155\n",
      "training loss: 2.8736348152160645\n",
      "step 156\n",
      "training loss: 2.859722852706909\n",
      "step 157\n",
      "training loss: 2.8246381282806396\n",
      "step 158\n",
      "training loss: 2.851759433746338\n",
      "step 159\n",
      "training loss: 2.8558006286621094\n",
      "step 160\n",
      "training loss: 2.8543789386749268\n",
      "validation loss: 2.8635499477386475\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.868380308151245\n",
      "step 162\n",
      "training loss: 2.864628314971924\n",
      "step 163\n",
      "training loss: 2.867037773132324\n",
      "step 164\n",
      "training loss: 2.8162879943847656\n",
      "step 165\n",
      "training loss: 2.8643059730529785\n",
      "step 166\n",
      "training loss: 2.8566296100616455\n",
      "step 167\n",
      "training loss: 2.862053155899048\n",
      "step 168\n",
      "training loss: 2.8643136024475098\n",
      "step 169\n",
      "training loss: 2.873164415359497\n",
      "step 170\n",
      "training loss: 2.8681113719940186\n",
      "validation loss: 2.8435869216918945\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.852621078491211\n",
      "step 172\n",
      "training loss: 2.8779399394989014\n",
      "step 173\n",
      "training loss: 2.875687599182129\n",
      "step 174\n",
      "training loss: 2.866046667098999\n",
      "step 175\n",
      "training loss: 2.8595826625823975\n",
      "step 176\n",
      "training loss: 2.8690948486328125\n",
      "step 177\n",
      "training loss: 2.8597869873046875\n",
      "step 178\n",
      "training loss: 2.8599679470062256\n",
      "step 179\n",
      "training loss: 2.8386731147766113\n",
      "step 180\n",
      "training loss: 2.821903944015503\n",
      "validation loss: 2.846752882003784\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8699541091918945\n",
      "step 182\n",
      "training loss: 2.849782705307007\n",
      "step 183\n",
      "training loss: 2.850295305252075\n",
      "step 184\n",
      "training loss: 2.8602304458618164\n",
      "step 185\n",
      "training loss: 2.8934571743011475\n",
      "step 186\n",
      "training loss: 2.8426318168640137\n",
      "step 187\n",
      "training loss: 2.871997833251953\n",
      "step 188\n",
      "training loss: 2.8629446029663086\n",
      "step 189\n",
      "training loss: 2.869593620300293\n",
      "step 190\n",
      "training loss: 2.8477745056152344\n",
      "validation loss: 2.8358752727508545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8608407974243164\n",
      "step 192\n",
      "training loss: 2.8358755111694336\n",
      "step 193\n",
      "training loss: 2.8498589992523193\n",
      "step 194\n",
      "training loss: 2.8604793548583984\n",
      "step 195\n",
      "training loss: 2.8464503288269043\n",
      "step 196\n",
      "training loss: 2.8870809078216553\n",
      "step 197\n",
      "training loss: 2.8584864139556885\n",
      "step 198\n",
      "training loss: 2.85306453704834\n",
      "step 199\n",
      "training loss: 2.8636693954467773\n",
      "step 200\n",
      "training loss: 2.8689470291137695\n",
      "validation loss: 2.842832326889038\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.853130340576172\n",
      "step 202\n",
      "training loss: 2.8424019813537598\n",
      "step 203\n",
      "training loss: 2.8284642696380615\n",
      "step 204\n",
      "training loss: 2.815964698791504\n",
      "step 205\n",
      "training loss: 2.875554323196411\n",
      "step 206\n",
      "training loss: 2.853881597518921\n",
      "step 207\n",
      "training loss: 2.8810389041900635\n",
      "step 208\n",
      "training loss: 2.8537375926971436\n",
      "step 209\n",
      "training loss: 2.8656888008117676\n",
      "step 210\n",
      "training loss: 2.8807969093322754\n",
      "validation loss: 2.870664358139038\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.857623815536499\n",
      "step 212\n",
      "training loss: 2.853099822998047\n",
      "step 213\n",
      "training loss: 2.867122173309326\n",
      "step 214\n",
      "training loss: 2.84490966796875\n",
      "step 215\n",
      "training loss: 2.852384567260742\n",
      "step 216\n",
      "training loss: 2.8462488651275635\n",
      "step 217\n",
      "training loss: 2.850999355316162\n",
      "step 218\n",
      "training loss: 2.838247776031494\n",
      "step 219\n",
      "training loss: 2.859154224395752\n",
      "step 220\n",
      "training loss: 2.8632373809814453\n",
      "validation loss: 2.876037359237671\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.845256805419922\n",
      "step 222\n",
      "training loss: 2.875594139099121\n",
      "step 223\n",
      "training loss: 2.852072238922119\n",
      "step 224\n",
      "training loss: 2.863755941390991\n",
      "step 225\n",
      "training loss: 2.857224225997925\n",
      "step 226\n",
      "training loss: 2.867858409881592\n",
      "step 227\n",
      "training loss: 2.7909605503082275\n",
      "step 228\n",
      "training loss: 2.8498058319091797\n",
      "step 229\n",
      "training loss: 2.8314571380615234\n",
      "step 230\n",
      "training loss: 2.846453905105591\n",
      "validation loss: 2.832496404647827\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.870946168899536\n",
      "step 232\n",
      "training loss: 2.8731746673583984\n",
      "step 233\n",
      "training loss: 2.862731456756592\n",
      "step 234\n",
      "training loss: 2.8625004291534424\n",
      "step 235\n",
      "training loss: 2.8584399223327637\n",
      "step 236\n",
      "training loss: 2.847651243209839\n",
      "step 237\n",
      "training loss: 2.855808734893799\n",
      "step 238\n",
      "training loss: 2.8702809810638428\n",
      "step 239\n",
      "training loss: 2.886627435684204\n",
      "step 240\n",
      "training loss: 2.8564577102661133\n",
      "validation loss: 2.8502140045166016\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8452072143554688\n",
      "step 242\n",
      "training loss: 2.848327159881592\n",
      "step 243\n",
      "training loss: 2.8457868099212646\n",
      "step 244\n",
      "training loss: 2.8608920574188232\n",
      "step 245\n",
      "training loss: 2.864356756210327\n",
      "step 246\n",
      "training loss: 2.858603000640869\n",
      "step 247\n",
      "training loss: 2.8657517433166504\n",
      "step 248\n",
      "training loss: 2.8741400241851807\n",
      "step 249\n",
      "training loss: 2.8479206562042236\n",
      "step 250\n",
      "training loss: 2.8727846145629883\n",
      "validation loss: 2.8825583457946777\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.876288414001465\n",
      "step 252\n",
      "training loss: 2.845405340194702\n",
      "step 253\n",
      "training loss: 2.848898410797119\n",
      "step 254\n",
      "training loss: 2.8736279010772705\n",
      "step 255\n",
      "training loss: 2.8658270835876465\n",
      "step 256\n",
      "training loss: 2.8646066188812256\n",
      "step 257\n",
      "training loss: 2.864597797393799\n",
      "step 258\n",
      "training loss: 2.868206739425659\n",
      "step 259\n",
      "training loss: 2.8593175411224365\n",
      "step 260\n",
      "training loss: 2.864652395248413\n",
      "validation loss: 2.8726723194122314\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.862220048904419\n",
      "step 262\n",
      "training loss: 2.8608875274658203\n",
      "step 263\n",
      "training loss: 2.857719898223877\n",
      "step 264\n",
      "training loss: 2.8259360790252686\n",
      "step 265\n",
      "training loss: 2.8511531352996826\n",
      "step 266\n",
      "training loss: 2.8689677715301514\n",
      "step 267\n",
      "training loss: 2.8489363193511963\n",
      "step 268\n",
      "training loss: 2.857520341873169\n",
      "step 269\n",
      "training loss: 2.886709690093994\n",
      "step 270\n",
      "training loss: 2.863959312438965\n",
      "validation loss: 2.863715648651123\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8783411979675293\n",
      "----------3.0 min per epoch----------\n",
      "epoch 19\n",
      "step 0\n",
      "training loss: 2.883366584777832\n",
      "validation loss: 2.944042682647705\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8693950176239014\n",
      "step 2\n",
      "training loss: 2.8438634872436523\n",
      "step 3\n",
      "training loss: 2.8755362033843994\n",
      "step 4\n",
      "training loss: 2.8787875175476074\n",
      "step 5\n",
      "training loss: 2.868560314178467\n",
      "step 6\n",
      "training loss: 2.8648457527160645\n",
      "step 7\n",
      "training loss: 2.876112937927246\n",
      "step 8\n",
      "training loss: 2.885359287261963\n",
      "step 9\n",
      "training loss: 2.8057944774627686\n",
      "step 10\n",
      "training loss: 2.8487014770507812\n",
      "validation loss: 2.879450798034668\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.818284749984741\n",
      "step 12\n",
      "training loss: 2.854586124420166\n",
      "step 13\n",
      "training loss: 2.8787686824798584\n",
      "step 14\n",
      "training loss: 2.873286008834839\n",
      "step 15\n",
      "training loss: 2.884401559829712\n",
      "step 16\n",
      "training loss: 2.8708343505859375\n",
      "step 17\n",
      "training loss: 2.799686908721924\n",
      "step 18\n",
      "training loss: 2.7672371864318848\n",
      "step 19\n",
      "training loss: 2.84490704536438\n",
      "step 20\n",
      "training loss: 2.883657693862915\n",
      "validation loss: 2.8392086029052734\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.871232509613037\n",
      "step 22\n",
      "training loss: 2.8511011600494385\n",
      "step 23\n",
      "training loss: 2.861163377761841\n",
      "step 24\n",
      "training loss: 2.871419668197632\n",
      "step 25\n",
      "training loss: 2.876002788543701\n",
      "step 26\n",
      "training loss: 2.877614974975586\n",
      "step 27\n",
      "training loss: 2.865719795227051\n",
      "step 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.882047176361084\n",
      "step 29\n",
      "training loss: 2.879563331604004\n",
      "step 30\n",
      "training loss: 2.8343002796173096\n",
      "validation loss: 2.867541790008545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8435287475585938\n",
      "step 32\n",
      "training loss: 2.858217716217041\n",
      "step 33\n",
      "training loss: 2.858961582183838\n",
      "step 34\n",
      "training loss: 2.8731918334960938\n",
      "step 35\n",
      "training loss: 2.8672521114349365\n",
      "step 36\n",
      "training loss: 2.871000051498413\n",
      "step 37\n",
      "training loss: 2.87544584274292\n",
      "step 38\n",
      "training loss: 2.8684298992156982\n",
      "step 39\n",
      "training loss: 2.8867857456207275\n",
      "step 40\n",
      "training loss: 2.862078905105591\n",
      "validation loss: 2.8918778896331787\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.87488055229187\n",
      "step 42\n",
      "training loss: 2.881377696990967\n",
      "step 43\n",
      "training loss: 2.8787710666656494\n",
      "step 44\n",
      "training loss: 2.871385097503662\n",
      "step 45\n",
      "training loss: 2.872255802154541\n",
      "step 46\n",
      "training loss: 2.858283758163452\n",
      "step 47\n",
      "training loss: 2.838721513748169\n",
      "step 48\n",
      "training loss: 2.8632521629333496\n",
      "step 49\n",
      "training loss: 2.8698737621307373\n",
      "step 50\n",
      "training loss: 2.8431713581085205\n",
      "validation loss: 2.8329102993011475\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.861300230026245\n",
      "step 52\n",
      "training loss: 2.8631207942962646\n",
      "step 53\n",
      "training loss: 2.849691390991211\n",
      "step 54\n",
      "training loss: 2.8803014755249023\n",
      "step 55\n",
      "training loss: 2.8567698001861572\n",
      "step 56\n",
      "training loss: 2.870501756668091\n",
      "step 57\n",
      "training loss: 2.85501766204834\n",
      "step 58\n",
      "training loss: 2.8711061477661133\n",
      "step 59\n",
      "training loss: 2.8222084045410156\n",
      "step 60\n",
      "training loss: 2.882631540298462\n",
      "validation loss: 2.8631136417388916\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.848940372467041\n",
      "step 62\n",
      "training loss: 2.8787848949432373\n",
      "step 63\n",
      "training loss: 2.864337921142578\n",
      "step 64\n",
      "training loss: 2.8482120037078857\n",
      "step 65\n",
      "training loss: 2.8444106578826904\n",
      "step 66\n",
      "training loss: 2.864872694015503\n",
      "step 67\n",
      "training loss: 2.8529977798461914\n",
      "step 68\n",
      "training loss: 2.8437788486480713\n",
      "step 69\n",
      "training loss: 2.879436492919922\n",
      "step 70\n",
      "training loss: 2.8660476207733154\n",
      "validation loss: 2.8645074367523193\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8584201335906982\n",
      "step 72\n",
      "training loss: 2.8435800075531006\n",
      "step 73\n",
      "training loss: 2.8584883213043213\n",
      "step 74\n",
      "training loss: 2.8590760231018066\n",
      "step 75\n",
      "training loss: 2.833688497543335\n",
      "step 76\n",
      "training loss: 2.8490099906921387\n",
      "step 77\n",
      "training loss: 2.849043607711792\n",
      "step 78\n",
      "training loss: 2.8461995124816895\n",
      "step 79\n",
      "training loss: 2.8366410732269287\n",
      "step 80\n",
      "training loss: 2.8711953163146973\n",
      "validation loss: 2.8664023876190186\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8441643714904785\n",
      "step 82\n",
      "training loss: 2.8775715827941895\n",
      "step 83\n",
      "training loss: 2.8602325916290283\n",
      "step 84\n",
      "training loss: 2.8365931510925293\n",
      "step 85\n",
      "training loss: 2.874051094055176\n",
      "step 86\n",
      "training loss: 2.8498456478118896\n",
      "step 87\n",
      "training loss: 2.8610317707061768\n",
      "step 88\n",
      "training loss: 2.8642537593841553\n",
      "step 89\n",
      "training loss: 2.8314156532287598\n",
      "step 90\n",
      "training loss: 2.876979351043701\n",
      "validation loss: 2.8980889320373535\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8568804264068604\n",
      "step 92\n",
      "training loss: 2.8758046627044678\n",
      "step 93\n",
      "training loss: 2.8710885047912598\n",
      "step 94\n",
      "training loss: 2.8931984901428223\n",
      "step 95\n",
      "training loss: 2.8795316219329834\n",
      "step 96\n",
      "training loss: 2.831315040588379\n",
      "step 97\n",
      "training loss: 2.8719711303710938\n",
      "step 98\n",
      "training loss: 2.85776424407959\n",
      "step 99\n",
      "training loss: 2.8476154804229736\n",
      "step 100\n",
      "training loss: 2.8571174144744873\n",
      "validation loss: 2.8649110794067383\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8561792373657227\n",
      "step 102\n",
      "training loss: 2.8775858879089355\n",
      "step 103\n",
      "training loss: 2.8596417903900146\n",
      "step 104\n",
      "training loss: 2.8761236667633057\n",
      "step 105\n",
      "training loss: 2.877497911453247\n",
      "step 106\n",
      "training loss: 2.8787198066711426\n",
      "step 107\n",
      "training loss: 2.856861114501953\n",
      "step 108\n",
      "training loss: 2.86763596534729\n",
      "step 109\n",
      "training loss: 2.862415075302124\n",
      "step 110\n",
      "training loss: 2.857072353363037\n",
      "validation loss: 2.8629393577575684\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8658759593963623\n",
      "step 112\n",
      "training loss: 2.8425116539001465\n",
      "step 113\n",
      "training loss: 2.866610527038574\n",
      "step 114\n",
      "training loss: 2.8727641105651855\n",
      "step 115\n",
      "training loss: 2.8142688274383545\n",
      "step 116\n",
      "training loss: 2.8550331592559814\n",
      "step 117\n",
      "training loss: 2.850579023361206\n",
      "step 118\n",
      "training loss: 2.8655753135681152\n",
      "step 119\n",
      "training loss: 2.872885227203369\n",
      "step 120\n",
      "training loss: 2.8446848392486572\n",
      "validation loss: 2.8264808654785156\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8653972148895264\n",
      "step 122\n",
      "training loss: 2.813628673553467\n",
      "step 123\n",
      "training loss: 2.851088523864746\n",
      "step 124\n",
      "training loss: 2.874887466430664\n",
      "step 125\n",
      "training loss: 2.8659822940826416\n",
      "step 126\n",
      "training loss: 2.8603498935699463\n",
      "step 127\n",
      "training loss: 2.8484811782836914\n",
      "step 128\n",
      "training loss: 2.8673434257507324\n",
      "step 129\n",
      "training loss: 2.8509678840637207\n",
      "step 130\n",
      "training loss: 2.8618412017822266\n",
      "validation loss: 2.8297908306121826\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8673698902130127\n",
      "step 132\n",
      "training loss: 2.8596765995025635\n",
      "step 133\n",
      "training loss: 2.859537124633789\n",
      "step 134\n",
      "training loss: 2.8590328693389893\n",
      "step 135\n",
      "training loss: 2.8368029594421387\n",
      "step 136\n",
      "training loss: 2.840149164199829\n",
      "step 137\n",
      "training loss: 2.8326480388641357\n",
      "step 138\n",
      "training loss: 2.8663346767425537\n",
      "step 139\n",
      "training loss: 2.871277332305908\n",
      "step 140\n",
      "training loss: 2.8604860305786133\n",
      "validation loss: 2.8319876194000244\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8614165782928467\n",
      "step 142\n",
      "training loss: 2.852295398712158\n",
      "step 143\n",
      "training loss: 2.873542308807373\n",
      "step 144\n",
      "training loss: 2.8899245262145996\n",
      "step 145\n",
      "training loss: 2.807875394821167\n",
      "step 146\n",
      "training loss: 2.861259937286377\n",
      "step 147\n",
      "training loss: 2.853595018386841\n",
      "step 148\n",
      "training loss: 2.8766486644744873\n",
      "step 149\n",
      "training loss: 2.859027862548828\n",
      "step 150\n",
      "training loss: 2.8788137435913086\n",
      "validation loss: 2.860525608062744\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8540565967559814\n",
      "step 152\n",
      "training loss: 2.8425309658050537\n",
      "step 153\n",
      "training loss: 2.8542428016662598\n",
      "step 154\n",
      "training loss: 2.864858865737915\n",
      "step 155\n",
      "training loss: 2.8513097763061523\n",
      "step 156\n",
      "training loss: 2.8755600452423096\n",
      "step 157\n",
      "training loss: 2.862504005432129\n",
      "step 158\n",
      "training loss: 2.8255815505981445\n",
      "step 159\n",
      "training loss: 2.8498175144195557\n",
      "step 160\n",
      "training loss: 2.853759765625\n",
      "validation loss: 2.868461847305298\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8544228076934814\n",
      "step 162\n",
      "training loss: 2.870344400405884\n",
      "step 163\n",
      "training loss: 2.8671324253082275\n",
      "step 164\n",
      "training loss: 2.8661088943481445\n",
      "step 165\n",
      "training loss: 2.816277027130127\n",
      "step 166\n",
      "training loss: 2.861842632293701\n",
      "step 167\n",
      "training loss: 2.8579821586608887\n",
      "step 168\n",
      "training loss: 2.8607349395751953\n",
      "step 169\n",
      "training loss: 2.863548517227173\n",
      "step 170\n",
      "training loss: 2.8743646144866943\n",
      "validation loss: 2.859736442565918\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8684563636779785\n",
      "step 172\n",
      "training loss: 2.8514204025268555\n",
      "step 173\n",
      "training loss: 2.8777964115142822\n",
      "step 174\n",
      "training loss: 2.8737688064575195\n",
      "step 175\n",
      "training loss: 2.864840030670166\n",
      "step 176\n",
      "training loss: 2.859510898590088\n",
      "step 177\n",
      "training loss: 2.8678088188171387\n",
      "step 178\n",
      "training loss: 2.8572745323181152\n",
      "step 179\n",
      "training loss: 2.8576714992523193\n",
      "step 180\n",
      "training loss: 2.8385465145111084\n",
      "validation loss: 2.8589439392089844\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8250606060028076\n",
      "step 182\n",
      "training loss: 2.8693811893463135\n",
      "step 183\n",
      "training loss: 2.849501848220825\n",
      "step 184\n",
      "training loss: 2.8474156856536865\n",
      "step 185\n",
      "training loss: 2.8640100955963135\n",
      "step 186\n",
      "training loss: 2.891511917114258\n",
      "step 187\n",
      "training loss: 2.842911720275879\n",
      "step 188\n",
      "training loss: 2.8717222213745117\n",
      "step 189\n",
      "training loss: 2.863274335861206\n",
      "step 190\n",
      "training loss: 2.8704721927642822\n",
      "validation loss: 2.865396022796631\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8500232696533203\n",
      "step 192\n",
      "training loss: 2.8582913875579834\n",
      "step 193\n",
      "training loss: 2.8362374305725098\n",
      "step 194\n",
      "training loss: 2.8499274253845215\n",
      "step 195\n",
      "training loss: 2.8585779666900635\n",
      "step 196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8449954986572266\n",
      "step 197\n",
      "training loss: 2.8865151405334473\n",
      "step 198\n",
      "training loss: 2.860305070877075\n",
      "step 199\n",
      "training loss: 2.846014976501465\n",
      "step 200\n",
      "training loss: 2.8611862659454346\n",
      "validation loss: 2.8432388305664062\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.86582350730896\n",
      "step 202\n",
      "training loss: 2.852656126022339\n",
      "step 203\n",
      "training loss: 2.842578172683716\n",
      "step 204\n",
      "training loss: 2.828066110610962\n",
      "step 205\n",
      "training loss: 2.8177578449249268\n",
      "step 206\n",
      "training loss: 2.8749406337738037\n",
      "step 207\n",
      "training loss: 2.8539209365844727\n",
      "step 208\n",
      "training loss: 2.8810677528381348\n",
      "step 209\n",
      "training loss: 2.855323553085327\n",
      "step 210\n",
      "training loss: 2.864502191543579\n",
      "validation loss: 2.85080623626709\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.87937068939209\n",
      "step 212\n",
      "training loss: 2.857297658920288\n",
      "step 213\n",
      "training loss: 2.8545918464660645\n",
      "step 214\n",
      "training loss: 2.865494966506958\n",
      "step 215\n",
      "training loss: 2.843116044998169\n",
      "step 216\n",
      "training loss: 2.8557283878326416\n",
      "step 217\n",
      "training loss: 2.84441876411438\n",
      "step 218\n",
      "training loss: 2.8508119583129883\n",
      "step 219\n",
      "training loss: 2.838897466659546\n",
      "step 220\n",
      "training loss: 2.8614542484283447\n",
      "validation loss: 2.836829900741577\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8630876541137695\n",
      "step 222\n",
      "training loss: 2.847846031188965\n",
      "step 223\n",
      "training loss: 2.8749661445617676\n",
      "step 224\n",
      "training loss: 2.853715419769287\n",
      "step 225\n",
      "training loss: 2.8640780448913574\n",
      "step 226\n",
      "training loss: 2.8555338382720947\n",
      "step 227\n",
      "training loss: 2.865689516067505\n",
      "step 228\n",
      "training loss: 2.7894206047058105\n",
      "step 229\n",
      "training loss: 2.8507444858551025\n",
      "step 230\n",
      "training loss: 2.832409620285034\n",
      "validation loss: 2.83830189704895\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8445677757263184\n",
      "step 232\n",
      "training loss: 2.869971752166748\n",
      "step 233\n",
      "training loss: 2.871953248977661\n",
      "step 234\n",
      "training loss: 2.8628149032592773\n",
      "step 235\n",
      "training loss: 2.862917184829712\n",
      "step 236\n",
      "training loss: 2.859339952468872\n",
      "step 237\n",
      "training loss: 2.8487730026245117\n",
      "step 238\n",
      "training loss: 2.8552212715148926\n",
      "step 239\n",
      "training loss: 2.8715078830718994\n",
      "step 240\n",
      "training loss: 2.8860647678375244\n",
      "validation loss: 2.8688182830810547\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8565969467163086\n",
      "step 242\n",
      "training loss: 2.8440072536468506\n",
      "step 243\n",
      "training loss: 2.8498752117156982\n",
      "step 244\n",
      "training loss: 2.8448386192321777\n",
      "step 245\n",
      "training loss: 2.861769676208496\n",
      "step 246\n",
      "training loss: 2.864013910293579\n",
      "step 247\n",
      "training loss: 2.8535661697387695\n",
      "step 248\n",
      "training loss: 2.8669681549072266\n",
      "step 249\n",
      "training loss: 2.8730828762054443\n",
      "step 250\n",
      "training loss: 2.8476154804229736\n",
      "validation loss: 2.874288558959961\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8721468448638916\n",
      "step 252\n",
      "training loss: 2.877087116241455\n",
      "step 253\n",
      "training loss: 2.846440553665161\n",
      "step 254\n",
      "training loss: 2.8485167026519775\n",
      "step 255\n",
      "training loss: 2.871541738510132\n",
      "step 256\n",
      "training loss: 2.8661036491394043\n",
      "step 257\n",
      "training loss: 2.864795684814453\n",
      "step 258\n",
      "training loss: 2.8636975288391113\n",
      "step 259\n",
      "training loss: 2.86698055267334\n",
      "step 260\n",
      "training loss: 2.8575711250305176\n",
      "validation loss: 2.8300857543945312\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8644492626190186\n",
      "step 262\n",
      "training loss: 2.862161159515381\n",
      "step 263\n",
      "training loss: 2.8614535331726074\n",
      "step 264\n",
      "training loss: 2.8590176105499268\n",
      "step 265\n",
      "training loss: 2.8229191303253174\n",
      "step 266\n",
      "training loss: 2.8524858951568604\n",
      "step 267\n",
      "training loss: 2.869340419769287\n",
      "step 268\n",
      "training loss: 2.850524663925171\n",
      "step 269\n",
      "training loss: 2.848940134048462\n",
      "step 270\n",
      "training loss: 2.8869690895080566\n",
      "validation loss: 2.853832721710205\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.859898567199707\n",
      "----------3.0 min per epoch----------\n",
      "epoch 20\n",
      "step 0\n",
      "training loss: 2.8772425651550293\n",
      "validation loss: 2.8806562423706055\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8796374797821045\n",
      "step 2\n",
      "training loss: 2.8681118488311768\n",
      "step 3\n",
      "training loss: 2.847198963165283\n",
      "step 4\n",
      "training loss: 2.878182888031006\n",
      "step 5\n",
      "training loss: 2.878222703933716\n",
      "step 6\n",
      "training loss: 2.868813991546631\n",
      "step 7\n",
      "training loss: 2.864985704421997\n",
      "step 8\n",
      "training loss: 2.875767946243286\n",
      "step 9\n",
      "training loss: 2.8871710300445557\n",
      "step 10\n",
      "training loss: 2.8060789108276367\n",
      "validation loss: 2.87841534614563\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.84909987449646\n",
      "step 12\n",
      "training loss: 2.818554162979126\n",
      "step 13\n",
      "training loss: 2.852126359939575\n",
      "step 14\n",
      "training loss: 2.8805198669433594\n",
      "step 15\n",
      "training loss: 2.8754162788391113\n",
      "step 16\n",
      "training loss: 2.8866829872131348\n",
      "step 17\n",
      "training loss: 2.8763906955718994\n",
      "step 18\n",
      "training loss: 2.802406072616577\n",
      "step 19\n",
      "training loss: 2.7569122314453125\n",
      "step 20\n",
      "training loss: 2.8491621017456055\n",
      "validation loss: 2.8653011322021484\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8857767581939697\n",
      "step 22\n",
      "training loss: 2.8717215061187744\n",
      "step 23\n",
      "training loss: 2.8527424335479736\n",
      "step 24\n",
      "training loss: 2.8603672981262207\n",
      "step 25\n",
      "training loss: 2.8707199096679688\n",
      "step 26\n",
      "training loss: 2.8809309005737305\n",
      "step 27\n",
      "training loss: 2.8771469593048096\n",
      "step 28\n",
      "training loss: 2.862760066986084\n",
      "step 29\n",
      "training loss: 2.87980580329895\n",
      "step 30\n",
      "training loss: 2.8802831172943115\n",
      "validation loss: 2.942249059677124\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8361501693725586\n",
      "step 32\n",
      "training loss: 2.8434338569641113\n",
      "step 33\n",
      "training loss: 2.857675552368164\n",
      "step 34\n",
      "training loss: 2.8608362674713135\n",
      "step 35\n",
      "training loss: 2.875094413757324\n",
      "step 36\n",
      "training loss: 2.8667404651641846\n",
      "step 37\n",
      "training loss: 2.873283863067627\n",
      "step 38\n",
      "training loss: 2.873840570449829\n",
      "step 39\n",
      "training loss: 2.8671395778656006\n",
      "step 40\n",
      "training loss: 2.8855602741241455\n",
      "validation loss: 2.884167432785034\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.860442638397217\n",
      "step 42\n",
      "training loss: 2.8751842975616455\n",
      "step 43\n",
      "training loss: 2.8832037448883057\n",
      "step 44\n",
      "training loss: 2.8787994384765625\n",
      "step 45\n",
      "training loss: 2.87142276763916\n",
      "step 46\n",
      "training loss: 2.8763577938079834\n",
      "step 47\n",
      "training loss: 2.860412120819092\n",
      "step 48\n",
      "training loss: 2.839205741882324\n",
      "step 49\n",
      "training loss: 2.8623452186584473\n",
      "step 50\n",
      "training loss: 2.8734095096588135\n",
      "validation loss: 2.85417103767395\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.846648693084717\n",
      "step 52\n",
      "training loss: 2.860546588897705\n",
      "step 53\n",
      "training loss: 2.8613932132720947\n",
      "step 54\n",
      "training loss: 2.8506269454956055\n",
      "step 55\n",
      "training loss: 2.8807947635650635\n",
      "step 56\n",
      "training loss: 2.856311559677124\n",
      "step 57\n",
      "training loss: 2.871286153793335\n",
      "step 58\n",
      "training loss: 2.8564844131469727\n",
      "step 59\n",
      "training loss: 2.8708581924438477\n",
      "step 60\n",
      "training loss: 2.8241734504699707\n",
      "validation loss: 2.8658180236816406\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.882774591445923\n",
      "step 62\n",
      "training loss: 2.849926471710205\n",
      "step 63\n",
      "training loss: 2.8817813396453857\n",
      "step 64\n",
      "training loss: 2.8688278198242188\n",
      "step 65\n",
      "training loss: 2.8469882011413574\n",
      "step 66\n",
      "training loss: 2.843552589416504\n",
      "step 67\n",
      "training loss: 2.8650014400482178\n",
      "step 68\n",
      "training loss: 2.8527159690856934\n",
      "step 69\n",
      "training loss: 2.8474841117858887\n",
      "step 70\n",
      "training loss: 2.8780715465545654\n",
      "validation loss: 2.8972275257110596\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.867515802383423\n",
      "step 72\n",
      "training loss: 2.8593077659606934\n",
      "step 73\n",
      "training loss: 2.8439931869506836\n",
      "step 74\n",
      "training loss: 2.855938196182251\n",
      "step 75\n",
      "training loss: 2.856877088546753\n",
      "step 76\n",
      "training loss: 2.834164619445801\n",
      "step 77\n",
      "training loss: 2.8486223220825195\n",
      "step 78\n",
      "training loss: 2.8498730659484863\n",
      "step 79\n",
      "training loss: 2.8437795639038086\n",
      "step 80\n",
      "training loss: 2.836376905441284\n",
      "validation loss: 2.8328371047973633\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8719375133514404\n",
      "step 82\n",
      "training loss: 2.842686176300049\n",
      "step 83\n",
      "training loss: 2.878601551055908\n",
      "step 84\n",
      "training loss: 2.8610260486602783\n",
      "step 85\n",
      "training loss: 2.8363301753997803\n",
      "step 86\n",
      "training loss: 2.874906539916992\n",
      "step 87\n",
      "training loss: 2.8507659435272217\n",
      "step 88\n",
      "training loss: 2.8584814071655273\n",
      "step 89\n",
      "training loss: 2.8657758235931396\n",
      "step 90\n",
      "training loss: 2.835416316986084\n",
      "validation loss: 2.859320878982544\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8764610290527344\n",
      "step 92\n",
      "training loss: 2.8547096252441406\n",
      "step 93\n",
      "training loss: 2.877103090286255\n",
      "step 94\n",
      "training loss: 2.8720102310180664\n",
      "step 95\n",
      "training loss: 2.8947558403015137\n",
      "step 96\n",
      "training loss: 2.879988431930542\n",
      "step 97\n",
      "training loss: 2.8330764770507812\n",
      "step 98\n",
      "training loss: 2.8723673820495605\n",
      "step 99\n",
      "training loss: 2.856837511062622\n",
      "step 100\n",
      "training loss: 2.8454232215881348\n",
      "validation loss: 2.864048957824707\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8570168018341064\n",
      "step 102\n",
      "training loss: 2.85697865486145\n",
      "step 103\n",
      "training loss: 2.8761987686157227\n",
      "step 104\n",
      "training loss: 2.860635995864868\n",
      "step 105\n",
      "training loss: 2.875906467437744\n",
      "step 106\n",
      "training loss: 2.875547170639038\n",
      "step 107\n",
      "training loss: 2.8787548542022705\n",
      "step 108\n",
      "training loss: 2.8563616275787354\n",
      "step 109\n",
      "training loss: 2.866788625717163\n",
      "step 110\n",
      "training loss: 2.8634777069091797\n",
      "validation loss: 2.8711512088775635\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8542275428771973\n",
      "step 112\n",
      "training loss: 2.868476390838623\n",
      "step 113\n",
      "training loss: 2.8423826694488525\n",
      "step 114\n",
      "training loss: 2.8697280883789062\n",
      "step 115\n",
      "training loss: 2.8715591430664062\n",
      "step 116\n",
      "training loss: 2.8127105236053467\n",
      "step 117\n",
      "training loss: 2.853769302368164\n",
      "step 118\n",
      "training loss: 2.8488190174102783\n",
      "step 119\n",
      "training loss: 2.8639016151428223\n",
      "step 120\n",
      "training loss: 2.8732433319091797\n",
      "validation loss: 2.9008846282958984\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8438167572021484\n",
      "step 122\n",
      "training loss: 2.865435838699341\n",
      "step 123\n",
      "training loss: 2.8155465126037598\n",
      "step 124\n",
      "training loss: 2.8516831398010254\n",
      "step 125\n",
      "training loss: 2.8736913204193115\n",
      "step 126\n",
      "training loss: 2.8666534423828125\n",
      "step 127\n",
      "training loss: 2.8586604595184326\n",
      "step 128\n",
      "training loss: 2.84733247756958\n",
      "step 129\n",
      "training loss: 2.864612102508545\n",
      "step 130\n",
      "training loss: 2.8523223400115967\n",
      "validation loss: 2.863757371902466\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8647797107696533\n",
      "step 132\n",
      "training loss: 2.8672757148742676\n",
      "step 133\n",
      "training loss: 2.863060712814331\n",
      "step 134\n",
      "training loss: 2.856170892715454\n",
      "step 135\n",
      "training loss: 2.8584301471710205\n",
      "step 136\n",
      "training loss: 2.837981939315796\n",
      "step 137\n",
      "training loss: 2.841663360595703\n",
      "step 138\n",
      "training loss: 2.831488847732544\n",
      "step 139\n",
      "training loss: 2.8680248260498047\n",
      "step 140\n",
      "training loss: 2.871354818344116\n",
      "validation loss: 2.8599154949188232\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.862365245819092\n",
      "step 142\n",
      "training loss: 2.860684633255005\n",
      "step 143\n",
      "training loss: 2.8520405292510986\n",
      "step 144\n",
      "training loss: 2.8716695308685303\n",
      "step 145\n",
      "training loss: 2.890090227127075\n",
      "step 146\n",
      "training loss: 2.8111300468444824\n",
      "step 147\n",
      "training loss: 2.8598761558532715\n",
      "step 148\n",
      "training loss: 2.850794792175293\n",
      "step 149\n",
      "training loss: 2.8767900466918945\n",
      "step 150\n",
      "training loss: 2.856966733932495\n",
      "validation loss: 2.8289690017700195\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.876716136932373\n",
      "step 152\n",
      "training loss: 2.8513777256011963\n",
      "step 153\n",
      "training loss: 2.8409581184387207\n",
      "step 154\n",
      "training loss: 2.8523929119110107\n",
      "step 155\n",
      "training loss: 2.8641655445098877\n",
      "step 156\n",
      "training loss: 2.8509609699249268\n",
      "step 157\n",
      "training loss: 2.876826524734497\n",
      "step 158\n",
      "training loss: 2.862332582473755\n",
      "step 159\n",
      "training loss: 2.8234784603118896\n",
      "step 160\n",
      "training loss: 2.8505735397338867\n",
      "validation loss: 2.8298871517181396\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.852386236190796\n",
      "step 162\n",
      "training loss: 2.8557419776916504\n",
      "step 163\n",
      "training loss: 2.8670883178710938\n",
      "step 164\n",
      "training loss: 2.8642725944519043\n",
      "step 165\n",
      "training loss: 2.866603136062622\n",
      "step 166\n",
      "training loss: 2.8143486976623535\n",
      "step 167\n",
      "training loss: 2.8642218112945557\n",
      "step 168\n",
      "training loss: 2.857301712036133\n",
      "step 169\n",
      "training loss: 2.862123727798462\n",
      "step 170\n",
      "training loss: 2.8629097938537598\n",
      "validation loss: 2.8306429386138916\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8737242221832275\n",
      "step 172\n",
      "training loss: 2.8676235675811768\n",
      "step 173\n",
      "training loss: 2.850738763809204\n",
      "step 174\n",
      "training loss: 2.877877712249756\n",
      "step 175\n",
      "training loss: 2.8753228187561035\n",
      "step 176\n",
      "training loss: 2.8657891750335693\n",
      "step 177\n",
      "training loss: 2.8594279289245605\n",
      "step 178\n",
      "training loss: 2.8677077293395996\n",
      "step 179\n",
      "training loss: 2.858407974243164\n",
      "step 180\n",
      "training loss: 2.8570709228515625\n",
      "validation loss: 2.8588318824768066\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.836193323135376\n",
      "step 182\n",
      "training loss: 2.8227005004882812\n",
      "step 183\n",
      "training loss: 2.868891477584839\n",
      "step 184\n",
      "training loss: 2.850064992904663\n",
      "step 185\n",
      "training loss: 2.8498995304107666\n",
      "step 186\n",
      "training loss: 2.862084150314331\n",
      "step 187\n",
      "training loss: 2.8934526443481445\n",
      "step 188\n",
      "training loss: 2.842616558074951\n",
      "step 189\n",
      "training loss: 2.87101674079895\n",
      "step 190\n",
      "training loss: 2.8619771003723145\n",
      "validation loss: 2.8697168827056885\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8703789710998535\n",
      "step 192\n",
      "training loss: 2.8499984741210938\n",
      "step 193\n",
      "training loss: 2.85847806930542\n",
      "step 194\n",
      "training loss: 2.835533857345581\n",
      "step 195\n",
      "training loss: 2.8503472805023193\n",
      "step 196\n",
      "training loss: 2.858609199523926\n",
      "step 197\n",
      "training loss: 2.843276262283325\n",
      "step 198\n",
      "training loss: 2.886807918548584\n",
      "step 199\n",
      "training loss: 2.860865592956543\n",
      "step 200\n",
      "training loss: 2.8499767780303955\n",
      "validation loss: 2.8577733039855957\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.864061117172241\n",
      "step 202\n",
      "training loss: 2.866417407989502\n",
      "step 203\n",
      "training loss: 2.8530547618865967\n",
      "step 204\n",
      "training loss: 2.8407511711120605\n",
      "step 205\n",
      "training loss: 2.827913761138916\n",
      "step 206\n",
      "training loss: 2.812079668045044\n",
      "step 207\n",
      "training loss: 2.8750016689300537\n",
      "step 208\n",
      "training loss: 2.8517489433288574\n",
      "step 209\n",
      "training loss: 2.8806917667388916\n",
      "step 210\n",
      "training loss: 2.853999137878418\n",
      "validation loss: 2.858133554458618\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8639066219329834\n",
      "step 212\n",
      "training loss: 2.879650115966797\n",
      "step 213\n",
      "training loss: 2.856440782546997\n",
      "step 214\n",
      "training loss: 2.8518145084381104\n",
      "step 215\n",
      "training loss: 2.866091728210449\n",
      "step 216\n",
      "training loss: 2.842543840408325\n",
      "step 217\n",
      "training loss: 2.8527684211730957\n",
      "step 218\n",
      "training loss: 2.8450851440429688\n",
      "step 219\n",
      "training loss: 2.8493993282318115\n",
      "step 220\n",
      "training loss: 2.8396127223968506\n",
      "validation loss: 2.864109992980957\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8574717044830322\n",
      "step 222\n",
      "training loss: 2.862326145172119\n",
      "step 223\n",
      "training loss: 2.8449645042419434\n",
      "step 224\n",
      "training loss: 2.8742516040802\n",
      "step 225\n",
      "training loss: 2.8502752780914307\n",
      "step 226\n",
      "training loss: 2.8619191646575928\n",
      "step 227\n",
      "training loss: 2.8560705184936523\n",
      "step 228\n",
      "training loss: 2.8647632598876953\n",
      "step 229\n",
      "training loss: 2.789041042327881\n",
      "step 230\n",
      "training loss: 2.84914493560791\n",
      "validation loss: 2.8421642780303955\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.832385540008545\n",
      "step 232\n",
      "training loss: 2.8448047637939453\n",
      "step 233\n",
      "training loss: 2.870328426361084\n",
      "step 234\n",
      "training loss: 2.87221097946167\n",
      "step 235\n",
      "training loss: 2.8603293895721436\n",
      "step 236\n",
      "training loss: 2.860720157623291\n",
      "step 237\n",
      "training loss: 2.85845685005188\n",
      "step 238\n",
      "training loss: 2.8470985889434814\n",
      "step 239\n",
      "training loss: 2.8557300567626953\n",
      "step 240\n",
      "training loss: 2.8693478107452393\n",
      "validation loss: 2.848120927810669\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.884575128555298\n",
      "step 242\n",
      "training loss: 2.857344150543213\n",
      "step 243\n",
      "training loss: 2.8453896045684814\n",
      "step 244\n",
      "training loss: 2.8489744663238525\n",
      "step 245\n",
      "training loss: 2.846301794052124\n",
      "step 246\n",
      "training loss: 2.8607239723205566\n",
      "step 247\n",
      "training loss: 2.86067271232605\n",
      "step 248\n",
      "training loss: 2.854004383087158\n",
      "step 249\n",
      "training loss: 2.865732192993164\n",
      "step 250\n",
      "training loss: 2.8716301918029785\n",
      "validation loss: 2.831080198287964\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.845609426498413\n",
      "step 252\n",
      "training loss: 2.872182846069336\n",
      "step 253\n",
      "training loss: 2.8753280639648438\n",
      "step 254\n",
      "training loss: 2.8432505130767822\n",
      "step 255\n",
      "training loss: 2.8459668159484863\n",
      "step 256\n",
      "training loss: 2.8701300621032715\n",
      "step 257\n",
      "training loss: 2.8645212650299072\n",
      "step 258\n",
      "training loss: 2.863206148147583\n",
      "step 259\n",
      "training loss: 2.8645973205566406\n",
      "step 260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8674402236938477\n",
      "validation loss: 2.835381031036377\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.858144998550415\n",
      "step 262\n",
      "training loss: 2.862593650817871\n",
      "step 263\n",
      "training loss: 2.861208200454712\n",
      "step 264\n",
      "training loss: 2.858579397201538\n",
      "step 265\n",
      "training loss: 2.8563411235809326\n",
      "step 266\n",
      "training loss: 2.8195271492004395\n",
      "step 267\n",
      "training loss: 2.852717161178589\n",
      "step 268\n",
      "training loss: 2.8666646480560303\n",
      "step 269\n",
      "training loss: 2.8427414894104004\n",
      "step 270\n",
      "training loss: 2.8507261276245117\n",
      "validation loss: 2.874577760696411\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8838655948638916\n",
      "----------3.0 min per epoch----------\n",
      "epoch 21\n",
      "step 0\n",
      "training loss: 2.8612804412841797\n",
      "validation loss: 2.8778250217437744\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.875974655151367\n",
      "step 2\n",
      "training loss: 2.880326509475708\n",
      "step 3\n",
      "training loss: 2.8684113025665283\n",
      "step 4\n",
      "training loss: 2.8487067222595215\n",
      "step 5\n",
      "training loss: 2.8758528232574463\n",
      "step 6\n",
      "training loss: 2.875260829925537\n",
      "step 7\n",
      "training loss: 2.868129014968872\n",
      "step 8\n",
      "training loss: 2.865687608718872\n",
      "step 9\n",
      "training loss: 2.8758089542388916\n",
      "step 10\n",
      "training loss: 2.88573956489563\n",
      "validation loss: 2.837064743041992\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.803363084793091\n",
      "step 12\n",
      "training loss: 2.850130796432495\n",
      "step 13\n",
      "training loss: 2.8154783248901367\n",
      "step 14\n",
      "training loss: 2.848755121231079\n",
      "step 15\n",
      "training loss: 2.8813178539276123\n",
      "step 16\n",
      "training loss: 2.8735830783843994\n",
      "step 17\n",
      "training loss: 2.8829498291015625\n",
      "step 18\n",
      "training loss: 2.8698196411132812\n",
      "step 19\n",
      "training loss: 2.7938387393951416\n",
      "step 20\n",
      "training loss: 2.7608482837677\n",
      "validation loss: 2.8612613677978516\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8365261554718018\n",
      "step 22\n",
      "training loss: 2.88602876663208\n",
      "step 23\n",
      "training loss: 2.874593734741211\n",
      "step 24\n",
      "training loss: 2.8522555828094482\n",
      "step 25\n",
      "training loss: 2.859365224838257\n",
      "step 26\n",
      "training loss: 2.8734893798828125\n",
      "step 27\n",
      "training loss: 2.878437042236328\n",
      "step 28\n",
      "training loss: 2.876995801925659\n",
      "step 29\n",
      "training loss: 2.8660850524902344\n",
      "step 30\n",
      "training loss: 2.8796184062957764\n",
      "validation loss: 2.888631582260132\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.880805015563965\n",
      "step 32\n",
      "training loss: 2.838789939880371\n",
      "step 33\n",
      "training loss: 2.847229242324829\n",
      "step 34\n",
      "training loss: 2.857541561126709\n",
      "step 35\n",
      "training loss: 2.858140707015991\n",
      "step 36\n",
      "training loss: 2.871612310409546\n",
      "step 37\n",
      "training loss: 2.867269277572632\n",
      "step 38\n",
      "training loss: 2.873994827270508\n",
      "step 39\n",
      "training loss: 2.875098943710327\n",
      "step 40\n",
      "training loss: 2.8666930198669434\n",
      "validation loss: 2.8816919326782227\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.885063409805298\n",
      "step 42\n",
      "training loss: 2.866002321243286\n",
      "step 43\n",
      "training loss: 2.874484062194824\n",
      "step 44\n",
      "training loss: 2.883087158203125\n",
      "step 45\n",
      "training loss: 2.8815298080444336\n",
      "step 46\n",
      "training loss: 2.873215436935425\n",
      "step 47\n",
      "training loss: 2.874696969985962\n",
      "step 48\n",
      "training loss: 2.858351707458496\n",
      "step 49\n",
      "training loss: 2.841035842895508\n",
      "step 50\n",
      "training loss: 2.8609423637390137\n",
      "validation loss: 2.870133876800537\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8722188472747803\n",
      "step 52\n",
      "training loss: 2.8395538330078125\n",
      "step 53\n",
      "training loss: 2.861837148666382\n",
      "step 54\n",
      "training loss: 2.8597540855407715\n",
      "step 55\n",
      "training loss: 2.851506471633911\n",
      "step 56\n",
      "training loss: 2.880110025405884\n",
      "step 57\n",
      "training loss: 2.856447696685791\n",
      "step 58\n",
      "training loss: 2.8707470893859863\n",
      "step 59\n",
      "training loss: 2.854797840118408\n",
      "step 60\n",
      "training loss: 2.869666814804077\n",
      "validation loss: 2.946169137954712\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8229329586029053\n",
      "step 62\n",
      "training loss: 2.8834404945373535\n",
      "step 63\n",
      "training loss: 2.8484256267547607\n",
      "step 64\n",
      "training loss: 2.8805181980133057\n",
      "step 65\n",
      "training loss: 2.8671791553497314\n",
      "step 66\n",
      "training loss: 2.846813917160034\n",
      "step 67\n",
      "training loss: 2.842843532562256\n",
      "step 68\n",
      "training loss: 2.8649117946624756\n",
      "step 69\n",
      "training loss: 2.851301908493042\n",
      "step 70\n",
      "training loss: 2.8457775115966797\n",
      "validation loss: 2.8823864459991455\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.878908395767212\n",
      "step 72\n",
      "training loss: 2.868899345397949\n",
      "step 73\n",
      "training loss: 2.8584096431732178\n",
      "step 74\n",
      "training loss: 2.842540979385376\n",
      "step 75\n",
      "training loss: 2.8569135665893555\n",
      "step 76\n",
      "training loss: 2.859102487564087\n",
      "step 77\n",
      "training loss: 2.8327901363372803\n",
      "step 78\n",
      "training loss: 2.8487930297851562\n",
      "step 79\n",
      "training loss: 2.8497142791748047\n",
      "step 80\n",
      "training loss: 2.845386505126953\n",
      "validation loss: 2.854729413986206\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8360493183135986\n",
      "step 82\n",
      "training loss: 2.8708767890930176\n",
      "step 83\n",
      "training loss: 2.842738628387451\n",
      "step 84\n",
      "training loss: 2.8780949115753174\n",
      "step 85\n",
      "training loss: 2.8612139225006104\n",
      "step 86\n",
      "training loss: 2.8384323120117188\n",
      "step 87\n",
      "training loss: 2.8757190704345703\n",
      "step 88\n",
      "training loss: 2.8502917289733887\n",
      "step 89\n",
      "training loss: 2.861840009689331\n",
      "step 90\n",
      "training loss: 2.8653600215911865\n",
      "validation loss: 2.867323637008667\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.829949140548706\n",
      "step 92\n",
      "training loss: 2.875566244125366\n",
      "step 93\n",
      "training loss: 2.8560824394226074\n",
      "step 94\n",
      "training loss: 2.8745250701904297\n",
      "step 95\n",
      "training loss: 2.87176251411438\n",
      "step 96\n",
      "training loss: 2.8935983180999756\n",
      "step 97\n",
      "training loss: 2.8768744468688965\n",
      "step 98\n",
      "training loss: 2.832289934158325\n",
      "step 99\n",
      "training loss: 2.870551824569702\n",
      "step 100\n",
      "training loss: 2.8573927879333496\n",
      "validation loss: 2.8847813606262207\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.848360300064087\n",
      "step 102\n",
      "training loss: 2.8573153018951416\n",
      "step 103\n",
      "training loss: 2.8570852279663086\n",
      "step 104\n",
      "training loss: 2.875997543334961\n",
      "step 105\n",
      "training loss: 2.8611671924591064\n",
      "step 106\n",
      "training loss: 2.87520694732666\n",
      "step 107\n",
      "training loss: 2.8774702548980713\n",
      "step 108\n",
      "training loss: 2.8780288696289062\n",
      "step 109\n",
      "training loss: 2.858842611312866\n",
      "step 110\n",
      "training loss: 2.869513988494873\n",
      "validation loss: 2.835562229156494\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.865017890930176\n",
      "step 112\n",
      "training loss: 2.855046510696411\n",
      "step 113\n",
      "training loss: 2.8655879497528076\n",
      "step 114\n",
      "training loss: 2.844208002090454\n",
      "step 115\n",
      "training loss: 2.8676071166992188\n",
      "step 116\n",
      "training loss: 2.873199462890625\n",
      "step 117\n",
      "training loss: 2.813520908355713\n",
      "step 118\n",
      "training loss: 2.852935791015625\n",
      "step 119\n",
      "training loss: 2.847836494445801\n",
      "step 120\n",
      "training loss: 2.8650588989257812\n",
      "validation loss: 2.856114149093628\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8731651306152344\n",
      "step 122\n",
      "training loss: 2.8439769744873047\n",
      "step 123\n",
      "training loss: 2.865938425064087\n",
      "step 124\n",
      "training loss: 2.8157730102539062\n",
      "step 125\n",
      "training loss: 2.851503372192383\n",
      "step 126\n",
      "training loss: 2.8743128776550293\n",
      "step 127\n",
      "training loss: 2.8664417266845703\n",
      "step 128\n",
      "training loss: 2.8578622341156006\n",
      "step 129\n",
      "training loss: 2.8477795124053955\n",
      "step 130\n",
      "training loss: 2.8677854537963867\n",
      "validation loss: 2.8629140853881836\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8521344661712646\n",
      "step 132\n",
      "training loss: 2.862393617630005\n",
      "step 133\n",
      "training loss: 2.8665771484375\n",
      "step 134\n",
      "training loss: 2.8625805377960205\n",
      "step 135\n",
      "training loss: 2.857729196548462\n",
      "step 136\n",
      "training loss: 2.859442710876465\n",
      "step 137\n",
      "training loss: 2.8376855850219727\n",
      "step 138\n",
      "training loss: 2.840616464614868\n",
      "step 139\n",
      "training loss: 2.83406400680542\n",
      "step 140\n",
      "training loss: 2.867821216583252\n",
      "validation loss: 2.859649658203125\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8697056770324707\n",
      "step 142\n",
      "training loss: 2.862969398498535\n",
      "step 143\n",
      "training loss: 2.8599791526794434\n",
      "step 144\n",
      "training loss: 2.851641893386841\n",
      "step 145\n",
      "training loss: 2.8747751712799072\n",
      "step 146\n",
      "training loss: 2.891880989074707\n",
      "step 147\n",
      "training loss: 2.8095099925994873\n",
      "step 148\n",
      "training loss: 2.8597989082336426\n",
      "step 149\n",
      "training loss: 2.8510563373565674\n",
      "step 150\n",
      "training loss: 2.877410411834717\n",
      "validation loss: 2.902679920196533\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8584704399108887\n",
      "step 152\n",
      "training loss: 2.877596616744995\n",
      "step 153\n",
      "training loss: 2.853638172149658\n",
      "step 154\n",
      "training loss: 2.840989589691162\n",
      "step 155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.854078769683838\n",
      "step 156\n",
      "training loss: 2.8651504516601562\n",
      "step 157\n",
      "training loss: 2.851181983947754\n",
      "step 158\n",
      "training loss: 2.8766350746154785\n",
      "step 159\n",
      "training loss: 2.8613779544830322\n",
      "step 160\n",
      "training loss: 2.822190046310425\n",
      "validation loss: 2.8649067878723145\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.850475788116455\n",
      "step 162\n",
      "training loss: 2.852842330932617\n",
      "step 163\n",
      "training loss: 2.8553504943847656\n",
      "step 164\n",
      "training loss: 2.8702051639556885\n",
      "step 165\n",
      "training loss: 2.8646230697631836\n",
      "step 166\n",
      "training loss: 2.8664116859436035\n",
      "step 167\n",
      "training loss: 2.8142189979553223\n",
      "step 168\n",
      "training loss: 2.86368727684021\n",
      "step 169\n",
      "training loss: 2.8561837673187256\n",
      "step 170\n",
      "training loss: 2.8646557331085205\n",
      "validation loss: 2.860626697540283\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8621551990509033\n",
      "step 172\n",
      "training loss: 2.87394118309021\n",
      "step 173\n",
      "training loss: 2.8679862022399902\n",
      "step 174\n",
      "training loss: 2.8504741191864014\n",
      "step 175\n",
      "training loss: 2.878319501876831\n",
      "step 176\n",
      "training loss: 2.874376058578491\n",
      "step 177\n",
      "training loss: 2.8646838665008545\n",
      "step 178\n",
      "training loss: 2.859754800796509\n",
      "step 179\n",
      "training loss: 2.868520736694336\n",
      "step 180\n",
      "training loss: 2.857935905456543\n",
      "validation loss: 2.8318989276885986\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8578312397003174\n",
      "step 182\n",
      "training loss: 2.8395097255706787\n",
      "step 183\n",
      "training loss: 2.8205177783966064\n",
      "step 184\n",
      "training loss: 2.8696978092193604\n",
      "step 185\n",
      "training loss: 2.849761486053467\n",
      "step 186\n",
      "training loss: 2.851133346557617\n",
      "step 187\n",
      "training loss: 2.8604071140289307\n",
      "step 188\n",
      "training loss: 2.892296552658081\n",
      "step 189\n",
      "training loss: 2.8432281017303467\n",
      "step 190\n",
      "training loss: 2.872058391571045\n",
      "validation loss: 2.830777406692505\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.864755392074585\n",
      "step 192\n",
      "training loss: 2.869663715362549\n",
      "step 193\n",
      "training loss: 2.849285840988159\n",
      "step 194\n",
      "training loss: 2.8575873374938965\n",
      "step 195\n",
      "training loss: 2.8367245197296143\n",
      "step 196\n",
      "training loss: 2.850306749343872\n",
      "step 197\n",
      "training loss: 2.858229637145996\n",
      "step 198\n",
      "training loss: 2.845684766769409\n",
      "step 199\n",
      "training loss: 2.8874640464782715\n",
      "step 200\n",
      "training loss: 2.862791061401367\n",
      "validation loss: 2.8336923122406006\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.849822998046875\n",
      "step 202\n",
      "training loss: 2.8640215396881104\n",
      "step 203\n",
      "training loss: 2.8659708499908447\n",
      "step 204\n",
      "training loss: 2.8527963161468506\n",
      "step 205\n",
      "training loss: 2.841981887817383\n",
      "step 206\n",
      "training loss: 2.8288586139678955\n",
      "step 207\n",
      "training loss: 2.8142993450164795\n",
      "step 208\n",
      "training loss: 2.8754520416259766\n",
      "step 209\n",
      "training loss: 2.850679636001587\n",
      "step 210\n",
      "training loss: 2.8817265033721924\n",
      "validation loss: 2.8588550090789795\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.85463547706604\n",
      "step 212\n",
      "training loss: 2.8653194904327393\n",
      "step 213\n",
      "training loss: 2.8793528079986572\n",
      "step 214\n",
      "training loss: 2.8569259643554688\n",
      "step 215\n",
      "training loss: 2.8539111614227295\n",
      "step 216\n",
      "training loss: 2.8696746826171875\n",
      "step 217\n",
      "training loss: 2.844813108444214\n",
      "step 218\n",
      "training loss: 2.8540289402008057\n",
      "step 219\n",
      "training loss: 2.84568452835083\n",
      "step 220\n",
      "training loss: 2.8505356311798096\n",
      "validation loss: 2.868804931640625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8373167514801025\n",
      "step 222\n",
      "training loss: 2.8590590953826904\n",
      "step 223\n",
      "training loss: 2.862715005874634\n",
      "step 224\n",
      "training loss: 2.8449440002441406\n",
      "step 225\n",
      "training loss: 2.8740475177764893\n",
      "step 226\n",
      "training loss: 2.8483834266662598\n",
      "step 227\n",
      "training loss: 2.8628313541412354\n",
      "step 228\n",
      "training loss: 2.8565590381622314\n",
      "step 229\n",
      "training loss: 2.8663766384124756\n",
      "step 230\n",
      "training loss: 2.790154457092285\n",
      "validation loss: 2.8575727939605713\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8500921726226807\n",
      "step 232\n",
      "training loss: 2.831428050994873\n",
      "step 233\n",
      "training loss: 2.8450162410736084\n",
      "step 234\n",
      "training loss: 2.869384765625\n",
      "step 235\n",
      "training loss: 2.871349573135376\n",
      "step 236\n",
      "training loss: 2.8620707988739014\n",
      "step 237\n",
      "training loss: 2.8595731258392334\n",
      "step 238\n",
      "training loss: 2.858173370361328\n",
      "step 239\n",
      "training loss: 2.84898042678833\n",
      "step 240\n",
      "training loss: 2.856630325317383\n",
      "validation loss: 2.8581721782684326\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.86897873878479\n",
      "step 242\n",
      "training loss: 2.8837835788726807\n",
      "step 243\n",
      "training loss: 2.8554983139038086\n",
      "step 244\n",
      "training loss: 2.8467366695404053\n",
      "step 245\n",
      "training loss: 2.84796404838562\n",
      "step 246\n",
      "training loss: 2.845414638519287\n",
      "step 247\n",
      "training loss: 2.8598828315734863\n",
      "step 248\n",
      "training loss: 2.8606574535369873\n",
      "step 249\n",
      "training loss: 2.8572592735290527\n",
      "step 250\n",
      "training loss: 2.865462303161621\n",
      "validation loss: 2.8648219108581543\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.873521327972412\n",
      "step 252\n",
      "training loss: 2.8487067222595215\n",
      "step 253\n",
      "training loss: 2.8736660480499268\n",
      "step 254\n",
      "training loss: 2.878268241882324\n",
      "step 255\n",
      "training loss: 2.8462626934051514\n",
      "step 256\n",
      "training loss: 2.84663724899292\n",
      "step 257\n",
      "training loss: 2.869596481323242\n",
      "step 258\n",
      "training loss: 2.8620219230651855\n",
      "step 259\n",
      "training loss: 2.8659095764160156\n",
      "step 260\n",
      "training loss: 2.8656327724456787\n",
      "validation loss: 2.841651678085327\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.86725115776062\n",
      "step 262\n",
      "training loss: 2.8588109016418457\n",
      "step 263\n",
      "training loss: 2.865060567855835\n",
      "step 264\n",
      "training loss: 2.862239360809326\n",
      "step 265\n",
      "training loss: 2.860396146774292\n",
      "step 266\n",
      "training loss: 2.855642795562744\n",
      "step 267\n",
      "training loss: 2.823586940765381\n",
      "step 268\n",
      "training loss: 2.8534927368164062\n",
      "step 269\n",
      "training loss: 2.870354413986206\n",
      "step 270\n",
      "training loss: 2.8463950157165527\n",
      "validation loss: 2.8445322513580322\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8522965908050537\n",
      "----------3.0 min per epoch----------\n",
      "epoch 22\n",
      "step 0\n",
      "training loss: 2.886523723602295\n",
      "validation loss: 2.8311805725097656\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8628811836242676\n",
      "step 2\n",
      "training loss: 2.8751840591430664\n",
      "step 3\n",
      "training loss: 2.8792219161987305\n",
      "step 4\n",
      "training loss: 2.866520881652832\n",
      "step 5\n",
      "training loss: 2.848710060119629\n",
      "step 6\n",
      "training loss: 2.8762617111206055\n",
      "step 7\n",
      "training loss: 2.875894069671631\n",
      "step 8\n",
      "training loss: 2.867426633834839\n",
      "step 9\n",
      "training loss: 2.8654520511627197\n",
      "step 10\n",
      "training loss: 2.877347707748413\n",
      "validation loss: 2.8406434059143066\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.886230230331421\n",
      "step 12\n",
      "training loss: 2.804624080657959\n",
      "step 13\n",
      "training loss: 2.8473825454711914\n",
      "step 14\n",
      "training loss: 2.8210442066192627\n",
      "step 15\n",
      "training loss: 2.8493478298187256\n",
      "step 16\n",
      "training loss: 2.880983591079712\n",
      "step 17\n",
      "training loss: 2.8711049556732178\n",
      "step 18\n",
      "training loss: 2.8832290172576904\n",
      "step 19\n",
      "training loss: 2.8729233741760254\n",
      "step 20\n",
      "training loss: 2.7927069664001465\n",
      "validation loss: 2.889732837677002\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.7512731552124023\n",
      "step 22\n",
      "training loss: 2.831583023071289\n",
      "step 23\n",
      "training loss: 2.892308473587036\n",
      "step 24\n",
      "training loss: 2.872074842453003\n",
      "step 25\n",
      "training loss: 2.8541500568389893\n",
      "step 26\n",
      "training loss: 2.8581702709198\n",
      "step 27\n",
      "training loss: 2.8739173412323\n",
      "step 28\n",
      "training loss: 2.879262685775757\n",
      "step 29\n",
      "training loss: 2.878499746322632\n",
      "step 30\n",
      "training loss: 2.8639578819274902\n",
      "validation loss: 2.880403518676758\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.880439519882202\n",
      "step 32\n",
      "training loss: 2.8787171840667725\n",
      "step 33\n",
      "training loss: 2.8373587131500244\n",
      "step 34\n",
      "training loss: 2.844606876373291\n",
      "step 35\n",
      "training loss: 2.8572728633880615\n",
      "step 36\n",
      "training loss: 2.8586812019348145\n",
      "step 37\n",
      "training loss: 2.8740177154541016\n",
      "step 38\n",
      "training loss: 2.8643014430999756\n",
      "step 39\n",
      "training loss: 2.8738603591918945\n",
      "step 40\n",
      "training loss: 2.8771438598632812\n",
      "validation loss: 2.832901954650879\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.868211269378662\n",
      "step 42\n",
      "training loss: 2.8847622871398926\n",
      "step 43\n",
      "training loss: 2.862049102783203\n",
      "step 44\n",
      "training loss: 2.875840902328491\n",
      "step 45\n",
      "training loss: 2.8825526237487793\n",
      "step 46\n",
      "training loss: 2.8777713775634766\n",
      "step 47\n",
      "training loss: 2.8753957748413086\n",
      "step 48\n",
      "training loss: 2.877082347869873\n",
      "step 49\n",
      "training loss: 2.8608975410461426\n",
      "step 50\n",
      "training loss: 2.8419694900512695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.8567819595336914\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8629026412963867\n",
      "step 52\n",
      "training loss: 2.875450611114502\n",
      "step 53\n",
      "training loss: 2.8436903953552246\n",
      "step 54\n",
      "training loss: 2.8614563941955566\n",
      "step 55\n",
      "training loss: 2.8644485473632812\n",
      "step 56\n",
      "training loss: 2.85160756111145\n",
      "step 57\n",
      "training loss: 2.882655143737793\n",
      "step 58\n",
      "training loss: 2.8562893867492676\n",
      "step 59\n",
      "training loss: 2.872227668762207\n",
      "step 60\n",
      "training loss: 2.855526924133301\n",
      "validation loss: 2.8897624015808105\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.869401693344116\n",
      "step 62\n",
      "training loss: 2.8237216472625732\n",
      "step 63\n",
      "training loss: 2.8849294185638428\n",
      "step 64\n",
      "training loss: 2.8506650924682617\n",
      "step 65\n",
      "training loss: 2.881601572036743\n",
      "step 66\n",
      "training loss: 2.8638556003570557\n",
      "step 67\n",
      "training loss: 2.8489434719085693\n",
      "step 68\n",
      "training loss: 2.843010663986206\n",
      "step 69\n",
      "training loss: 2.863567352294922\n",
      "step 70\n",
      "training loss: 2.852151870727539\n",
      "validation loss: 2.8796310424804688\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8453714847564697\n",
      "step 72\n",
      "training loss: 2.881613254547119\n",
      "step 73\n",
      "training loss: 2.86885666847229\n",
      "step 74\n",
      "training loss: 2.859473466873169\n",
      "step 75\n",
      "training loss: 2.841172695159912\n",
      "step 76\n",
      "training loss: 2.8590123653411865\n",
      "step 77\n",
      "training loss: 2.857558250427246\n",
      "step 78\n",
      "training loss: 2.837864875793457\n",
      "step 79\n",
      "training loss: 2.850104331970215\n",
      "step 80\n",
      "training loss: 2.852463483810425\n",
      "validation loss: 2.8666012287139893\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8436033725738525\n",
      "step 82\n",
      "training loss: 2.8350963592529297\n",
      "step 83\n",
      "training loss: 2.873335599899292\n",
      "step 84\n",
      "training loss: 2.8454439640045166\n",
      "step 85\n",
      "training loss: 2.8814918994903564\n",
      "step 86\n",
      "training loss: 2.8604705333709717\n",
      "step 87\n",
      "training loss: 2.8388237953186035\n",
      "step 88\n",
      "training loss: 2.8759241104125977\n",
      "step 89\n",
      "training loss: 2.8529133796691895\n",
      "step 90\n",
      "training loss: 2.8608434200286865\n",
      "validation loss: 2.9338316917419434\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8637585639953613\n",
      "step 92\n",
      "training loss: 2.83183217048645\n",
      "step 93\n",
      "training loss: 2.877307415008545\n",
      "step 94\n",
      "training loss: 2.8565661907196045\n",
      "step 95\n",
      "training loss: 2.875946283340454\n",
      "step 96\n",
      "training loss: 2.8719253540039062\n",
      "step 97\n",
      "training loss: 2.893962860107422\n",
      "step 98\n",
      "training loss: 2.8790695667266846\n",
      "step 99\n",
      "training loss: 2.834238052368164\n",
      "step 100\n",
      "training loss: 2.870483875274658\n",
      "validation loss: 2.8774704933166504\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.859609842300415\n",
      "step 102\n",
      "training loss: 2.849881649017334\n",
      "step 103\n",
      "training loss: 2.8552732467651367\n",
      "step 104\n",
      "training loss: 2.8585944175720215\n",
      "step 105\n",
      "training loss: 2.8752236366271973\n",
      "step 106\n",
      "training loss: 2.859450340270996\n",
      "step 107\n",
      "training loss: 2.8750665187835693\n",
      "step 108\n",
      "training loss: 2.8749966621398926\n",
      "step 109\n",
      "training loss: 2.8782856464385986\n",
      "step 110\n",
      "training loss: 2.854588270187378\n",
      "validation loss: 2.8575451374053955\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.868993043899536\n",
      "step 112\n",
      "training loss: 2.862043857574463\n",
      "step 113\n",
      "training loss: 2.8562562465667725\n",
      "step 114\n",
      "training loss: 2.8661553859710693\n",
      "step 115\n",
      "training loss: 2.8443098068237305\n",
      "step 116\n",
      "training loss: 2.8675453662872314\n",
      "step 117\n",
      "training loss: 2.872478485107422\n",
      "step 118\n",
      "training loss: 2.817760944366455\n",
      "step 119\n",
      "training loss: 2.85577654838562\n",
      "step 120\n",
      "training loss: 2.8487930297851562\n",
      "validation loss: 2.8653805255889893\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.863651752471924\n",
      "step 122\n",
      "training loss: 2.872603416442871\n",
      "step 123\n",
      "training loss: 2.8432793617248535\n",
      "step 124\n",
      "training loss: 2.8673977851867676\n",
      "step 125\n",
      "training loss: 2.8165743350982666\n",
      "step 126\n",
      "training loss: 2.850658416748047\n",
      "step 127\n",
      "training loss: 2.8748769760131836\n",
      "step 128\n",
      "training loss: 2.8662068843841553\n",
      "step 129\n",
      "training loss: 2.858142614364624\n",
      "step 130\n",
      "training loss: 2.847367525100708\n",
      "validation loss: 2.888991594314575\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.865851879119873\n",
      "step 132\n",
      "training loss: 2.8517580032348633\n",
      "step 133\n",
      "training loss: 2.863063335418701\n",
      "step 134\n",
      "training loss: 2.8694729804992676\n",
      "step 135\n",
      "training loss: 2.8639752864837646\n",
      "step 136\n",
      "training loss: 2.8570656776428223\n",
      "step 137\n",
      "training loss: 2.8611385822296143\n",
      "step 138\n",
      "training loss: 2.8356010913848877\n",
      "step 139\n",
      "training loss: 2.8405048847198486\n",
      "step 140\n",
      "training loss: 2.834455966949463\n",
      "validation loss: 2.8342654705047607\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8677215576171875\n",
      "step 142\n",
      "training loss: 2.8690450191497803\n",
      "step 143\n",
      "training loss: 2.861967086791992\n",
      "step 144\n",
      "training loss: 2.860865354537964\n",
      "step 145\n",
      "training loss: 2.85225772857666\n",
      "step 146\n",
      "training loss: 2.873772144317627\n",
      "step 147\n",
      "training loss: 2.8917787075042725\n",
      "step 148\n",
      "training loss: 2.809123992919922\n",
      "step 149\n",
      "training loss: 2.8579318523406982\n",
      "step 150\n",
      "training loss: 2.8513197898864746\n",
      "validation loss: 2.859668731689453\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.875831365585327\n",
      "step 152\n",
      "training loss: 2.858684778213501\n",
      "step 153\n",
      "training loss: 2.8794877529144287\n",
      "step 154\n",
      "training loss: 2.8537843227386475\n",
      "step 155\n",
      "training loss: 2.842047691345215\n",
      "step 156\n",
      "training loss: 2.8534181118011475\n",
      "step 157\n",
      "training loss: 2.8664114475250244\n",
      "step 158\n",
      "training loss: 2.8512825965881348\n",
      "step 159\n",
      "training loss: 2.8788809776306152\n",
      "step 160\n",
      "training loss: 2.8606152534484863\n",
      "validation loss: 2.86474871635437\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.821427345275879\n",
      "step 162\n",
      "training loss: 2.8509442806243896\n",
      "step 163\n",
      "training loss: 2.854800224304199\n",
      "step 164\n",
      "training loss: 2.8550193309783936\n",
      "step 165\n",
      "training loss: 2.870096206665039\n",
      "step 166\n",
      "training loss: 2.8654329776763916\n",
      "step 167\n",
      "training loss: 2.8666911125183105\n",
      "step 168\n",
      "training loss: 2.8150742053985596\n",
      "step 169\n",
      "training loss: 2.8645904064178467\n",
      "step 170\n",
      "training loss: 2.856548547744751\n",
      "validation loss: 2.867619276046753\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8654725551605225\n",
      "step 172\n",
      "training loss: 2.8620352745056152\n",
      "step 173\n",
      "training loss: 2.874370574951172\n",
      "step 174\n",
      "training loss: 2.8704261779785156\n",
      "step 175\n",
      "training loss: 2.8519136905670166\n",
      "step 176\n",
      "training loss: 2.8780245780944824\n",
      "step 177\n",
      "training loss: 2.876631259918213\n",
      "step 178\n",
      "training loss: 2.8665943145751953\n",
      "step 179\n",
      "training loss: 2.8595163822174072\n",
      "step 180\n",
      "training loss: 2.867656707763672\n",
      "validation loss: 2.8917737007141113\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8585503101348877\n",
      "step 182\n",
      "training loss: 2.858503818511963\n",
      "step 183\n",
      "training loss: 2.8402440547943115\n",
      "step 184\n",
      "training loss: 2.8235790729522705\n",
      "step 185\n",
      "training loss: 2.869291305541992\n",
      "step 186\n",
      "training loss: 2.850154161453247\n",
      "step 187\n",
      "training loss: 2.8528175354003906\n",
      "step 188\n",
      "training loss: 2.863903522491455\n",
      "step 189\n",
      "training loss: 2.892782688140869\n",
      "step 190\n",
      "training loss: 2.8432345390319824\n",
      "validation loss: 2.8659443855285645\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8700218200683594\n",
      "step 192\n",
      "training loss: 2.8635215759277344\n",
      "step 193\n",
      "training loss: 2.870554208755493\n",
      "step 194\n",
      "training loss: 2.8494443893432617\n",
      "step 195\n",
      "training loss: 2.857471466064453\n",
      "step 196\n",
      "training loss: 2.8375611305236816\n",
      "step 197\n",
      "training loss: 2.8523707389831543\n",
      "step 198\n",
      "training loss: 2.8592493534088135\n",
      "step 199\n",
      "training loss: 2.842761516571045\n",
      "step 200\n",
      "training loss: 2.8867833614349365\n",
      "validation loss: 2.864022731781006\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8622303009033203\n",
      "step 202\n",
      "training loss: 2.8514299392700195\n",
      "step 203\n",
      "training loss: 2.860518455505371\n",
      "step 204\n",
      "training loss: 2.867171287536621\n",
      "step 205\n",
      "training loss: 2.853600025177002\n",
      "step 206\n",
      "training loss: 2.8424792289733887\n",
      "step 207\n",
      "training loss: 2.829486608505249\n",
      "step 208\n",
      "training loss: 2.815730094909668\n",
      "step 209\n",
      "training loss: 2.875967264175415\n",
      "step 210\n",
      "training loss: 2.853156328201294\n",
      "validation loss: 2.8264636993408203\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.878561019897461\n",
      "step 212\n",
      "training loss: 2.85387921333313\n",
      "step 213\n",
      "training loss: 2.861617088317871\n",
      "step 214\n",
      "training loss: 2.878474235534668\n",
      "step 215\n",
      "training loss: 2.8562543392181396\n",
      "step 216\n",
      "training loss: 2.8523850440979004\n",
      "step 217\n",
      "training loss: 2.8672337532043457\n",
      "step 218\n",
      "training loss: 2.8443245887756348\n",
      "step 219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8548474311828613\n",
      "step 220\n",
      "training loss: 2.8451223373413086\n",
      "validation loss: 2.8270175457000732\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.849687099456787\n",
      "step 222\n",
      "training loss: 2.8372139930725098\n",
      "step 223\n",
      "training loss: 2.8596231937408447\n",
      "step 224\n",
      "training loss: 2.864082098007202\n",
      "step 225\n",
      "training loss: 2.8460769653320312\n",
      "step 226\n",
      "training loss: 2.8747365474700928\n",
      "step 227\n",
      "training loss: 2.8497140407562256\n",
      "step 228\n",
      "training loss: 2.8626768589019775\n",
      "step 229\n",
      "training loss: 2.85783052444458\n",
      "step 230\n",
      "training loss: 2.867130756378174\n",
      "validation loss: 2.8304944038391113\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.7859628200531006\n",
      "step 232\n",
      "training loss: 2.851454496383667\n",
      "step 233\n",
      "training loss: 2.830850839614868\n",
      "step 234\n",
      "training loss: 2.8531711101531982\n",
      "step 235\n",
      "training loss: 2.871666431427002\n",
      "step 236\n",
      "training loss: 2.874040126800537\n",
      "step 237\n",
      "training loss: 2.860222101211548\n",
      "step 238\n",
      "training loss: 2.8619775772094727\n",
      "step 239\n",
      "training loss: 2.860921621322632\n",
      "step 240\n",
      "training loss: 2.8489091396331787\n",
      "validation loss: 2.857621192932129\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8563578128814697\n",
      "step 242\n",
      "training loss: 2.8703413009643555\n",
      "step 243\n",
      "training loss: 2.8846213817596436\n",
      "step 244\n",
      "training loss: 2.8563714027404785\n",
      "step 245\n",
      "training loss: 2.847050189971924\n",
      "step 246\n",
      "training loss: 2.8482813835144043\n",
      "step 247\n",
      "training loss: 2.8481285572052\n",
      "step 248\n",
      "training loss: 2.860724687576294\n",
      "step 249\n",
      "training loss: 2.863356590270996\n",
      "step 250\n",
      "training loss: 2.8577427864074707\n",
      "validation loss: 2.8671395778656006\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.864658832550049\n",
      "step 252\n",
      "training loss: 2.8709592819213867\n",
      "step 253\n",
      "training loss: 2.8492634296417236\n",
      "step 254\n",
      "training loss: 2.8739335536956787\n",
      "step 255\n",
      "training loss: 2.876542091369629\n",
      "step 256\n",
      "training loss: 2.8469839096069336\n",
      "step 257\n",
      "training loss: 2.8494679927825928\n",
      "step 258\n",
      "training loss: 2.8714537620544434\n",
      "step 259\n",
      "training loss: 2.866105556488037\n",
      "step 260\n",
      "training loss: 2.8635222911834717\n",
      "validation loss: 2.8574602603912354\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8658764362335205\n",
      "step 262\n",
      "training loss: 2.8690454959869385\n",
      "step 263\n",
      "training loss: 2.858271598815918\n",
      "step 264\n",
      "training loss: 2.864609479904175\n",
      "step 265\n",
      "training loss: 2.861727237701416\n",
      "step 266\n",
      "training loss: 2.860668182373047\n",
      "step 267\n",
      "training loss: 2.856843948364258\n",
      "step 268\n",
      "training loss: 2.82084321975708\n",
      "step 269\n",
      "training loss: 2.850529193878174\n",
      "step 270\n",
      "training loss: 2.8724656105041504\n",
      "validation loss: 2.8591084480285645\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8478221893310547\n",
      "----------3.0 min per epoch----------\n",
      "epoch 23\n",
      "step 0\n",
      "training loss: 2.855344772338867\n",
      "validation loss: 2.8658385276794434\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8876609802246094\n",
      "step 2\n",
      "training loss: 2.8632700443267822\n",
      "step 3\n",
      "training loss: 2.8769607543945312\n",
      "step 4\n",
      "training loss: 2.8798608779907227\n",
      "step 5\n",
      "training loss: 2.865450382232666\n",
      "step 6\n",
      "training loss: 2.846372365951538\n",
      "step 7\n",
      "training loss: 2.876223564147949\n",
      "step 8\n",
      "training loss: 2.8738009929656982\n",
      "step 9\n",
      "training loss: 2.8684535026550293\n",
      "step 10\n",
      "training loss: 2.8637731075286865\n",
      "validation loss: 2.854531764984131\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8767101764678955\n",
      "step 12\n",
      "training loss: 2.8871021270751953\n",
      "step 13\n",
      "training loss: 2.802290201187134\n",
      "step 14\n",
      "training loss: 2.846414566040039\n",
      "step 15\n",
      "training loss: 2.8145108222961426\n",
      "step 16\n",
      "training loss: 2.853372812271118\n",
      "step 17\n",
      "training loss: 2.881371259689331\n",
      "step 18\n",
      "training loss: 2.873722791671753\n",
      "step 19\n",
      "training loss: 2.8825759887695312\n",
      "step 20\n",
      "training loss: 2.867213249206543\n",
      "validation loss: 2.8470797538757324\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.7945573329925537\n",
      "step 22\n",
      "training loss: 2.7517142295837402\n",
      "step 23\n",
      "training loss: 2.832230567932129\n",
      "step 24\n",
      "training loss: 2.8848092555999756\n",
      "step 25\n",
      "training loss: 2.868408679962158\n",
      "step 26\n",
      "training loss: 2.8544301986694336\n",
      "step 27\n",
      "training loss: 2.8593719005584717\n",
      "step 28\n",
      "training loss: 2.8770840167999268\n",
      "step 29\n",
      "training loss: 2.8794806003570557\n",
      "step 30\n",
      "training loss: 2.877593517303467\n",
      "validation loss: 2.83428692817688\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.864021062850952\n",
      "step 32\n",
      "training loss: 2.8780887126922607\n",
      "step 33\n",
      "training loss: 2.8805394172668457\n",
      "step 34\n",
      "training loss: 2.8331291675567627\n",
      "step 35\n",
      "training loss: 2.846449375152588\n",
      "step 36\n",
      "training loss: 2.855384588241577\n",
      "step 37\n",
      "training loss: 2.8606953620910645\n",
      "step 38\n",
      "training loss: 2.8754055500030518\n",
      "step 39\n",
      "training loss: 2.8697993755340576\n",
      "step 40\n",
      "training loss: 2.8733069896698\n",
      "validation loss: 2.842298984527588\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8776297569274902\n",
      "step 42\n",
      "training loss: 2.866349935531616\n",
      "step 43\n",
      "training loss: 2.8850815296173096\n",
      "step 44\n",
      "training loss: 2.8637609481811523\n",
      "step 45\n",
      "training loss: 2.873948335647583\n",
      "step 46\n",
      "training loss: 2.8822903633117676\n",
      "step 47\n",
      "training loss: 2.876278877258301\n",
      "step 48\n",
      "training loss: 2.873958110809326\n",
      "step 49\n",
      "training loss: 2.873217821121216\n",
      "step 50\n",
      "training loss: 2.861948013305664\n",
      "validation loss: 2.8725850582122803\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8410451412200928\n",
      "step 52\n",
      "training loss: 2.8637428283691406\n",
      "step 53\n",
      "training loss: 2.871143341064453\n",
      "step 54\n",
      "training loss: 2.845292329788208\n",
      "step 55\n",
      "training loss: 2.859699249267578\n",
      "step 56\n",
      "training loss: 2.862703323364258\n",
      "step 57\n",
      "training loss: 2.8490564823150635\n",
      "step 58\n",
      "training loss: 2.8811373710632324\n",
      "step 59\n",
      "training loss: 2.857628107070923\n",
      "step 60\n",
      "training loss: 2.870511531829834\n",
      "validation loss: 2.8756496906280518\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.854686975479126\n",
      "step 62\n",
      "training loss: 2.8688435554504395\n",
      "step 63\n",
      "training loss: 2.8244380950927734\n",
      "step 64\n",
      "training loss: 2.8825926780700684\n",
      "step 65\n",
      "training loss: 2.8514106273651123\n",
      "step 66\n",
      "training loss: 2.881472587585449\n",
      "step 67\n",
      "training loss: 2.864772081375122\n",
      "step 68\n",
      "training loss: 2.84696888923645\n",
      "step 69\n",
      "training loss: 2.8435750007629395\n",
      "step 70\n",
      "training loss: 2.864299774169922\n",
      "validation loss: 2.8311383724212646\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8548519611358643\n",
      "step 72\n",
      "training loss: 2.8421103954315186\n",
      "step 73\n",
      "training loss: 2.880622386932373\n",
      "step 74\n",
      "training loss: 2.869932174682617\n",
      "step 75\n",
      "training loss: 2.8613805770874023\n",
      "step 76\n",
      "training loss: 2.841259479522705\n",
      "step 77\n",
      "training loss: 2.858705759048462\n",
      "step 78\n",
      "training loss: 2.8594532012939453\n",
      "step 79\n",
      "training loss: 2.8344361782073975\n",
      "step 80\n",
      "training loss: 2.8496854305267334\n",
      "validation loss: 2.8575360774993896\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.851301670074463\n",
      "step 82\n",
      "training loss: 2.8449409008026123\n",
      "step 83\n",
      "training loss: 2.8354921340942383\n",
      "step 84\n",
      "training loss: 2.871459722518921\n",
      "step 85\n",
      "training loss: 2.843491792678833\n",
      "step 86\n",
      "training loss: 2.879446029663086\n",
      "step 87\n",
      "training loss: 2.861912488937378\n",
      "step 88\n",
      "training loss: 2.83984112739563\n",
      "step 89\n",
      "training loss: 2.8763511180877686\n",
      "step 90\n",
      "training loss: 2.8497283458709717\n",
      "validation loss: 2.88230562210083\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.859853982925415\n",
      "step 92\n",
      "training loss: 2.8621623516082764\n",
      "step 93\n",
      "training loss: 2.8296635150909424\n",
      "step 94\n",
      "training loss: 2.877429962158203\n",
      "step 95\n",
      "training loss: 2.856076717376709\n",
      "step 96\n",
      "training loss: 2.8760123252868652\n",
      "step 97\n",
      "training loss: 2.8710827827453613\n",
      "step 98\n",
      "training loss: 2.8930563926696777\n",
      "step 99\n",
      "training loss: 2.8771512508392334\n",
      "step 100\n",
      "training loss: 2.8363277912139893\n",
      "validation loss: 2.879537343978882\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.873408794403076\n",
      "step 102\n",
      "training loss: 2.8594279289245605\n",
      "step 103\n",
      "training loss: 2.847898244857788\n",
      "step 104\n",
      "training loss: 2.858025312423706\n",
      "step 105\n",
      "training loss: 2.8576362133026123\n",
      "step 106\n",
      "training loss: 2.8768093585968018\n",
      "step 107\n",
      "training loss: 2.8607709407806396\n",
      "step 108\n",
      "training loss: 2.8770782947540283\n",
      "step 109\n",
      "training loss: 2.877286434173584\n",
      "step 110\n",
      "training loss: 2.878168821334839\n",
      "validation loss: 2.8690075874328613\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8545732498168945\n",
      "step 112\n",
      "training loss: 2.8673255443573\n",
      "step 113\n",
      "training loss: 2.862842559814453\n",
      "step 114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8567538261413574\n",
      "step 115\n",
      "training loss: 2.868561029434204\n",
      "step 116\n",
      "training loss: 2.845238208770752\n",
      "step 117\n",
      "training loss: 2.8695597648620605\n",
      "step 118\n",
      "training loss: 2.8727633953094482\n",
      "step 119\n",
      "training loss: 2.8130407333374023\n",
      "step 120\n",
      "training loss: 2.854809045791626\n",
      "validation loss: 2.9403235912323\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.849914789199829\n",
      "step 122\n",
      "training loss: 2.864382028579712\n",
      "step 123\n",
      "training loss: 2.87166166305542\n",
      "step 124\n",
      "training loss: 2.8453781604766846\n",
      "step 125\n",
      "training loss: 2.868171453475952\n",
      "step 126\n",
      "training loss: 2.8150153160095215\n",
      "step 127\n",
      "training loss: 2.853200674057007\n",
      "step 128\n",
      "training loss: 2.874296188354492\n",
      "step 129\n",
      "training loss: 2.864577054977417\n",
      "step 130\n",
      "training loss: 2.8578062057495117\n",
      "validation loss: 2.8775634765625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8473925590515137\n",
      "step 132\n",
      "training loss: 2.8641445636749268\n",
      "step 133\n",
      "training loss: 2.852015972137451\n",
      "step 134\n",
      "training loss: 2.8627631664276123\n",
      "step 135\n",
      "training loss: 2.865579128265381\n",
      "step 136\n",
      "training loss: 2.861891269683838\n",
      "step 137\n",
      "training loss: 2.8572211265563965\n",
      "step 138\n",
      "training loss: 2.8593811988830566\n",
      "step 139\n",
      "training loss: 2.8372888565063477\n",
      "step 140\n",
      "training loss: 2.8392882347106934\n",
      "validation loss: 2.8592898845672607\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8339576721191406\n",
      "step 142\n",
      "training loss: 2.866828203201294\n",
      "step 143\n",
      "training loss: 2.869976282119751\n",
      "step 144\n",
      "training loss: 2.8618509769439697\n",
      "step 145\n",
      "training loss: 2.8608500957489014\n",
      "step 146\n",
      "training loss: 2.8528554439544678\n",
      "step 147\n",
      "training loss: 2.869236946105957\n",
      "step 148\n",
      "training loss: 2.8898162841796875\n",
      "step 149\n",
      "training loss: 2.810370683670044\n",
      "step 150\n",
      "training loss: 2.8599321842193604\n",
      "validation loss: 2.857098340988159\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8532891273498535\n",
      "step 152\n",
      "training loss: 2.8783926963806152\n",
      "step 153\n",
      "training loss: 2.8599066734313965\n",
      "step 154\n",
      "training loss: 2.8801534175872803\n",
      "step 155\n",
      "training loss: 2.8552441596984863\n",
      "step 156\n",
      "training loss: 2.8414900302886963\n",
      "step 157\n",
      "training loss: 2.8561196327209473\n",
      "step 158\n",
      "training loss: 2.8661949634552\n",
      "step 159\n",
      "training loss: 2.850055456161499\n",
      "step 160\n",
      "training loss: 2.877920627593994\n",
      "validation loss: 2.8950278759002686\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.862518072128296\n",
      "step 162\n",
      "training loss: 2.8214163780212402\n",
      "step 163\n",
      "training loss: 2.8504865169525146\n",
      "step 164\n",
      "training loss: 2.8537304401397705\n",
      "step 165\n",
      "training loss: 2.857346534729004\n",
      "step 166\n",
      "training loss: 2.8679966926574707\n",
      "step 167\n",
      "training loss: 2.865330219268799\n",
      "step 168\n",
      "training loss: 2.8665366172790527\n",
      "step 169\n",
      "training loss: 2.8160483837127686\n",
      "step 170\n",
      "training loss: 2.866088628768921\n",
      "validation loss: 2.8297314643859863\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.856621503829956\n",
      "step 172\n",
      "training loss: 2.867233991622925\n",
      "step 173\n",
      "training loss: 2.865187168121338\n",
      "step 174\n",
      "training loss: 2.873497486114502\n",
      "step 175\n",
      "training loss: 2.8692989349365234\n",
      "step 176\n",
      "training loss: 2.8523480892181396\n",
      "step 177\n",
      "training loss: 2.8793556690216064\n",
      "step 178\n",
      "training loss: 2.8761329650878906\n",
      "step 179\n",
      "training loss: 2.8650410175323486\n",
      "step 180\n",
      "training loss: 2.8607237339019775\n",
      "validation loss: 2.8568105697631836\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8684446811676025\n",
      "step 182\n",
      "training loss: 2.858488082885742\n",
      "step 183\n",
      "training loss: 2.8596248626708984\n",
      "step 184\n",
      "training loss: 2.8372645378112793\n",
      "step 185\n",
      "training loss: 2.8204102516174316\n",
      "step 186\n",
      "training loss: 2.8683605194091797\n",
      "step 187\n",
      "training loss: 2.8495895862579346\n",
      "step 188\n",
      "training loss: 2.851818323135376\n",
      "step 189\n",
      "training loss: 2.865952730178833\n",
      "step 190\n",
      "training loss: 2.893357753753662\n",
      "validation loss: 2.8667044639587402\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.847226858139038\n",
      "step 192\n",
      "training loss: 2.871774911880493\n",
      "step 193\n",
      "training loss: 2.8642420768737793\n",
      "step 194\n",
      "training loss: 2.871892213821411\n",
      "step 195\n",
      "training loss: 2.8475706577301025\n",
      "step 196\n",
      "training loss: 2.8576977252960205\n",
      "step 197\n",
      "training loss: 2.8346147537231445\n",
      "step 198\n",
      "training loss: 2.8503456115722656\n",
      "step 199\n",
      "training loss: 2.8612797260284424\n",
      "step 200\n",
      "training loss: 2.8420310020446777\n",
      "validation loss: 2.869978904724121\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8866922855377197\n",
      "step 202\n",
      "training loss: 2.862410068511963\n",
      "step 203\n",
      "training loss: 2.8502302169799805\n",
      "step 204\n",
      "training loss: 2.8611650466918945\n",
      "step 205\n",
      "training loss: 2.867764711380005\n",
      "step 206\n",
      "training loss: 2.8520472049713135\n",
      "step 207\n",
      "training loss: 2.8424019813537598\n",
      "step 208\n",
      "training loss: 2.828545331954956\n",
      "step 209\n",
      "training loss: 2.813075065612793\n",
      "step 210\n",
      "training loss: 2.8767075538635254\n",
      "validation loss: 2.902918577194214\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.853938102722168\n",
      "step 212\n",
      "training loss: 2.878697395324707\n",
      "step 213\n",
      "training loss: 2.853961229324341\n",
      "step 214\n",
      "training loss: 2.8631770610809326\n",
      "step 215\n",
      "training loss: 2.8792169094085693\n",
      "step 216\n",
      "training loss: 2.8570683002471924\n",
      "step 217\n",
      "training loss: 2.8549768924713135\n",
      "step 218\n",
      "training loss: 2.8660523891448975\n",
      "step 219\n",
      "training loss: 2.8455026149749756\n",
      "step 220\n",
      "training loss: 2.8552801609039307\n",
      "validation loss: 2.866380214691162\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8467581272125244\n",
      "step 222\n",
      "training loss: 2.8517682552337646\n",
      "step 223\n",
      "training loss: 2.8396716117858887\n",
      "step 224\n",
      "training loss: 2.859588861465454\n",
      "step 225\n",
      "training loss: 2.864429235458374\n",
      "step 226\n",
      "training loss: 2.84616756439209\n",
      "step 227\n",
      "training loss: 2.8743247985839844\n",
      "step 228\n",
      "training loss: 2.8515784740448\n",
      "step 229\n",
      "training loss: 2.8621742725372314\n",
      "step 230\n",
      "training loss: 2.855729341506958\n",
      "validation loss: 2.858882427215576\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.86525821685791\n",
      "step 232\n",
      "training loss: 2.791985034942627\n",
      "step 233\n",
      "training loss: 2.8506739139556885\n",
      "step 234\n",
      "training loss: 2.830303192138672\n",
      "step 235\n",
      "training loss: 2.849297523498535\n",
      "step 236\n",
      "training loss: 2.8704793453216553\n",
      "step 237\n",
      "training loss: 2.870981454849243\n",
      "step 238\n",
      "training loss: 2.8622677326202393\n",
      "step 239\n",
      "training loss: 2.861673593521118\n",
      "step 240\n",
      "training loss: 2.8596699237823486\n",
      "validation loss: 2.8254082202911377\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8511786460876465\n",
      "step 242\n",
      "training loss: 2.85575270652771\n",
      "step 243\n",
      "training loss: 2.8716542720794678\n",
      "step 244\n",
      "training loss: 2.8866987228393555\n",
      "step 245\n",
      "training loss: 2.8562676906585693\n",
      "step 246\n",
      "training loss: 2.845637083053589\n",
      "step 247\n",
      "training loss: 2.8487963676452637\n",
      "step 248\n",
      "training loss: 2.848208427429199\n",
      "step 249\n",
      "training loss: 2.8601722717285156\n",
      "step 250\n",
      "training loss: 2.864081621170044\n",
      "validation loss: 2.8309531211853027\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8578968048095703\n",
      "step 252\n",
      "training loss: 2.8655519485473633\n",
      "step 253\n",
      "training loss: 2.871979236602783\n",
      "step 254\n",
      "training loss: 2.8493926525115967\n",
      "step 255\n",
      "training loss: 2.874030351638794\n",
      "step 256\n",
      "training loss: 2.8767058849334717\n",
      "step 257\n",
      "training loss: 2.8458361625671387\n",
      "step 258\n",
      "training loss: 2.845818281173706\n",
      "step 259\n",
      "training loss: 2.8707332611083984\n",
      "step 260\n",
      "training loss: 2.865816354751587\n",
      "validation loss: 2.829197883605957\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.865933656692505\n",
      "step 262\n",
      "training loss: 2.8663434982299805\n",
      "step 263\n",
      "training loss: 2.8672261238098145\n",
      "step 264\n",
      "training loss: 2.8579068183898926\n",
      "step 265\n",
      "training loss: 2.8643157482147217\n",
      "step 266\n",
      "training loss: 2.861825942993164\n",
      "step 267\n",
      "training loss: 2.8595430850982666\n",
      "step 268\n",
      "training loss: 2.8596303462982178\n",
      "step 269\n",
      "training loss: 2.821589469909668\n",
      "step 270\n",
      "training loss: 2.8569531440734863\n",
      "validation loss: 2.8567869663238525\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8717172145843506\n",
      "----------3.0 min per epoch----------\n",
      "epoch 24\n",
      "step 0\n",
      "training loss: 2.8494131565093994\n",
      "validation loss: 2.8657028675079346\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8582897186279297\n",
      "step 2\n",
      "training loss: 2.8860630989074707\n",
      "step 3\n",
      "training loss: 2.8649356365203857\n",
      "step 4\n",
      "training loss: 2.87978458404541\n",
      "step 5\n",
      "training loss: 2.8804337978363037\n",
      "step 6\n",
      "training loss: 2.866325855255127\n",
      "step 7\n",
      "training loss: 2.851611375808716\n",
      "step 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.87581205368042\n",
      "step 9\n",
      "training loss: 2.8751912117004395\n",
      "step 10\n",
      "training loss: 2.869842529296875\n",
      "validation loss: 2.868962287902832\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8654489517211914\n",
      "step 12\n",
      "training loss: 2.8783457279205322\n",
      "step 13\n",
      "training loss: 2.8864896297454834\n",
      "step 14\n",
      "training loss: 2.804713487625122\n",
      "step 15\n",
      "training loss: 2.847635507583618\n",
      "step 16\n",
      "training loss: 2.821990728378296\n",
      "step 17\n",
      "training loss: 2.855341911315918\n",
      "step 18\n",
      "training loss: 2.8851871490478516\n",
      "step 19\n",
      "training loss: 2.8793632984161377\n",
      "step 20\n",
      "training loss: 2.8854923248291016\n",
      "validation loss: 2.865246534347534\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.871211290359497\n",
      "step 22\n",
      "training loss: 2.794093370437622\n",
      "step 23\n",
      "training loss: 2.7514805793762207\n",
      "step 24\n",
      "training loss: 2.825425386428833\n",
      "step 25\n",
      "training loss: 2.8927485942840576\n",
      "step 26\n",
      "training loss: 2.8734281063079834\n",
      "step 27\n",
      "training loss: 2.8585972785949707\n",
      "step 28\n",
      "training loss: 2.860875129699707\n",
      "step 29\n",
      "training loss: 2.8736846446990967\n",
      "step 30\n",
      "training loss: 2.8753108978271484\n",
      "validation loss: 2.8641161918640137\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.876847982406616\n",
      "step 32\n",
      "training loss: 2.865015745162964\n",
      "step 33\n",
      "training loss: 2.8794310092926025\n",
      "step 34\n",
      "training loss: 2.8808658123016357\n",
      "step 35\n",
      "training loss: 2.8371777534484863\n",
      "step 36\n",
      "training loss: 2.8460731506347656\n",
      "step 37\n",
      "training loss: 2.859975576400757\n",
      "step 38\n",
      "training loss: 2.862978458404541\n",
      "step 39\n",
      "training loss: 2.8748819828033447\n",
      "step 40\n",
      "training loss: 2.8677737712860107\n",
      "validation loss: 2.8520383834838867\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8745293617248535\n",
      "step 42\n",
      "training loss: 2.877995014190674\n",
      "step 43\n",
      "training loss: 2.867327928543091\n",
      "step 44\n",
      "training loss: 2.8870716094970703\n",
      "step 45\n",
      "training loss: 2.8634438514709473\n",
      "step 46\n",
      "training loss: 2.873962879180908\n",
      "step 47\n",
      "training loss: 2.8803048133850098\n",
      "step 48\n",
      "training loss: 2.876349687576294\n",
      "step 49\n",
      "training loss: 2.8734757900238037\n",
      "step 50\n",
      "training loss: 2.876844644546509\n",
      "validation loss: 2.854081630706787\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8615832328796387\n",
      "step 52\n",
      "training loss: 2.8385982513427734\n",
      "step 53\n",
      "training loss: 2.8620941638946533\n",
      "step 54\n",
      "training loss: 2.875194787979126\n",
      "step 55\n",
      "training loss: 2.8424534797668457\n",
      "step 56\n",
      "training loss: 2.8599956035614014\n",
      "step 57\n",
      "training loss: 2.862609386444092\n",
      "step 58\n",
      "training loss: 2.852263927459717\n",
      "step 59\n",
      "training loss: 2.8816261291503906\n",
      "step 60\n",
      "training loss: 2.8559205532073975\n",
      "validation loss: 2.8341474533081055\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.870448589324951\n",
      "step 62\n",
      "training loss: 2.856327533721924\n",
      "step 63\n",
      "training loss: 2.8693740367889404\n",
      "step 64\n",
      "training loss: 2.822133779525757\n",
      "step 65\n",
      "training loss: 2.884786605834961\n",
      "step 66\n",
      "training loss: 2.848552942276001\n",
      "step 67\n",
      "training loss: 2.88210391998291\n",
      "step 68\n",
      "training loss: 2.866274833679199\n",
      "step 69\n",
      "training loss: 2.846027135848999\n",
      "step 70\n",
      "training loss: 2.8415048122406006\n",
      "validation loss: 2.8430614471435547\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8640568256378174\n",
      "step 72\n",
      "training loss: 2.8517017364501953\n",
      "step 73\n",
      "training loss: 2.843994379043579\n",
      "step 74\n",
      "training loss: 2.8792436122894287\n",
      "step 75\n",
      "training loss: 2.8670616149902344\n",
      "step 76\n",
      "training loss: 2.8594002723693848\n",
      "step 77\n",
      "training loss: 2.8408780097961426\n",
      "step 78\n",
      "training loss: 2.8570263385772705\n",
      "step 79\n",
      "training loss: 2.8592193126678467\n",
      "step 80\n",
      "training loss: 2.8350913524627686\n",
      "validation loss: 2.8697729110717773\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.848398447036743\n",
      "step 82\n",
      "training loss: 2.851545810699463\n",
      "step 83\n",
      "training loss: 2.842869520187378\n",
      "step 84\n",
      "training loss: 2.8352136611938477\n",
      "step 85\n",
      "training loss: 2.8704471588134766\n",
      "step 86\n",
      "training loss: 2.8426196575164795\n",
      "step 87\n",
      "training loss: 2.8786699771881104\n",
      "step 88\n",
      "training loss: 2.8594188690185547\n",
      "step 89\n",
      "training loss: 2.837557315826416\n",
      "step 90\n",
      "training loss: 2.875992774963379\n",
      "validation loss: 2.8738248348236084\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8478481769561768\n",
      "step 92\n",
      "training loss: 2.8592727184295654\n",
      "step 93\n",
      "training loss: 2.8629965782165527\n",
      "step 94\n",
      "training loss: 2.830580234527588\n",
      "step 95\n",
      "training loss: 2.8745908737182617\n",
      "step 96\n",
      "training loss: 2.8561346530914307\n",
      "step 97\n",
      "training loss: 2.875458240509033\n",
      "step 98\n",
      "training loss: 2.8720457553863525\n",
      "step 99\n",
      "training loss: 2.8950862884521484\n",
      "step 100\n",
      "training loss: 2.877908945083618\n",
      "validation loss: 2.8336691856384277\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.833953619003296\n",
      "step 102\n",
      "training loss: 2.867492914199829\n",
      "step 103\n",
      "training loss: 2.856642484664917\n",
      "step 104\n",
      "training loss: 2.842916965484619\n",
      "step 105\n",
      "training loss: 2.8590087890625\n",
      "step 106\n",
      "training loss: 2.8559653759002686\n",
      "step 107\n",
      "training loss: 2.8739371299743652\n",
      "step 108\n",
      "training loss: 2.8596739768981934\n",
      "step 109\n",
      "training loss: 2.874518632888794\n",
      "step 110\n",
      "training loss: 2.874251365661621\n",
      "validation loss: 2.8493759632110596\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8789350986480713\n",
      "step 112\n",
      "training loss: 2.8551454544067383\n",
      "step 113\n",
      "training loss: 2.865581750869751\n",
      "step 114\n",
      "training loss: 2.863407850265503\n",
      "step 115\n",
      "training loss: 2.8565807342529297\n",
      "step 116\n",
      "training loss: 2.86545729637146\n",
      "step 117\n",
      "training loss: 2.843712091445923\n",
      "step 118\n",
      "training loss: 2.8673789501190186\n",
      "step 119\n",
      "training loss: 2.8714332580566406\n",
      "step 120\n",
      "training loss: 2.81463623046875\n",
      "validation loss: 2.882894992828369\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8537440299987793\n",
      "step 122\n",
      "training loss: 2.8502304553985596\n",
      "step 123\n",
      "training loss: 2.863835334777832\n",
      "step 124\n",
      "training loss: 2.8729360103607178\n",
      "step 125\n",
      "training loss: 2.8435568809509277\n",
      "step 126\n",
      "training loss: 2.8650691509246826\n",
      "step 127\n",
      "training loss: 2.813061237335205\n",
      "step 128\n",
      "training loss: 2.8489129543304443\n",
      "step 129\n",
      "training loss: 2.8742194175720215\n",
      "step 130\n",
      "training loss: 2.866020679473877\n",
      "validation loss: 2.8753340244293213\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8594980239868164\n",
      "step 132\n",
      "training loss: 2.8474044799804688\n",
      "step 133\n",
      "training loss: 2.866318702697754\n",
      "step 134\n",
      "training loss: 2.852900981903076\n",
      "step 135\n",
      "training loss: 2.8616204261779785\n",
      "step 136\n",
      "training loss: 2.8647656440734863\n",
      "step 137\n",
      "training loss: 2.86065673828125\n",
      "step 138\n",
      "training loss: 2.858379364013672\n",
      "step 139\n",
      "training loss: 2.858940839767456\n",
      "step 140\n",
      "training loss: 2.8366286754608154\n",
      "validation loss: 2.8616669178009033\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8411078453063965\n",
      "step 142\n",
      "training loss: 2.8315234184265137\n",
      "step 143\n",
      "training loss: 2.868337869644165\n",
      "step 144\n",
      "training loss: 2.8696706295013428\n",
      "step 145\n",
      "training loss: 2.8590853214263916\n",
      "step 146\n",
      "training loss: 2.8606393337249756\n",
      "step 147\n",
      "training loss: 2.8534088134765625\n",
      "step 148\n",
      "training loss: 2.8679592609405518\n",
      "step 149\n",
      "training loss: 2.8893344402313232\n",
      "step 150\n",
      "training loss: 2.8094518184661865\n",
      "validation loss: 2.935884714126587\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.858966112136841\n",
      "step 152\n",
      "training loss: 2.8525032997131348\n",
      "step 153\n",
      "training loss: 2.876737117767334\n",
      "step 154\n",
      "training loss: 2.8570282459259033\n",
      "step 155\n",
      "training loss: 2.879964828491211\n",
      "step 156\n",
      "training loss: 2.8507437705993652\n",
      "step 157\n",
      "training loss: 2.8385651111602783\n",
      "step 158\n",
      "training loss: 2.8530023097991943\n",
      "step 159\n",
      "training loss: 2.8651466369628906\n",
      "step 160\n",
      "training loss: 2.8500163555145264\n",
      "validation loss: 2.879439115524292\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8785135746002197\n",
      "step 162\n",
      "training loss: 2.8594970703125\n",
      "step 163\n",
      "training loss: 2.8223769664764404\n",
      "step 164\n",
      "training loss: 2.8507919311523438\n",
      "step 165\n",
      "training loss: 2.8524081707000732\n",
      "step 166\n",
      "training loss: 2.855757236480713\n",
      "step 167\n",
      "training loss: 2.8689093589782715\n",
      "step 168\n",
      "training loss: 2.8648438453674316\n",
      "step 169\n",
      "training loss: 2.8654544353485107\n",
      "step 170\n",
      "training loss: 2.8109982013702393\n",
      "validation loss: 2.851855754852295\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.862724542617798\n",
      "step 172\n",
      "training loss: 2.8560614585876465\n",
      "step 173\n",
      "training loss: 2.862342119216919\n",
      "step 174\n",
      "training loss: 2.86195707321167\n",
      "step 175\n",
      "training loss: 2.872310161590576\n",
      "step 176\n",
      "training loss: 2.8679256439208984\n",
      "step 177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8528027534484863\n",
      "step 178\n",
      "training loss: 2.878232955932617\n",
      "step 179\n",
      "training loss: 2.875868082046509\n",
      "step 180\n",
      "training loss: 2.865525007247925\n",
      "validation loss: 2.8589844703674316\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.861342430114746\n",
      "step 182\n",
      "training loss: 2.8680717945098877\n",
      "step 183\n",
      "training loss: 2.8592958450317383\n",
      "step 184\n",
      "training loss: 2.8565709590911865\n",
      "step 185\n",
      "training loss: 2.836099147796631\n",
      "step 186\n",
      "training loss: 2.817620277404785\n",
      "step 187\n",
      "training loss: 2.8685927391052246\n",
      "step 188\n",
      "training loss: 2.8489255905151367\n",
      "step 189\n",
      "training loss: 2.848992109298706\n",
      "step 190\n",
      "training loss: 2.8636245727539062\n",
      "validation loss: 2.883911609649658\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8920094966888428\n",
      "step 192\n",
      "training loss: 2.843153953552246\n",
      "step 193\n",
      "training loss: 2.870297431945801\n",
      "step 194\n",
      "training loss: 2.8605425357818604\n",
      "step 195\n",
      "training loss: 2.870408058166504\n",
      "step 196\n",
      "training loss: 2.8462307453155518\n",
      "step 197\n",
      "training loss: 2.858875036239624\n",
      "step 198\n",
      "training loss: 2.834416627883911\n",
      "step 199\n",
      "training loss: 2.849604606628418\n",
      "step 200\n",
      "training loss: 2.857563018798828\n",
      "validation loss: 2.830932855606079\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8437633514404297\n",
      "step 202\n",
      "training loss: 2.8833162784576416\n",
      "step 203\n",
      "training loss: 2.8584768772125244\n",
      "step 204\n",
      "training loss: 2.8475515842437744\n",
      "step 205\n",
      "training loss: 2.8596785068511963\n",
      "step 206\n",
      "training loss: 2.866312026977539\n",
      "step 207\n",
      "training loss: 2.8512163162231445\n",
      "step 208\n",
      "training loss: 2.8424532413482666\n",
      "step 209\n",
      "training loss: 2.8277814388275146\n",
      "step 210\n",
      "training loss: 2.813072681427002\n",
      "validation loss: 2.8559484481811523\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.876230001449585\n",
      "step 212\n",
      "training loss: 2.8532555103302\n",
      "step 213\n",
      "training loss: 2.8786182403564453\n",
      "step 214\n",
      "training loss: 2.8529868125915527\n",
      "step 215\n",
      "training loss: 2.859294891357422\n",
      "step 216\n",
      "training loss: 2.879347562789917\n",
      "step 217\n",
      "training loss: 2.85593843460083\n",
      "step 218\n",
      "training loss: 2.851940393447876\n",
      "step 219\n",
      "training loss: 2.8675153255462646\n",
      "step 220\n",
      "training loss: 2.844313859939575\n",
      "validation loss: 2.86616587638855\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8509368896484375\n",
      "step 222\n",
      "training loss: 2.845710039138794\n",
      "step 223\n",
      "training loss: 2.850898027420044\n",
      "step 224\n",
      "training loss: 2.837240219116211\n",
      "step 225\n",
      "training loss: 2.856987714767456\n",
      "step 226\n",
      "training loss: 2.862762451171875\n",
      "step 227\n",
      "training loss: 2.845423698425293\n",
      "step 228\n",
      "training loss: 2.874898672103882\n",
      "step 229\n",
      "training loss: 2.8498926162719727\n",
      "step 230\n",
      "training loss: 2.8626904487609863\n",
      "validation loss: 2.869288206100464\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8560144901275635\n",
      "step 232\n",
      "training loss: 2.8650529384613037\n",
      "step 233\n",
      "training loss: 2.7905726432800293\n",
      "step 234\n",
      "training loss: 2.850614070892334\n",
      "step 235\n",
      "training loss: 2.8304836750030518\n",
      "step 236\n",
      "training loss: 2.8429014682769775\n",
      "step 237\n",
      "training loss: 2.868464231491089\n",
      "step 238\n",
      "training loss: 2.8695197105407715\n",
      "step 239\n",
      "training loss: 2.8608784675598145\n",
      "step 240\n",
      "training loss: 2.8609375953674316\n",
      "validation loss: 2.893256425857544\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8595807552337646\n",
      "step 242\n",
      "training loss: 2.8502302169799805\n",
      "step 243\n",
      "training loss: 2.855534553527832\n",
      "step 244\n",
      "training loss: 2.869826555252075\n",
      "step 245\n",
      "training loss: 2.885253667831421\n",
      "step 246\n",
      "training loss: 2.8555548191070557\n",
      "step 247\n",
      "training loss: 2.8443448543548584\n",
      "step 248\n",
      "training loss: 2.84834361076355\n",
      "step 249\n",
      "training loss: 2.8449809551239014\n",
      "step 250\n",
      "training loss: 2.8598577976226807\n",
      "validation loss: 2.8660125732421875\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.862403631210327\n",
      "step 252\n",
      "training loss: 2.8557887077331543\n",
      "step 253\n",
      "training loss: 2.8658735752105713\n",
      "step 254\n",
      "training loss: 2.87165904045105\n",
      "step 255\n",
      "training loss: 2.849196672439575\n",
      "step 256\n",
      "training loss: 2.872263193130493\n",
      "step 257\n",
      "training loss: 2.8760509490966797\n",
      "step 258\n",
      "training loss: 2.845319986343384\n",
      "step 259\n",
      "training loss: 2.8458948135375977\n",
      "step 260\n",
      "training loss: 2.8700129985809326\n",
      "validation loss: 2.8577089309692383\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8642189502716064\n",
      "step 262\n",
      "training loss: 2.8629884719848633\n",
      "step 263\n",
      "training loss: 2.8651132583618164\n",
      "step 264\n",
      "training loss: 2.8686695098876953\n",
      "step 265\n",
      "training loss: 2.8590376377105713\n",
      "step 266\n",
      "training loss: 2.861992359161377\n",
      "step 267\n",
      "training loss: 2.860252618789673\n",
      "step 268\n",
      "training loss: 2.8571348190307617\n",
      "step 269\n",
      "training loss: 2.8571348190307617\n",
      "step 270\n",
      "training loss: 2.820155620574951\n",
      "validation loss: 2.8232946395874023\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8463356494903564\n",
      "----------3.0 min per epoch----------\n",
      "epoch 25\n",
      "step 0\n",
      "training loss: 2.86991024017334\n",
      "validation loss: 2.8327114582061768\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.845581293106079\n",
      "step 2\n",
      "training loss: 2.8502063751220703\n",
      "step 3\n",
      "training loss: 2.883056402206421\n",
      "step 4\n",
      "training loss: 2.8625705242156982\n",
      "step 5\n",
      "training loss: 2.877624750137329\n",
      "step 6\n",
      "training loss: 2.8784339427948\n",
      "step 7\n",
      "training loss: 2.867356061935425\n",
      "step 8\n",
      "training loss: 2.849128246307373\n",
      "step 9\n",
      "training loss: 2.874142646789551\n",
      "step 10\n",
      "training loss: 2.8753304481506348\n",
      "validation loss: 2.8297219276428223\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8674092292785645\n",
      "step 12\n",
      "training loss: 2.863771438598633\n",
      "step 13\n",
      "training loss: 2.876859664916992\n",
      "step 14\n",
      "training loss: 2.882873773574829\n",
      "step 15\n",
      "training loss: 2.803349494934082\n",
      "step 16\n",
      "training loss: 2.84590220451355\n",
      "step 17\n",
      "training loss: 2.812706470489502\n",
      "step 18\n",
      "training loss: 2.851621627807617\n",
      "step 19\n",
      "training loss: 2.8808867931365967\n",
      "step 20\n",
      "training loss: 2.8705809116363525\n",
      "validation loss: 2.8611066341400146\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8814163208007812\n",
      "step 22\n",
      "training loss: 2.8641245365142822\n",
      "step 23\n",
      "training loss: 2.7832064628601074\n",
      "step 24\n",
      "training loss: 2.7469966411590576\n",
      "step 25\n",
      "training loss: 2.8304591178894043\n",
      "step 26\n",
      "training loss: 2.8847007751464844\n",
      "step 27\n",
      "training loss: 2.8714141845703125\n",
      "step 28\n",
      "training loss: 2.8602662086486816\n",
      "step 29\n",
      "training loss: 2.85990047454834\n",
      "step 30\n",
      "training loss: 2.8720407485961914\n",
      "validation loss: 2.8756296634674072\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.876638889312744\n",
      "step 32\n",
      "training loss: 2.8792245388031006\n",
      "step 33\n",
      "training loss: 2.8643112182617188\n",
      "step 34\n",
      "training loss: 2.8820786476135254\n",
      "step 35\n",
      "training loss: 2.881981611251831\n",
      "step 36\n",
      "training loss: 2.8361096382141113\n",
      "step 37\n",
      "training loss: 2.8486709594726562\n",
      "step 38\n",
      "training loss: 2.8580946922302246\n",
      "step 39\n",
      "training loss: 2.8597676753997803\n",
      "step 40\n",
      "training loss: 2.8775315284729004\n",
      "validation loss: 2.8675155639648438\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.867985248565674\n",
      "step 42\n",
      "training loss: 2.8727238178253174\n",
      "step 43\n",
      "training loss: 2.8760409355163574\n",
      "step 44\n",
      "training loss: 2.8683242797851562\n",
      "step 45\n",
      "training loss: 2.884836435317993\n",
      "step 46\n",
      "training loss: 2.8646862506866455\n",
      "step 47\n",
      "training loss: 2.8740851879119873\n",
      "step 48\n",
      "training loss: 2.883871078491211\n",
      "step 49\n",
      "training loss: 2.877009630203247\n",
      "step 50\n",
      "training loss: 2.872981548309326\n",
      "validation loss: 2.863523006439209\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8766794204711914\n",
      "step 52\n",
      "training loss: 2.861732006072998\n",
      "step 53\n",
      "training loss: 2.8411026000976562\n",
      "step 54\n",
      "training loss: 2.86409592628479\n",
      "step 55\n",
      "training loss: 2.876065731048584\n",
      "step 56\n",
      "training loss: 2.8360612392425537\n",
      "step 57\n",
      "training loss: 2.8586204051971436\n",
      "step 58\n",
      "training loss: 2.861874580383301\n",
      "step 59\n",
      "training loss: 2.849918842315674\n",
      "step 60\n",
      "training loss: 2.8840737342834473\n",
      "validation loss: 2.8674960136413574\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.853421926498413\n",
      "step 62\n",
      "training loss: 2.8683841228485107\n",
      "step 63\n",
      "training loss: 2.8563272953033447\n",
      "step 64\n",
      "training loss: 2.8699514865875244\n",
      "step 65\n",
      "training loss: 2.82076358795166\n",
      "step 66\n",
      "training loss: 2.8824658393859863\n",
      "step 67\n",
      "training loss: 2.8474199771881104\n",
      "step 68\n",
      "training loss: 2.881948471069336\n",
      "step 69\n",
      "training loss: 2.8645241260528564\n",
      "step 70\n",
      "training loss: 2.8437459468841553\n",
      "validation loss: 2.845747232437134\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8424153327941895\n",
      "step 72\n",
      "training loss: 2.8628690242767334\n",
      "step 73\n",
      "training loss: 2.853849411010742\n",
      "step 74\n",
      "training loss: 2.8449854850769043\n",
      "step 75\n",
      "training loss: 2.878662586212158\n",
      "step 76\n",
      "training loss: 2.8663039207458496\n",
      "step 77\n",
      "training loss: 2.8577773571014404\n",
      "step 78\n",
      "training loss: 2.8432676792144775\n",
      "step 79\n",
      "training loss: 2.8573875427246094\n",
      "step 80\n",
      "training loss: 2.8586227893829346\n",
      "validation loss: 2.847179889678955\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.834869861602783\n",
      "step 82\n",
      "training loss: 2.848605155944824\n",
      "step 83\n",
      "training loss: 2.85044527053833\n",
      "step 84\n",
      "training loss: 2.8432421684265137\n",
      "step 85\n",
      "training loss: 2.8350183963775635\n",
      "step 86\n",
      "training loss: 2.873769760131836\n",
      "step 87\n",
      "training loss: 2.8438568115234375\n",
      "step 88\n",
      "training loss: 2.8787841796875\n",
      "step 89\n",
      "training loss: 2.8600873947143555\n",
      "step 90\n",
      "training loss: 2.8388259410858154\n",
      "validation loss: 2.8321456909179688\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.874864339828491\n",
      "step 92\n",
      "training loss: 2.850424289703369\n",
      "step 93\n",
      "training loss: 2.8601441383361816\n",
      "step 94\n",
      "training loss: 2.8646199703216553\n",
      "step 95\n",
      "training loss: 2.832547903060913\n",
      "step 96\n",
      "training loss: 2.875872850418091\n",
      "step 97\n",
      "training loss: 2.8545689582824707\n",
      "step 98\n",
      "training loss: 2.8753087520599365\n",
      "step 99\n",
      "training loss: 2.8702540397644043\n",
      "step 100\n",
      "training loss: 2.893380880355835\n",
      "validation loss: 2.840937852859497\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8788106441497803\n",
      "step 102\n",
      "training loss: 2.835150718688965\n",
      "step 103\n",
      "training loss: 2.870457172393799\n",
      "step 104\n",
      "training loss: 2.8578221797943115\n",
      "step 105\n",
      "training loss: 2.8433375358581543\n",
      "step 106\n",
      "training loss: 2.858773946762085\n",
      "step 107\n",
      "training loss: 2.8575661182403564\n",
      "step 108\n",
      "training loss: 2.877743721008301\n",
      "step 109\n",
      "training loss: 2.860415458679199\n",
      "step 110\n",
      "training loss: 2.8754494190216064\n",
      "validation loss: 2.869638681411743\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8740346431732178\n",
      "step 112\n",
      "training loss: 2.879767417907715\n",
      "step 113\n",
      "training loss: 2.8527538776397705\n",
      "step 114\n",
      "training loss: 2.8686017990112305\n",
      "step 115\n",
      "training loss: 2.863860607147217\n",
      "step 116\n",
      "training loss: 2.8551764488220215\n",
      "step 117\n",
      "training loss: 2.8672571182250977\n",
      "step 118\n",
      "training loss: 2.8412022590637207\n",
      "step 119\n",
      "training loss: 2.866856813430786\n",
      "step 120\n",
      "training loss: 2.8718225955963135\n",
      "validation loss: 2.874497175216675\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8133456707000732\n",
      "step 122\n",
      "training loss: 2.85184645652771\n",
      "step 123\n",
      "training loss: 2.8492469787597656\n",
      "step 124\n",
      "training loss: 2.8647007942199707\n",
      "step 125\n",
      "training loss: 2.872286558151245\n",
      "step 126\n",
      "training loss: 2.8448691368103027\n",
      "step 127\n",
      "training loss: 2.8677263259887695\n",
      "step 128\n",
      "training loss: 2.813856601715088\n",
      "step 129\n",
      "training loss: 2.851391077041626\n",
      "step 130\n",
      "training loss: 2.8755133152008057\n",
      "validation loss: 2.826740026473999\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8636507987976074\n",
      "step 132\n",
      "training loss: 2.859577178955078\n",
      "step 133\n",
      "training loss: 2.845410108566284\n",
      "step 134\n",
      "training loss: 2.865629196166992\n",
      "step 135\n",
      "training loss: 2.851576566696167\n",
      "step 136\n",
      "training loss: 2.863098382949829\n",
      "step 137\n",
      "training loss: 2.8679213523864746\n",
      "step 138\n",
      "training loss: 2.862569570541382\n",
      "step 139\n",
      "training loss: 2.8585007190704346\n",
      "step 140\n",
      "training loss: 2.858926773071289\n",
      "validation loss: 2.8519132137298584\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8369147777557373\n",
      "step 142\n",
      "training loss: 2.8404600620269775\n",
      "step 143\n",
      "training loss: 2.832576036453247\n",
      "step 144\n",
      "training loss: 2.866682767868042\n",
      "step 145\n",
      "training loss: 2.8693363666534424\n",
      "step 146\n",
      "training loss: 2.8611059188842773\n",
      "step 147\n",
      "training loss: 2.8596179485321045\n",
      "step 148\n",
      "training loss: 2.85351300239563\n",
      "step 149\n",
      "training loss: 2.8712449073791504\n",
      "step 150\n",
      "training loss: 2.88936185836792\n",
      "validation loss: 2.878206729888916\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8077585697174072\n",
      "step 152\n",
      "training loss: 2.8589253425598145\n",
      "step 153\n",
      "training loss: 2.852679491043091\n",
      "step 154\n",
      "training loss: 2.877295970916748\n",
      "step 155\n",
      "training loss: 2.8558390140533447\n",
      "step 156\n",
      "training loss: 2.8789141178131104\n",
      "step 157\n",
      "training loss: 2.8512842655181885\n",
      "step 158\n",
      "training loss: 2.841154098510742\n",
      "step 159\n",
      "training loss: 2.854473114013672\n",
      "step 160\n",
      "training loss: 2.867241859436035\n",
      "validation loss: 2.8763656616210938\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.849135637283325\n",
      "step 162\n",
      "training loss: 2.874253749847412\n",
      "step 163\n",
      "training loss: 2.8596267700195312\n",
      "step 164\n",
      "training loss: 2.8216309547424316\n",
      "step 165\n",
      "training loss: 2.8521945476531982\n",
      "step 166\n",
      "training loss: 2.8530914783477783\n",
      "step 167\n",
      "training loss: 2.856515645980835\n",
      "step 168\n",
      "training loss: 2.8703420162200928\n",
      "step 169\n",
      "training loss: 2.865417242050171\n",
      "step 170\n",
      "training loss: 2.865447998046875\n",
      "validation loss: 2.867263078689575\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.814509153366089\n",
      "step 172\n",
      "training loss: 2.8626582622528076\n",
      "step 173\n",
      "training loss: 2.856119394302368\n",
      "step 174\n",
      "training loss: 2.8634088039398193\n",
      "step 175\n",
      "training loss: 2.861018419265747\n",
      "step 176\n",
      "training loss: 2.873483180999756\n",
      "step 177\n",
      "training loss: 2.8688743114471436\n",
      "step 178\n",
      "training loss: 2.8524105548858643\n",
      "step 179\n",
      "training loss: 2.8785674571990967\n",
      "step 180\n",
      "training loss: 2.877344846725464\n",
      "validation loss: 2.932394504547119\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.863616943359375\n",
      "step 182\n",
      "training loss: 2.859548568725586\n",
      "step 183\n",
      "training loss: 2.867675304412842\n",
      "step 184\n",
      "training loss: 2.857825994491577\n",
      "step 185\n",
      "training loss: 2.8590335845947266\n",
      "step 186\n",
      "training loss: 2.838827610015869\n",
      "step 187\n",
      "training loss: 2.8193368911743164\n",
      "step 188\n",
      "training loss: 2.867126941680908\n",
      "step 189\n",
      "training loss: 2.8472633361816406\n",
      "step 190\n",
      "training loss: 2.8493666648864746\n",
      "validation loss: 2.881834030151367\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8628039360046387\n",
      "step 192\n",
      "training loss: 2.8903632164001465\n",
      "step 193\n",
      "training loss: 2.8431832790374756\n",
      "step 194\n",
      "training loss: 2.870997190475464\n",
      "step 195\n",
      "training loss: 2.8608410358428955\n",
      "step 196\n",
      "training loss: 2.8692984580993652\n",
      "step 197\n",
      "training loss: 2.8487207889556885\n",
      "step 198\n",
      "training loss: 2.8576955795288086\n",
      "step 199\n",
      "training loss: 2.8350107669830322\n",
      "step 200\n",
      "training loss: 2.848372220993042\n",
      "validation loss: 2.858582019805908\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8589589595794678\n",
      "step 202\n",
      "training loss: 2.8452906608581543\n",
      "step 203\n",
      "training loss: 2.8834075927734375\n",
      "step 204\n",
      "training loss: 2.8589560985565186\n",
      "step 205\n",
      "training loss: 2.846921920776367\n",
      "step 206\n",
      "training loss: 2.8616113662719727\n",
      "step 207\n",
      "training loss: 2.86458683013916\n",
      "step 208\n",
      "training loss: 2.851393938064575\n",
      "step 209\n",
      "training loss: 2.842634916305542\n",
      "step 210\n",
      "training loss: 2.8261945247650146\n",
      "validation loss: 2.859081983566284\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8131484985351562\n",
      "step 212\n",
      "training loss: 2.874814748764038\n",
      "step 213\n",
      "training loss: 2.8535265922546387\n",
      "step 214\n",
      "training loss: 2.8782711029052734\n",
      "step 215\n",
      "training loss: 2.8524420261383057\n",
      "step 216\n",
      "training loss: 2.860840082168579\n",
      "step 217\n",
      "training loss: 2.879995822906494\n",
      "step 218\n",
      "training loss: 2.8565022945404053\n",
      "step 219\n",
      "training loss: 2.8539891242980957\n",
      "step 220\n",
      "training loss: 2.8668582439422607\n",
      "validation loss: 2.894010543823242\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8419060707092285\n",
      "step 222\n",
      "training loss: 2.853466033935547\n",
      "step 223\n",
      "training loss: 2.8459954261779785\n",
      "step 224\n",
      "training loss: 2.850245475769043\n",
      "step 225\n",
      "training loss: 2.8390719890594482\n",
      "step 226\n",
      "training loss: 2.8570053577423096\n",
      "step 227\n",
      "training loss: 2.864410400390625\n",
      "step 228\n",
      "training loss: 2.8452274799346924\n",
      "step 229\n",
      "training loss: 2.8747787475585938\n",
      "step 230\n",
      "training loss: 2.8507254123687744\n",
      "validation loss: 2.824582815170288\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.861957550048828\n",
      "step 232\n",
      "training loss: 2.8574929237365723\n",
      "step 233\n",
      "training loss: 2.8639070987701416\n",
      "step 234\n",
      "training loss: 2.789668560028076\n",
      "step 235\n",
      "training loss: 2.849494695663452\n",
      "step 236\n",
      "training loss: 2.832486152648926\n",
      "step 237\n",
      "training loss: 2.8495676517486572\n",
      "step 238\n",
      "training loss: 2.8660776615142822\n",
      "step 239\n",
      "training loss: 2.8698010444641113\n",
      "step 240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.861421585083008\n",
      "validation loss: 2.8534576892852783\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.86000919342041\n",
      "step 242\n",
      "training loss: 2.858285665512085\n",
      "step 243\n",
      "training loss: 2.848806381225586\n",
      "step 244\n",
      "training loss: 2.855424165725708\n",
      "step 245\n",
      "training loss: 2.8683271408081055\n",
      "step 246\n",
      "training loss: 2.8837697505950928\n",
      "step 247\n",
      "training loss: 2.8541855812072754\n",
      "step 248\n",
      "training loss: 2.8463776111602783\n",
      "step 249\n",
      "training loss: 2.848801851272583\n",
      "step 250\n",
      "training loss: 2.8460028171539307\n",
      "validation loss: 2.8633832931518555\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.858916997909546\n",
      "step 252\n",
      "training loss: 2.860872268676758\n",
      "step 253\n",
      "training loss: 2.855605125427246\n",
      "step 254\n",
      "training loss: 2.864924430847168\n",
      "step 255\n",
      "training loss: 2.872281074523926\n",
      "step 256\n",
      "training loss: 2.8464314937591553\n",
      "step 257\n",
      "training loss: 2.8722143173217773\n",
      "step 258\n",
      "training loss: 2.8753912448883057\n",
      "step 259\n",
      "training loss: 2.8442037105560303\n",
      "step 260\n",
      "training loss: 2.8468756675720215\n",
      "validation loss: 2.865675210952759\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8675484657287598\n",
      "step 262\n",
      "training loss: 2.864935874938965\n",
      "step 263\n",
      "training loss: 2.861715078353882\n",
      "step 264\n",
      "training loss: 2.863870143890381\n",
      "step 265\n",
      "training loss: 2.8655636310577393\n",
      "step 266\n",
      "training loss: 2.8569300174713135\n",
      "step 267\n",
      "training loss: 2.862253427505493\n",
      "step 268\n",
      "training loss: 2.861274242401123\n",
      "step 269\n",
      "training loss: 2.859741687774658\n",
      "step 270\n",
      "training loss: 2.855562925338745\n",
      "validation loss: 2.897554636001587\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8174233436584473\n",
      "----------3.0 min per epoch----------\n",
      "epoch 26\n",
      "step 0\n",
      "training loss: 2.853684663772583\n",
      "validation loss: 2.865676164627075\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.871572732925415\n",
      "step 2\n",
      "training loss: 2.8452086448669434\n",
      "step 3\n",
      "training loss: 2.8515520095825195\n",
      "step 4\n",
      "training loss: 2.8828892707824707\n",
      "step 5\n",
      "training loss: 2.863072633743286\n",
      "step 6\n",
      "training loss: 2.8794023990631104\n",
      "step 7\n",
      "training loss: 2.8792765140533447\n",
      "step 8\n",
      "training loss: 2.863865375518799\n",
      "step 9\n",
      "training loss: 2.8514885902404785\n",
      "step 10\n",
      "training loss: 2.8756802082061768\n",
      "validation loss: 2.8686649799346924\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.875244140625\n",
      "step 12\n",
      "training loss: 2.8672869205474854\n",
      "step 13\n",
      "training loss: 2.8641395568847656\n",
      "step 14\n",
      "training loss: 2.878525495529175\n",
      "step 15\n",
      "training loss: 2.883422613143921\n",
      "step 16\n",
      "training loss: 2.802917242050171\n",
      "step 17\n",
      "training loss: 2.845384120941162\n",
      "step 18\n",
      "training loss: 2.811002016067505\n",
      "step 19\n",
      "training loss: 2.847210168838501\n",
      "step 20\n",
      "training loss: 2.879692554473877\n",
      "validation loss: 2.8241121768951416\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8734467029571533\n",
      "step 22\n",
      "training loss: 2.8847572803497314\n",
      "step 23\n",
      "training loss: 2.866342067718506\n",
      "step 24\n",
      "training loss: 2.799745559692383\n",
      "step 25\n",
      "training loss: 2.7466208934783936\n",
      "step 26\n",
      "training loss: 2.8282127380371094\n",
      "step 27\n",
      "training loss: 2.8873229026794434\n",
      "step 28\n",
      "training loss: 2.8714816570281982\n",
      "step 29\n",
      "training loss: 2.8543496131896973\n",
      "step 30\n",
      "training loss: 2.860358953475952\n",
      "validation loss: 2.843210220336914\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.872380018234253\n",
      "step 32\n",
      "training loss: 2.87707257270813\n",
      "step 33\n",
      "training loss: 2.8760578632354736\n",
      "step 34\n",
      "training loss: 2.8639941215515137\n",
      "step 35\n",
      "training loss: 2.8823487758636475\n",
      "step 36\n",
      "training loss: 2.8796753883361816\n",
      "step 37\n",
      "training loss: 2.8347573280334473\n",
      "step 38\n",
      "training loss: 2.8437647819519043\n",
      "step 39\n",
      "training loss: 2.8569653034210205\n",
      "step 40\n",
      "training loss: 2.85891056060791\n",
      "validation loss: 2.8405063152313232\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8750698566436768\n",
      "step 42\n",
      "training loss: 2.869354248046875\n",
      "step 43\n",
      "training loss: 2.8772132396698\n",
      "step 44\n",
      "training loss: 2.875718593597412\n",
      "step 45\n",
      "training loss: 2.866722345352173\n",
      "step 46\n",
      "training loss: 2.8856358528137207\n",
      "step 47\n",
      "training loss: 2.8634085655212402\n",
      "step 48\n",
      "training loss: 2.8726797103881836\n",
      "step 49\n",
      "training loss: 2.881870746612549\n",
      "step 50\n",
      "training loss: 2.8754522800445557\n",
      "validation loss: 2.863487720489502\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8720433712005615\n",
      "step 52\n",
      "training loss: 2.874687433242798\n",
      "step 53\n",
      "training loss: 2.859837532043457\n",
      "step 54\n",
      "training loss: 2.8415966033935547\n",
      "step 55\n",
      "training loss: 2.8632705211639404\n",
      "step 56\n",
      "training loss: 2.874074935913086\n",
      "step 57\n",
      "training loss: 2.840684413909912\n",
      "step 58\n",
      "training loss: 2.859816789627075\n",
      "step 59\n",
      "training loss: 2.8606767654418945\n",
      "step 60\n",
      "training loss: 2.849775552749634\n",
      "validation loss: 2.8685858249664307\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8790881633758545\n",
      "step 62\n",
      "training loss: 2.8555102348327637\n",
      "step 63\n",
      "training loss: 2.869694709777832\n",
      "step 64\n",
      "training loss: 2.8541600704193115\n",
      "step 65\n",
      "training loss: 2.869948625564575\n",
      "step 66\n",
      "training loss: 2.8222126960754395\n",
      "step 67\n",
      "training loss: 2.8833274841308594\n",
      "step 68\n",
      "training loss: 2.84599232673645\n",
      "step 69\n",
      "training loss: 2.8787076473236084\n",
      "step 70\n",
      "training loss: 2.8648886680603027\n",
      "validation loss: 2.86205792427063\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.843984603881836\n",
      "step 72\n",
      "training loss: 2.84416127204895\n",
      "step 73\n",
      "training loss: 2.862835168838501\n",
      "step 74\n",
      "training loss: 2.8527724742889404\n",
      "step 75\n",
      "training loss: 2.8432328701019287\n",
      "step 76\n",
      "training loss: 2.879671335220337\n",
      "step 77\n",
      "training loss: 2.867008924484253\n",
      "step 78\n",
      "training loss: 2.857632637023926\n",
      "step 79\n",
      "training loss: 2.8399510383605957\n",
      "step 80\n",
      "training loss: 2.8573174476623535\n",
      "validation loss: 2.8561577796936035\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8583643436431885\n",
      "step 82\n",
      "training loss: 2.8327314853668213\n",
      "step 83\n",
      "training loss: 2.848834753036499\n",
      "step 84\n",
      "training loss: 2.8490207195281982\n",
      "step 85\n",
      "training loss: 2.843118906021118\n",
      "step 86\n",
      "training loss: 2.835855007171631\n",
      "step 87\n",
      "training loss: 2.869363784790039\n",
      "step 88\n",
      "training loss: 2.844222068786621\n",
      "step 89\n",
      "training loss: 2.8792195320129395\n",
      "step 90\n",
      "training loss: 2.8582189083099365\n",
      "validation loss: 2.8648157119750977\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8383843898773193\n",
      "step 92\n",
      "training loss: 2.8764357566833496\n",
      "step 93\n",
      "training loss: 2.849087715148926\n",
      "step 94\n",
      "training loss: 2.858161449432373\n",
      "step 95\n",
      "training loss: 2.8618061542510986\n",
      "step 96\n",
      "training loss: 2.830064535140991\n",
      "step 97\n",
      "training loss: 2.87457537651062\n",
      "step 98\n",
      "training loss: 2.8578855991363525\n",
      "step 99\n",
      "training loss: 2.8748269081115723\n",
      "step 100\n",
      "training loss: 2.870013952255249\n",
      "validation loss: 2.843181848526001\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.893946409225464\n",
      "step 102\n",
      "training loss: 2.8780832290649414\n",
      "step 103\n",
      "training loss: 2.8343024253845215\n",
      "step 104\n",
      "training loss: 2.8688180446624756\n",
      "step 105\n",
      "training loss: 2.857686996459961\n",
      "step 106\n",
      "training loss: 2.845890998840332\n",
      "step 107\n",
      "training loss: 2.85634708404541\n",
      "step 108\n",
      "training loss: 2.8581700325012207\n",
      "step 109\n",
      "training loss: 2.8762266635894775\n",
      "step 110\n",
      "training loss: 2.858036756515503\n",
      "validation loss: 2.8448805809020996\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8736932277679443\n",
      "step 112\n",
      "training loss: 2.875248432159424\n",
      "step 113\n",
      "training loss: 2.8773438930511475\n",
      "step 114\n",
      "training loss: 2.8549764156341553\n",
      "step 115\n",
      "training loss: 2.8656177520751953\n",
      "step 116\n",
      "training loss: 2.8614561557769775\n",
      "step 117\n",
      "training loss: 2.855297088623047\n",
      "step 118\n",
      "training loss: 2.8667356967926025\n",
      "step 119\n",
      "training loss: 2.841952323913574\n",
      "step 120\n",
      "training loss: 2.866215229034424\n",
      "validation loss: 2.833085060119629\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.872488498687744\n",
      "step 122\n",
      "training loss: 2.811812400817871\n",
      "step 123\n",
      "training loss: 2.853691339492798\n",
      "step 124\n",
      "training loss: 2.8496499061584473\n",
      "step 125\n",
      "training loss: 2.862610101699829\n",
      "step 126\n",
      "training loss: 2.872042179107666\n",
      "step 127\n",
      "training loss: 2.8456337451934814\n",
      "step 128\n",
      "training loss: 2.8640105724334717\n",
      "step 129\n",
      "training loss: 2.8123042583465576\n",
      "step 130\n",
      "training loss: 2.8513920307159424\n",
      "validation loss: 2.8428401947021484\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.875364303588867\n",
      "step 132\n",
      "training loss: 2.864359140396118\n",
      "step 133\n",
      "training loss: 2.8577704429626465\n",
      "step 134\n",
      "training loss: 2.8463616371154785\n",
      "step 135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.865776538848877\n",
      "step 136\n",
      "training loss: 2.8504104614257812\n",
      "step 137\n",
      "training loss: 2.8628244400024414\n",
      "step 138\n",
      "training loss: 2.8661468029022217\n",
      "step 139\n",
      "training loss: 2.860116720199585\n",
      "step 140\n",
      "training loss: 2.8585681915283203\n",
      "validation loss: 2.8699817657470703\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8591361045837402\n",
      "step 142\n",
      "training loss: 2.8346686363220215\n",
      "step 143\n",
      "training loss: 2.84079909324646\n",
      "step 144\n",
      "training loss: 2.831287145614624\n",
      "step 145\n",
      "training loss: 2.8650453090667725\n",
      "step 146\n",
      "training loss: 2.867943525314331\n",
      "step 147\n",
      "training loss: 2.8629343509674072\n",
      "step 148\n",
      "training loss: 2.8595118522644043\n",
      "step 149\n",
      "training loss: 2.8511977195739746\n",
      "step 150\n",
      "training loss: 2.870926856994629\n",
      "validation loss: 2.8738467693328857\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8890886306762695\n",
      "step 152\n",
      "training loss: 2.807220220565796\n",
      "step 153\n",
      "training loss: 2.8582725524902344\n",
      "step 154\n",
      "training loss: 2.8520147800445557\n",
      "step 155\n",
      "training loss: 2.8761439323425293\n",
      "step 156\n",
      "training loss: 2.8567163944244385\n",
      "step 157\n",
      "training loss: 2.877716541290283\n",
      "step 158\n",
      "training loss: 2.8514983654022217\n",
      "step 159\n",
      "training loss: 2.8406782150268555\n",
      "step 160\n",
      "training loss: 2.8555359840393066\n",
      "validation loss: 2.8308334350585938\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8654801845550537\n",
      "step 162\n",
      "training loss: 2.850595474243164\n",
      "step 163\n",
      "training loss: 2.8743581771850586\n",
      "step 164\n",
      "training loss: 2.857909917831421\n",
      "step 165\n",
      "training loss: 2.823453903198242\n",
      "step 166\n",
      "training loss: 2.852999210357666\n",
      "step 167\n",
      "training loss: 2.8538100719451904\n",
      "step 168\n",
      "training loss: 2.8555641174316406\n",
      "step 169\n",
      "training loss: 2.8722572326660156\n",
      "step 170\n",
      "training loss: 2.864943504333496\n",
      "validation loss: 2.8496358394622803\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.866353750228882\n",
      "step 172\n",
      "training loss: 2.8145980834960938\n",
      "step 173\n",
      "training loss: 2.8630616664886475\n",
      "step 174\n",
      "training loss: 2.854942560195923\n",
      "step 175\n",
      "training loss: 2.861976385116577\n",
      "step 176\n",
      "training loss: 2.8619391918182373\n",
      "step 177\n",
      "training loss: 2.873849630355835\n",
      "step 178\n",
      "training loss: 2.8692893981933594\n",
      "step 179\n",
      "training loss: 2.851815938949585\n",
      "step 180\n",
      "training loss: 2.8797686100006104\n",
      "validation loss: 2.8760900497436523\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8768627643585205\n",
      "step 182\n",
      "training loss: 2.864445686340332\n",
      "step 183\n",
      "training loss: 2.8601369857788086\n",
      "step 184\n",
      "training loss: 2.8678770065307617\n",
      "step 185\n",
      "training loss: 2.8584470748901367\n",
      "step 186\n",
      "training loss: 2.8572254180908203\n",
      "step 187\n",
      "training loss: 2.8385424613952637\n",
      "step 188\n",
      "training loss: 2.819032669067383\n",
      "step 189\n",
      "training loss: 2.8662290573120117\n",
      "step 190\n",
      "training loss: 2.847346067428589\n",
      "validation loss: 2.875305414199829\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8478996753692627\n",
      "step 192\n",
      "training loss: 2.860154151916504\n",
      "step 193\n",
      "training loss: 2.892683982849121\n",
      "step 194\n",
      "training loss: 2.8404622077941895\n",
      "step 195\n",
      "training loss: 2.871915340423584\n",
      "step 196\n",
      "training loss: 2.8625268936157227\n",
      "step 197\n",
      "training loss: 2.8692901134490967\n",
      "step 198\n",
      "training loss: 2.8467955589294434\n",
      "step 199\n",
      "training loss: 2.859372615814209\n",
      "step 200\n",
      "training loss: 2.836724281311035\n",
      "validation loss: 2.8665575981140137\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8494560718536377\n",
      "step 202\n",
      "training loss: 2.8562748432159424\n",
      "step 203\n",
      "training loss: 2.8439462184906006\n",
      "step 204\n",
      "training loss: 2.8831803798675537\n",
      "step 205\n",
      "training loss: 2.858682155609131\n",
      "step 206\n",
      "training loss: 2.8496286869049072\n",
      "step 207\n",
      "training loss: 2.8598313331604004\n",
      "step 208\n",
      "training loss: 2.864905595779419\n",
      "step 209\n",
      "training loss: 2.852494716644287\n",
      "step 210\n",
      "training loss: 2.8407399654388428\n",
      "validation loss: 2.9346888065338135\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8304073810577393\n",
      "step 212\n",
      "training loss: 2.813188314437866\n",
      "step 213\n",
      "training loss: 2.8767809867858887\n",
      "step 214\n",
      "training loss: 2.8534717559814453\n",
      "step 215\n",
      "training loss: 2.878377914428711\n",
      "step 216\n",
      "training loss: 2.853771924972534\n",
      "step 217\n",
      "training loss: 2.863293170928955\n",
      "step 218\n",
      "training loss: 2.880507230758667\n",
      "step 219\n",
      "training loss: 2.8567042350769043\n",
      "step 220\n",
      "training loss: 2.853682279586792\n",
      "validation loss: 2.8795721530914307\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8671929836273193\n",
      "step 222\n",
      "training loss: 2.8433337211608887\n",
      "step 223\n",
      "training loss: 2.8529655933380127\n",
      "step 224\n",
      "training loss: 2.845104932785034\n",
      "step 225\n",
      "training loss: 2.8490259647369385\n",
      "step 226\n",
      "training loss: 2.8387343883514404\n",
      "step 227\n",
      "training loss: 2.8577070236206055\n",
      "step 228\n",
      "training loss: 2.8637890815734863\n",
      "step 229\n",
      "training loss: 2.845978021621704\n",
      "step 230\n",
      "training loss: 2.874183177947998\n",
      "validation loss: 2.85191011428833\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.848161458969116\n",
      "step 232\n",
      "training loss: 2.8626279830932617\n",
      "step 233\n",
      "training loss: 2.86055064201355\n",
      "step 234\n",
      "training loss: 2.865913152694702\n",
      "step 235\n",
      "training loss: 2.788458824157715\n",
      "step 236\n",
      "training loss: 2.848438024520874\n",
      "step 237\n",
      "training loss: 2.833460569381714\n",
      "step 238\n",
      "training loss: 2.843384265899658\n",
      "step 239\n",
      "training loss: 2.8692266941070557\n",
      "step 240\n",
      "training loss: 2.8701915740966797\n",
      "validation loss: 2.859557867050171\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.862043619155884\n",
      "step 242\n",
      "training loss: 2.8613574504852295\n",
      "step 243\n",
      "training loss: 2.8596084117889404\n",
      "step 244\n",
      "training loss: 2.848555088043213\n",
      "step 245\n",
      "training loss: 2.8543736934661865\n",
      "step 246\n",
      "training loss: 2.8709616661071777\n",
      "step 247\n",
      "training loss: 2.885075569152832\n",
      "step 248\n",
      "training loss: 2.8556623458862305\n",
      "step 249\n",
      "training loss: 2.8453564643859863\n",
      "step 250\n",
      "training loss: 2.8488433361053467\n",
      "validation loss: 2.885439872741699\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8450608253479004\n",
      "step 252\n",
      "training loss: 2.8582873344421387\n",
      "step 253\n",
      "training loss: 2.858588218688965\n",
      "step 254\n",
      "training loss: 2.855433940887451\n",
      "step 255\n",
      "training loss: 2.8658478260040283\n",
      "step 256\n",
      "training loss: 2.8721694946289062\n",
      "step 257\n",
      "training loss: 2.8469319343566895\n",
      "step 258\n",
      "training loss: 2.8716633319854736\n",
      "step 259\n",
      "training loss: 2.8751513957977295\n",
      "step 260\n",
      "training loss: 2.8461220264434814\n",
      "validation loss: 2.826134443283081\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8479714393615723\n",
      "step 262\n",
      "training loss: 2.8702919483184814\n",
      "step 263\n",
      "training loss: 2.8662002086639404\n",
      "step 264\n",
      "training loss: 2.8618459701538086\n",
      "step 265\n",
      "training loss: 2.8646230697631836\n",
      "step 266\n",
      "training loss: 2.86735463142395\n",
      "step 267\n",
      "training loss: 2.855757713317871\n",
      "step 268\n",
      "training loss: 2.862504243850708\n",
      "step 269\n",
      "training loss: 2.8625476360321045\n",
      "step 270\n",
      "training loss: 2.860422372817993\n",
      "validation loss: 2.855541229248047\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8586466312408447\n",
      "----------3.0 min per epoch----------\n",
      "epoch 27\n",
      "step 0\n",
      "training loss: 2.8193917274475098\n",
      "validation loss: 2.862977981567383\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8470587730407715\n",
      "step 2\n",
      "training loss: 2.868691921234131\n",
      "step 3\n",
      "training loss: 2.846597671508789\n",
      "step 4\n",
      "training loss: 2.847780227661133\n",
      "step 5\n",
      "training loss: 2.8838083744049072\n",
      "step 6\n",
      "training loss: 2.860198497772217\n",
      "step 7\n",
      "training loss: 2.8785765171051025\n",
      "step 8\n",
      "training loss: 2.8802707195281982\n",
      "step 9\n",
      "training loss: 2.86428165435791\n",
      "step 10\n",
      "training loss: 2.8537209033966064\n",
      "validation loss: 2.8759377002716064\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8759396076202393\n",
      "step 12\n",
      "training loss: 2.8730485439300537\n",
      "step 13\n",
      "training loss: 2.8671460151672363\n",
      "step 14\n",
      "training loss: 2.862666368484497\n",
      "step 15\n",
      "training loss: 2.876784324645996\n",
      "step 16\n",
      "training loss: 2.8844990730285645\n",
      "step 17\n",
      "training loss: 2.8039743900299072\n",
      "step 18\n",
      "training loss: 2.844125270843506\n",
      "step 19\n",
      "training loss: 2.811211347579956\n",
      "step 20\n",
      "training loss: 2.848416566848755\n",
      "validation loss: 2.8794772624969482\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8792030811309814\n",
      "step 22\n",
      "training loss: 2.872840404510498\n",
      "step 23\n",
      "training loss: 2.881438970565796\n",
      "step 24\n",
      "training loss: 2.8636741638183594\n",
      "step 25\n",
      "training loss: 2.7916064262390137\n",
      "step 26\n",
      "training loss: 2.748296022415161\n",
      "step 27\n",
      "training loss: 2.820434808731079\n",
      "step 28\n",
      "training loss: 2.881591796875\n",
      "step 29\n",
      "training loss: 2.8741819858551025\n",
      "step 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8606910705566406\n",
      "validation loss: 2.8774619102478027\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.86112117767334\n",
      "step 32\n",
      "training loss: 2.8768908977508545\n",
      "step 33\n",
      "training loss: 2.8785479068756104\n",
      "step 34\n",
      "training loss: 2.880363702774048\n",
      "step 35\n",
      "training loss: 2.868253707885742\n",
      "step 36\n",
      "training loss: 2.880444288253784\n",
      "step 37\n",
      "training loss: 2.8818092346191406\n",
      "step 38\n",
      "training loss: 2.839642286300659\n",
      "step 39\n",
      "training loss: 2.855112075805664\n",
      "step 40\n",
      "training loss: 2.8583083152770996\n",
      "validation loss: 2.868551731109619\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.86562180519104\n",
      "step 42\n",
      "training loss: 2.876147985458374\n",
      "step 43\n",
      "training loss: 2.8743085861206055\n",
      "step 44\n",
      "training loss: 2.875600576400757\n",
      "step 45\n",
      "training loss: 2.8796305656433105\n",
      "step 46\n",
      "training loss: 2.86865234375\n",
      "step 47\n",
      "training loss: 2.885824680328369\n",
      "step 48\n",
      "training loss: 2.8641233444213867\n",
      "step 49\n",
      "training loss: 2.8744378089904785\n",
      "step 50\n",
      "training loss: 2.882054567337036\n",
      "validation loss: 2.8358163833618164\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8764939308166504\n",
      "step 52\n",
      "training loss: 2.875765323638916\n",
      "step 53\n",
      "training loss: 2.877676010131836\n",
      "step 54\n",
      "training loss: 2.8620922565460205\n",
      "step 55\n",
      "training loss: 2.8388383388519287\n",
      "step 56\n",
      "training loss: 2.860934257507324\n",
      "step 57\n",
      "training loss: 2.872420072555542\n",
      "step 58\n",
      "training loss: 2.844475746154785\n",
      "step 59\n",
      "training loss: 2.8603439331054688\n",
      "step 60\n",
      "training loss: 2.8618392944335938\n",
      "validation loss: 2.8337244987487793\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.851156711578369\n",
      "step 62\n",
      "training loss: 2.8802151679992676\n",
      "step 63\n",
      "training loss: 2.854684591293335\n",
      "step 64\n",
      "training loss: 2.871619939804077\n",
      "step 65\n",
      "training loss: 2.8524529933929443\n",
      "step 66\n",
      "training loss: 2.871263265609741\n",
      "step 67\n",
      "training loss: 2.8220016956329346\n",
      "step 68\n",
      "training loss: 2.882194757461548\n",
      "step 69\n",
      "training loss: 2.8459770679473877\n",
      "step 70\n",
      "training loss: 2.8804562091827393\n",
      "validation loss: 2.8330559730529785\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.863983154296875\n",
      "step 72\n",
      "training loss: 2.845353364944458\n",
      "step 73\n",
      "training loss: 2.8427205085754395\n",
      "step 74\n",
      "training loss: 2.862231731414795\n",
      "step 75\n",
      "training loss: 2.8516740798950195\n",
      "step 76\n",
      "training loss: 2.840636968612671\n",
      "step 77\n",
      "training loss: 2.8794121742248535\n",
      "step 78\n",
      "training loss: 2.867994546890259\n",
      "step 79\n",
      "training loss: 2.85933256149292\n",
      "step 80\n",
      "training loss: 2.841679096221924\n",
      "validation loss: 2.8599116802215576\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8563196659088135\n",
      "step 82\n",
      "training loss: 2.8567135334014893\n",
      "step 83\n",
      "training loss: 2.8339648246765137\n",
      "step 84\n",
      "training loss: 2.8480818271636963\n",
      "step 85\n",
      "training loss: 2.8494627475738525\n",
      "step 86\n",
      "training loss: 2.8423163890838623\n",
      "step 87\n",
      "training loss: 2.8346776962280273\n",
      "step 88\n",
      "training loss: 2.8713080883026123\n",
      "step 89\n",
      "training loss: 2.841754913330078\n",
      "step 90\n",
      "training loss: 2.877403736114502\n",
      "validation loss: 2.869647741317749\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8605539798736572\n",
      "step 92\n",
      "training loss: 2.8395578861236572\n",
      "step 93\n",
      "training loss: 2.876547336578369\n",
      "step 94\n",
      "training loss: 2.8489747047424316\n",
      "step 95\n",
      "training loss: 2.8599536418914795\n",
      "step 96\n",
      "training loss: 2.8622119426727295\n",
      "step 97\n",
      "training loss: 2.8299527168273926\n",
      "step 98\n",
      "training loss: 2.8772456645965576\n",
      "step 99\n",
      "training loss: 2.855095386505127\n",
      "step 100\n",
      "training loss: 2.875664472579956\n",
      "validation loss: 2.860224485397339\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8700129985809326\n",
      "step 102\n",
      "training loss: 2.8940253257751465\n",
      "step 103\n",
      "training loss: 2.877962112426758\n",
      "step 104\n",
      "training loss: 2.8325350284576416\n",
      "step 105\n",
      "training loss: 2.8699281215667725\n",
      "step 106\n",
      "training loss: 2.8556618690490723\n",
      "step 107\n",
      "training loss: 2.8452045917510986\n",
      "step 108\n",
      "training loss: 2.8557026386260986\n",
      "step 109\n",
      "training loss: 2.8561642169952393\n",
      "step 110\n",
      "training loss: 2.876359224319458\n",
      "validation loss: 2.8566946983337402\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.859255075454712\n",
      "step 112\n",
      "training loss: 2.875152826309204\n",
      "step 113\n",
      "training loss: 2.8737666606903076\n",
      "step 114\n",
      "training loss: 2.8767099380493164\n",
      "step 115\n",
      "training loss: 2.8529396057128906\n",
      "step 116\n",
      "training loss: 2.8687984943389893\n",
      "step 117\n",
      "training loss: 2.8604393005371094\n",
      "step 118\n",
      "training loss: 2.8523097038269043\n",
      "step 119\n",
      "training loss: 2.866828203201294\n",
      "step 120\n",
      "training loss: 2.842194080352783\n",
      "validation loss: 2.862940788269043\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.866509437561035\n",
      "step 122\n",
      "training loss: 2.8722851276397705\n",
      "step 123\n",
      "training loss: 2.8124537467956543\n",
      "step 124\n",
      "training loss: 2.853313684463501\n",
      "step 125\n",
      "training loss: 2.848094940185547\n",
      "step 126\n",
      "training loss: 2.862187385559082\n",
      "step 127\n",
      "training loss: 2.8712124824523926\n",
      "step 128\n",
      "training loss: 2.842724561691284\n",
      "step 129\n",
      "training loss: 2.864018440246582\n",
      "step 130\n",
      "training loss: 2.812999963760376\n",
      "validation loss: 2.841939687728882\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.848855972290039\n",
      "step 132\n",
      "training loss: 2.8745899200439453\n",
      "step 133\n",
      "training loss: 2.863520383834839\n",
      "step 134\n",
      "training loss: 2.857001543045044\n",
      "step 135\n",
      "training loss: 2.846665620803833\n",
      "step 136\n",
      "training loss: 2.8664135932922363\n",
      "step 137\n",
      "training loss: 2.851313829421997\n",
      "step 138\n",
      "training loss: 2.861280918121338\n",
      "step 139\n",
      "training loss: 2.8659281730651855\n",
      "step 140\n",
      "training loss: 2.857720375061035\n",
      "validation loss: 2.844696521759033\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.856049060821533\n",
      "step 142\n",
      "training loss: 2.8594768047332764\n",
      "step 143\n",
      "training loss: 2.8331329822540283\n",
      "step 144\n",
      "training loss: 2.840864419937134\n",
      "step 145\n",
      "training loss: 2.8305883407592773\n",
      "step 146\n",
      "training loss: 2.869218587875366\n",
      "step 147\n",
      "training loss: 2.865541696548462\n",
      "step 148\n",
      "training loss: 2.860069990158081\n",
      "step 149\n",
      "training loss: 2.858250856399536\n",
      "step 150\n",
      "training loss: 2.8525712490081787\n",
      "validation loss: 2.8304100036621094\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8685991764068604\n",
      "step 152\n",
      "training loss: 2.8888776302337646\n",
      "step 153\n",
      "training loss: 2.807771921157837\n",
      "step 154\n",
      "training loss: 2.858262538909912\n",
      "step 155\n",
      "training loss: 2.8517239093780518\n",
      "step 156\n",
      "training loss: 2.8764944076538086\n",
      "step 157\n",
      "training loss: 2.8583807945251465\n",
      "step 158\n",
      "training loss: 2.877659320831299\n",
      "step 159\n",
      "training loss: 2.8486149311065674\n",
      "step 160\n",
      "training loss: 2.841416120529175\n",
      "validation loss: 2.839798927307129\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8524343967437744\n",
      "step 162\n",
      "training loss: 2.8650379180908203\n",
      "step 163\n",
      "training loss: 2.850924491882324\n",
      "step 164\n",
      "training loss: 2.874325752258301\n",
      "step 165\n",
      "training loss: 2.8601202964782715\n",
      "step 166\n",
      "training loss: 2.8234121799468994\n",
      "step 167\n",
      "training loss: 2.8499982357025146\n",
      "step 168\n",
      "training loss: 2.8533718585968018\n",
      "step 169\n",
      "training loss: 2.854295492172241\n",
      "step 170\n",
      "training loss: 2.8698768615722656\n",
      "validation loss: 2.870227336883545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8638663291931152\n",
      "step 172\n",
      "training loss: 2.8662867546081543\n",
      "step 173\n",
      "training loss: 2.8164501190185547\n",
      "step 174\n",
      "training loss: 2.863440990447998\n",
      "step 175\n",
      "training loss: 2.857783317565918\n",
      "step 176\n",
      "training loss: 2.8615942001342773\n",
      "step 177\n",
      "training loss: 2.8620011806488037\n",
      "step 178\n",
      "training loss: 2.8744375705718994\n",
      "step 179\n",
      "training loss: 2.8676576614379883\n",
      "step 180\n",
      "training loss: 2.8526229858398438\n",
      "validation loss: 2.8720998764038086\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8776438236236572\n",
      "step 182\n",
      "training loss: 2.8772616386413574\n",
      "step 183\n",
      "training loss: 2.866004467010498\n",
      "step 184\n",
      "training loss: 2.8589413166046143\n",
      "step 185\n",
      "training loss: 2.867387056350708\n",
      "step 186\n",
      "training loss: 2.857599973678589\n",
      "step 187\n",
      "training loss: 2.8542861938476562\n",
      "step 188\n",
      "training loss: 2.8396811485290527\n",
      "step 189\n",
      "training loss: 2.8193740844726562\n",
      "step 190\n",
      "training loss: 2.8678627014160156\n",
      "validation loss: 2.8282809257507324\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.847339391708374\n",
      "step 192\n",
      "training loss: 2.8484206199645996\n",
      "step 193\n",
      "training loss: 2.8615522384643555\n",
      "step 194\n",
      "training loss: 2.891761302947998\n",
      "step 195\n",
      "training loss: 2.841327667236328\n",
      "step 196\n",
      "training loss: 2.8710837364196777\n",
      "step 197\n",
      "training loss: 2.8617427349090576\n",
      "step 198\n",
      "training loss: 2.869235038757324\n",
      "step 199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.847015619277954\n",
      "step 200\n",
      "training loss: 2.8586673736572266\n",
      "validation loss: 2.852379560470581\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.834150791168213\n",
      "step 202\n",
      "training loss: 2.848288059234619\n",
      "step 203\n",
      "training loss: 2.859905958175659\n",
      "step 204\n",
      "training loss: 2.8434441089630127\n",
      "step 205\n",
      "training loss: 2.882152557373047\n",
      "step 206\n",
      "training loss: 2.8582897186279297\n",
      "step 207\n",
      "training loss: 2.8477768898010254\n",
      "step 208\n",
      "training loss: 2.859421491622925\n",
      "step 209\n",
      "training loss: 2.8673548698425293\n",
      "step 210\n",
      "training loss: 2.8517470359802246\n",
      "validation loss: 2.8771228790283203\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.840052843093872\n",
      "step 212\n",
      "training loss: 2.829554796218872\n",
      "step 213\n",
      "training loss: 2.809110641479492\n",
      "step 214\n",
      "training loss: 2.8734567165374756\n",
      "step 215\n",
      "training loss: 2.85198974609375\n",
      "step 216\n",
      "training loss: 2.876279830932617\n",
      "step 217\n",
      "training loss: 2.852438449859619\n",
      "step 218\n",
      "training loss: 2.86291766166687\n",
      "step 219\n",
      "training loss: 2.8798794746398926\n",
      "step 220\n",
      "training loss: 2.8572065830230713\n",
      "validation loss: 2.874828815460205\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8515326976776123\n",
      "step 222\n",
      "training loss: 2.8649260997772217\n",
      "step 223\n",
      "training loss: 2.842864990234375\n",
      "step 224\n",
      "training loss: 2.8544161319732666\n",
      "step 225\n",
      "training loss: 2.843491315841675\n",
      "step 226\n",
      "training loss: 2.8469269275665283\n",
      "step 227\n",
      "training loss: 2.837305784225464\n",
      "step 228\n",
      "training loss: 2.8567121028900146\n",
      "step 229\n",
      "training loss: 2.859811305999756\n",
      "step 230\n",
      "training loss: 2.844515800476074\n",
      "validation loss: 2.8649725914001465\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.873809576034546\n",
      "step 232\n",
      "training loss: 2.8490238189697266\n",
      "step 233\n",
      "training loss: 2.8618991374969482\n",
      "step 234\n",
      "training loss: 2.8514211177825928\n",
      "step 235\n",
      "training loss: 2.8653347492218018\n",
      "step 236\n",
      "training loss: 2.7890071868896484\n",
      "step 237\n",
      "training loss: 2.8493332862854004\n",
      "step 238\n",
      "training loss: 2.8326618671417236\n",
      "step 239\n",
      "training loss: 2.8440043926239014\n",
      "step 240\n",
      "training loss: 2.8667807579040527\n",
      "validation loss: 2.937025308609009\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8701393604278564\n",
      "step 242\n",
      "training loss: 2.8609442710876465\n",
      "step 243\n",
      "training loss: 2.862518787384033\n",
      "step 244\n",
      "training loss: 2.8589072227478027\n",
      "step 245\n",
      "training loss: 2.8454132080078125\n",
      "step 246\n",
      "training loss: 2.8551952838897705\n",
      "step 247\n",
      "training loss: 2.869494676589966\n",
      "step 248\n",
      "training loss: 2.883814573287964\n",
      "step 249\n",
      "training loss: 2.853792190551758\n",
      "step 250\n",
      "training loss: 2.847167491912842\n",
      "validation loss: 2.8750200271606445\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8466596603393555\n",
      "step 252\n",
      "training loss: 2.8446860313415527\n",
      "step 253\n",
      "training loss: 2.8598954677581787\n",
      "step 254\n",
      "training loss: 2.861079692840576\n",
      "step 255\n",
      "training loss: 2.8539421558380127\n",
      "step 256\n",
      "training loss: 2.864819049835205\n",
      "step 257\n",
      "training loss: 2.871558904647827\n",
      "step 258\n",
      "training loss: 2.84792423248291\n",
      "step 259\n",
      "training loss: 2.8724472522735596\n",
      "step 260\n",
      "training loss: 2.8763906955718994\n",
      "validation loss: 2.8398404121398926\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.845149278640747\n",
      "step 262\n",
      "training loss: 2.847907066345215\n",
      "step 263\n",
      "training loss: 2.8719356060028076\n",
      "step 264\n",
      "training loss: 2.8634016513824463\n",
      "step 265\n",
      "training loss: 2.862294912338257\n",
      "step 266\n",
      "training loss: 2.864447832107544\n",
      "step 267\n",
      "training loss: 2.866382598876953\n",
      "step 268\n",
      "training loss: 2.8562636375427246\n",
      "step 269\n",
      "training loss: 2.8632543087005615\n",
      "step 270\n",
      "training loss: 2.861386775970459\n",
      "validation loss: 2.855271100997925\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8575313091278076\n",
      "----------3.0 min per epoch----------\n",
      "epoch 28\n",
      "step 0\n",
      "training loss: 2.8568015098571777\n",
      "validation loss: 2.8932700157165527\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8203601837158203\n",
      "step 2\n",
      "training loss: 2.84977126121521\n",
      "step 3\n",
      "training loss: 2.866100311279297\n",
      "step 4\n",
      "training loss: 2.8435356616973877\n",
      "step 5\n",
      "training loss: 2.8474080562591553\n",
      "step 6\n",
      "training loss: 2.883787155151367\n",
      "step 7\n",
      "training loss: 2.8595893383026123\n",
      "step 8\n",
      "training loss: 2.8761706352233887\n",
      "step 9\n",
      "training loss: 2.8775200843811035\n",
      "step 10\n",
      "training loss: 2.8656272888183594\n",
      "validation loss: 2.8198928833007812\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8501739501953125\n",
      "step 12\n",
      "training loss: 2.8747012615203857\n",
      "step 13\n",
      "training loss: 2.873147487640381\n",
      "step 14\n",
      "training loss: 2.868318557739258\n",
      "step 15\n",
      "training loss: 2.865161657333374\n",
      "step 16\n",
      "training loss: 2.87492299079895\n",
      "step 17\n",
      "training loss: 2.886702060699463\n",
      "step 18\n",
      "training loss: 2.802135944366455\n",
      "step 19\n",
      "training loss: 2.8441216945648193\n",
      "step 20\n",
      "training loss: 2.803222894668579\n",
      "validation loss: 2.8528828620910645\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8457350730895996\n",
      "step 22\n",
      "training loss: 2.879153251647949\n",
      "step 23\n",
      "training loss: 2.86731219291687\n",
      "step 24\n",
      "training loss: 2.884895086288452\n",
      "step 25\n",
      "training loss: 2.8643290996551514\n",
      "step 26\n",
      "training loss: 2.7809360027313232\n",
      "step 27\n",
      "training loss: 2.7381479740142822\n",
      "step 28\n",
      "training loss: 2.815661907196045\n",
      "step 29\n",
      "training loss: 2.886401891708374\n",
      "step 30\n",
      "training loss: 2.874000310897827\n",
      "validation loss: 2.8764302730560303\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8576035499572754\n",
      "step 32\n",
      "training loss: 2.8640637397766113\n",
      "step 33\n",
      "training loss: 2.877490758895874\n",
      "step 34\n",
      "training loss: 2.8763301372528076\n",
      "step 35\n",
      "training loss: 2.8776047229766846\n",
      "step 36\n",
      "training loss: 2.8626163005828857\n",
      "step 37\n",
      "training loss: 2.8793559074401855\n",
      "step 38\n",
      "training loss: 2.8792331218719482\n",
      "step 39\n",
      "training loss: 2.8351328372955322\n",
      "step 40\n",
      "training loss: 2.844792366027832\n",
      "validation loss: 2.8833999633789062\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8564035892486572\n",
      "step 42\n",
      "training loss: 2.8599061965942383\n",
      "step 43\n",
      "training loss: 2.8724992275238037\n",
      "step 44\n",
      "training loss: 2.86777663230896\n",
      "step 45\n",
      "training loss: 2.873485803604126\n",
      "step 46\n",
      "training loss: 2.8764045238494873\n",
      "step 47\n",
      "training loss: 2.8667497634887695\n",
      "step 48\n",
      "training loss: 2.883798360824585\n",
      "step 49\n",
      "training loss: 2.8619730472564697\n",
      "step 50\n",
      "training loss: 2.8708813190460205\n",
      "validation loss: 2.9016315937042236\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.881932258605957\n",
      "step 52\n",
      "training loss: 2.8766062259674072\n",
      "step 53\n",
      "training loss: 2.8735480308532715\n",
      "step 54\n",
      "training loss: 2.873617649078369\n",
      "step 55\n",
      "training loss: 2.859562635421753\n",
      "step 56\n",
      "training loss: 2.8393843173980713\n",
      "step 57\n",
      "training loss: 2.8633148670196533\n",
      "step 58\n",
      "training loss: 2.8733115196228027\n",
      "step 59\n",
      "training loss: 2.845139503479004\n",
      "step 60\n",
      "training loss: 2.8591198921203613\n",
      "validation loss: 2.8676960468292236\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.860344648361206\n",
      "step 62\n",
      "training loss: 2.8507583141326904\n",
      "step 63\n",
      "training loss: 2.882472038269043\n",
      "step 64\n",
      "training loss: 2.852886199951172\n",
      "step 65\n",
      "training loss: 2.8693289756774902\n",
      "step 66\n",
      "training loss: 2.8541195392608643\n",
      "step 67\n",
      "training loss: 2.868107795715332\n",
      "step 68\n",
      "training loss: 2.823441505432129\n",
      "step 69\n",
      "training loss: 2.8821938037872314\n",
      "step 70\n",
      "training loss: 2.848133087158203\n",
      "validation loss: 2.8634703159332275\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8813767433166504\n",
      "step 72\n",
      "training loss: 2.8652560710906982\n",
      "step 73\n",
      "training loss: 2.8471176624298096\n",
      "step 74\n",
      "training loss: 2.842787742614746\n",
      "step 75\n",
      "training loss: 2.8618314266204834\n",
      "step 76\n",
      "training loss: 2.852966547012329\n",
      "step 77\n",
      "training loss: 2.845433473587036\n",
      "step 78\n",
      "training loss: 2.8771443367004395\n",
      "step 79\n",
      "training loss: 2.867424488067627\n",
      "step 80\n",
      "training loss: 2.8611960411071777\n",
      "validation loss: 2.822643995285034\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8435192108154297\n",
      "step 82\n",
      "training loss: 2.857172727584839\n",
      "step 83\n",
      "training loss: 2.8575806617736816\n",
      "step 84\n",
      "training loss: 2.8336267471313477\n",
      "step 85\n",
      "training loss: 2.8492531776428223\n",
      "step 86\n",
      "training loss: 2.8513731956481934\n",
      "step 87\n",
      "training loss: 2.8426384925842285\n",
      "step 88\n",
      "training loss: 2.836765766143799\n",
      "step 89\n",
      "training loss: 2.868560314178467\n",
      "step 90\n",
      "training loss: 2.8413379192352295\n",
      "validation loss: 2.8287227153778076\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.876610517501831\n",
      "step 92\n",
      "training loss: 2.8591504096984863\n",
      "step 93\n",
      "training loss: 2.8374650478363037\n",
      "step 94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8749818801879883\n",
      "step 95\n",
      "training loss: 2.8478431701660156\n",
      "step 96\n",
      "training loss: 2.8568079471588135\n",
      "step 97\n",
      "training loss: 2.8619937896728516\n",
      "step 98\n",
      "training loss: 2.832536220550537\n",
      "step 99\n",
      "training loss: 2.875316858291626\n",
      "step 100\n",
      "training loss: 2.8539843559265137\n",
      "validation loss: 2.832261562347412\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.875047445297241\n",
      "step 102\n",
      "training loss: 2.870121717453003\n",
      "step 103\n",
      "training loss: 2.8932344913482666\n",
      "step 104\n",
      "training loss: 2.8777687549591064\n",
      "step 105\n",
      "training loss: 2.833036422729492\n",
      "step 106\n",
      "training loss: 2.874776601791382\n",
      "step 107\n",
      "training loss: 2.8582401275634766\n",
      "step 108\n",
      "training loss: 2.846431016921997\n",
      "step 109\n",
      "training loss: 2.8551125526428223\n",
      "step 110\n",
      "training loss: 2.8552610874176025\n",
      "validation loss: 2.856982469558716\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8772878646850586\n",
      "step 112\n",
      "training loss: 2.8612396717071533\n",
      "step 113\n",
      "training loss: 2.8764121532440186\n",
      "step 114\n",
      "training loss: 2.8747756481170654\n",
      "step 115\n",
      "training loss: 2.8782615661621094\n",
      "step 116\n",
      "training loss: 2.8529393672943115\n",
      "step 117\n",
      "training loss: 2.8689348697662354\n",
      "step 118\n",
      "training loss: 2.860200881958008\n",
      "step 119\n",
      "training loss: 2.8536267280578613\n",
      "step 120\n",
      "training loss: 2.866759777069092\n",
      "validation loss: 2.870056390762329\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8404383659362793\n",
      "step 122\n",
      "training loss: 2.867220163345337\n",
      "step 123\n",
      "training loss: 2.8736636638641357\n",
      "step 124\n",
      "training loss: 2.8129448890686035\n",
      "step 125\n",
      "training loss: 2.8550050258636475\n",
      "step 126\n",
      "training loss: 2.85115122795105\n",
      "step 127\n",
      "training loss: 2.8626458644866943\n",
      "step 128\n",
      "training loss: 2.8723185062408447\n",
      "step 129\n",
      "training loss: 2.8440709114074707\n",
      "step 130\n",
      "training loss: 2.8657846450805664\n",
      "validation loss: 2.8632259368896484\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8140807151794434\n",
      "step 132\n",
      "training loss: 2.8512821197509766\n",
      "step 133\n",
      "training loss: 2.8735125064849854\n",
      "step 134\n",
      "training loss: 2.863276481628418\n",
      "step 135\n",
      "training loss: 2.857034683227539\n",
      "step 136\n",
      "training loss: 2.8468785285949707\n",
      "step 137\n",
      "training loss: 2.8661322593688965\n",
      "step 138\n",
      "training loss: 2.851458787918091\n",
      "step 139\n",
      "training loss: 2.861633539199829\n",
      "step 140\n",
      "training loss: 2.8636324405670166\n",
      "validation loss: 2.856311798095703\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8606996536254883\n",
      "step 142\n",
      "training loss: 2.8569586277008057\n",
      "step 143\n",
      "training loss: 2.858943462371826\n",
      "step 144\n",
      "training loss: 2.835625648498535\n",
      "step 145\n",
      "training loss: 2.840526580810547\n",
      "step 146\n",
      "training loss: 2.832604169845581\n",
      "step 147\n",
      "training loss: 2.866246461868286\n",
      "step 148\n",
      "training loss: 2.868053436279297\n",
      "step 149\n",
      "training loss: 2.8599698543548584\n",
      "step 150\n",
      "training loss: 2.8604226112365723\n",
      "validation loss: 2.8637049198150635\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.852396249771118\n",
      "step 152\n",
      "training loss: 2.869842767715454\n",
      "step 153\n",
      "training loss: 2.888679265975952\n",
      "step 154\n",
      "training loss: 2.8075599670410156\n",
      "step 155\n",
      "training loss: 2.8563592433929443\n",
      "step 156\n",
      "training loss: 2.8520007133483887\n",
      "step 157\n",
      "training loss: 2.8739304542541504\n",
      "step 158\n",
      "training loss: 2.8564467430114746\n",
      "step 159\n",
      "training loss: 2.8765525817871094\n",
      "step 160\n",
      "training loss: 2.8513641357421875\n",
      "validation loss: 2.844320297241211\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8407208919525146\n",
      "step 162\n",
      "training loss: 2.8542048931121826\n",
      "step 163\n",
      "training loss: 2.8637287616729736\n",
      "step 164\n",
      "training loss: 2.8505311012268066\n",
      "step 165\n",
      "training loss: 2.8731014728546143\n",
      "step 166\n",
      "training loss: 2.859924793243408\n",
      "step 167\n",
      "training loss: 2.822970390319824\n",
      "step 168\n",
      "training loss: 2.849846363067627\n",
      "step 169\n",
      "training loss: 2.8529891967773438\n",
      "step 170\n",
      "training loss: 2.853760004043579\n",
      "validation loss: 2.849001407623291\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.869748592376709\n",
      "step 172\n",
      "training loss: 2.8634281158447266\n",
      "step 173\n",
      "training loss: 2.865346670150757\n",
      "step 174\n",
      "training loss: 2.8140769004821777\n",
      "step 175\n",
      "training loss: 2.861807346343994\n",
      "step 176\n",
      "training loss: 2.8572943210601807\n",
      "step 177\n",
      "training loss: 2.862546920776367\n",
      "step 178\n",
      "training loss: 2.861494541168213\n",
      "step 179\n",
      "training loss: 2.8732593059539795\n",
      "step 180\n",
      "training loss: 2.866425037384033\n",
      "validation loss: 2.832960844039917\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.851346254348755\n",
      "step 182\n",
      "training loss: 2.8781967163085938\n",
      "step 183\n",
      "training loss: 2.876616954803467\n",
      "step 184\n",
      "training loss: 2.8634331226348877\n",
      "step 185\n",
      "training loss: 2.8606674671173096\n",
      "step 186\n",
      "training loss: 2.8665807247161865\n",
      "step 187\n",
      "training loss: 2.8593838214874268\n",
      "step 188\n",
      "training loss: 2.8587751388549805\n",
      "step 189\n",
      "training loss: 2.838383913040161\n",
      "step 190\n",
      "training loss: 2.818437099456787\n",
      "validation loss: 2.8363454341888428\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8696961402893066\n",
      "step 192\n",
      "training loss: 2.847842216491699\n",
      "step 193\n",
      "training loss: 2.848208427429199\n",
      "step 194\n",
      "training loss: 2.8627095222473145\n",
      "step 195\n",
      "training loss: 2.889568328857422\n",
      "step 196\n",
      "training loss: 2.8432021141052246\n",
      "step 197\n",
      "training loss: 2.8698883056640625\n",
      "step 198\n",
      "training loss: 2.8632659912109375\n",
      "step 199\n",
      "training loss: 2.8686511516571045\n",
      "step 200\n",
      "training loss: 2.8464596271514893\n",
      "validation loss: 2.868990659713745\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8579530715942383\n",
      "step 202\n",
      "training loss: 2.8339791297912598\n",
      "step 203\n",
      "training loss: 2.8506240844726562\n",
      "step 204\n",
      "training loss: 2.8554720878601074\n",
      "step 205\n",
      "training loss: 2.843935966491699\n",
      "step 206\n",
      "training loss: 2.883697986602783\n",
      "step 207\n",
      "training loss: 2.8578643798828125\n",
      "step 208\n",
      "training loss: 2.846564292907715\n",
      "step 209\n",
      "training loss: 2.8614540100097656\n",
      "step 210\n",
      "training loss: 2.866511344909668\n",
      "validation loss: 2.8728859424591064\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8518834114074707\n",
      "step 212\n",
      "training loss: 2.8437349796295166\n",
      "step 213\n",
      "training loss: 2.8274447917938232\n",
      "step 214\n",
      "training loss: 2.8089146614074707\n",
      "step 215\n",
      "training loss: 2.875373363494873\n",
      "step 216\n",
      "training loss: 2.8512210845947266\n",
      "step 217\n",
      "training loss: 2.8772048950195312\n",
      "step 218\n",
      "training loss: 2.8525824546813965\n",
      "step 219\n",
      "training loss: 2.8590750694274902\n",
      "step 220\n",
      "training loss: 2.880647897720337\n",
      "validation loss: 2.8263282775878906\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.856337785720825\n",
      "step 222\n",
      "training loss: 2.85068678855896\n",
      "step 223\n",
      "training loss: 2.8643229007720947\n",
      "step 224\n",
      "training loss: 2.843639373779297\n",
      "step 225\n",
      "training loss: 2.8520851135253906\n",
      "step 226\n",
      "training loss: 2.8442776203155518\n",
      "step 227\n",
      "training loss: 2.849113702774048\n",
      "step 228\n",
      "training loss: 2.8385963439941406\n",
      "step 229\n",
      "training loss: 2.856492280960083\n",
      "step 230\n",
      "training loss: 2.8596513271331787\n",
      "validation loss: 2.851915121078491\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.844892978668213\n",
      "step 232\n",
      "training loss: 2.872434616088867\n",
      "step 233\n",
      "training loss: 2.8505842685699463\n",
      "step 234\n",
      "training loss: 2.8621985912323\n",
      "step 235\n",
      "training loss: 2.8535258769989014\n",
      "step 236\n",
      "training loss: 2.863812208175659\n",
      "step 237\n",
      "training loss: 2.790273904800415\n",
      "step 238\n",
      "training loss: 2.847353219985962\n",
      "step 239\n",
      "training loss: 2.829810619354248\n",
      "step 240\n",
      "training loss: 2.8461289405822754\n",
      "validation loss: 2.876805067062378\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8682329654693604\n",
      "step 242\n",
      "training loss: 2.869302988052368\n",
      "step 243\n",
      "training loss: 2.8633437156677246\n",
      "step 244\n",
      "training loss: 2.861361026763916\n",
      "step 245\n",
      "training loss: 2.8601975440979004\n",
      "step 246\n",
      "training loss: 2.8473291397094727\n",
      "step 247\n",
      "training loss: 2.8557872772216797\n",
      "step 248\n",
      "training loss: 2.8726232051849365\n",
      "step 249\n",
      "training loss: 2.8848485946655273\n",
      "step 250\n",
      "training loss: 2.855642080307007\n",
      "validation loss: 2.868560791015625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8453853130340576\n",
      "step 252\n",
      "training loss: 2.8486645221710205\n",
      "step 253\n",
      "training loss: 2.847766160964966\n",
      "step 254\n",
      "training loss: 2.858536958694458\n",
      "step 255\n",
      "training loss: 2.8585124015808105\n",
      "step 256\n",
      "training loss: 2.8530266284942627\n",
      "step 257\n",
      "training loss: 2.8650989532470703\n",
      "step 258\n",
      "training loss: 2.8699169158935547\n",
      "step 259\n",
      "training loss: 2.847116708755493\n",
      "step 260\n",
      "training loss: 2.872654914855957\n",
      "validation loss: 2.8656280040740967\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 261\n",
      "training loss: 2.877460479736328\n",
      "step 262\n",
      "training loss: 2.841899871826172\n",
      "step 263\n",
      "training loss: 2.844161033630371\n",
      "step 264\n",
      "training loss: 2.8694303035736084\n",
      "step 265\n",
      "training loss: 2.8648974895477295\n",
      "step 266\n",
      "training loss: 2.8626363277435303\n",
      "step 267\n",
      "training loss: 2.8640193939208984\n",
      "step 268\n",
      "training loss: 2.865780830383301\n",
      "step 269\n",
      "training loss: 2.8551862239837646\n",
      "step 270\n",
      "training loss: 2.8643808364868164\n",
      "validation loss: 2.9321935176849365\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8613359928131104\n",
      "----------3.0 min per epoch----------\n",
      "epoch 29\n",
      "step 0\n",
      "training loss: 2.857590913772583\n",
      "validation loss: 2.875819206237793\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.853428363800049\n",
      "step 2\n",
      "training loss: 2.8186938762664795\n",
      "step 3\n",
      "training loss: 2.849607467651367\n",
      "step 4\n",
      "training loss: 2.8710107803344727\n",
      "step 5\n",
      "training loss: 2.84588623046875\n",
      "step 6\n",
      "training loss: 2.847604513168335\n",
      "step 7\n",
      "training loss: 2.886948823928833\n",
      "step 8\n",
      "training loss: 2.8625130653381348\n",
      "step 9\n",
      "training loss: 2.877891778945923\n",
      "step 10\n",
      "training loss: 2.879143714904785\n",
      "validation loss: 2.8418538570404053\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8659675121307373\n",
      "step 12\n",
      "training loss: 2.8462774753570557\n",
      "step 13\n",
      "training loss: 2.87347674369812\n",
      "step 14\n",
      "training loss: 2.874664306640625\n",
      "step 15\n",
      "training loss: 2.8669345378875732\n",
      "step 16\n",
      "training loss: 2.864680051803589\n",
      "step 17\n",
      "training loss: 2.876131772994995\n",
      "step 18\n",
      "training loss: 2.885236978530884\n",
      "step 19\n",
      "training loss: 2.801626443862915\n",
      "step 20\n",
      "training loss: 2.848501682281494\n",
      "validation loss: 2.858928918838501\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.802058219909668\n",
      "step 22\n",
      "training loss: 2.843294143676758\n",
      "step 23\n",
      "training loss: 2.8796842098236084\n",
      "step 24\n",
      "training loss: 2.8690595626831055\n",
      "step 25\n",
      "training loss: 2.882070779800415\n",
      "step 26\n",
      "training loss: 2.866488456726074\n",
      "step 27\n",
      "training loss: 2.7758066654205322\n",
      "step 28\n",
      "training loss: 2.731299877166748\n",
      "step 29\n",
      "training loss: 2.8162734508514404\n",
      "step 30\n",
      "training loss: 2.8832449913024902\n",
      "validation loss: 2.903332233428955\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8741655349731445\n",
      "step 32\n",
      "training loss: 2.8576881885528564\n",
      "step 33\n",
      "training loss: 2.8615665435791016\n",
      "step 34\n",
      "training loss: 2.875056743621826\n",
      "step 35\n",
      "training loss: 2.8749818801879883\n",
      "step 36\n",
      "training loss: 2.8773252964019775\n",
      "step 37\n",
      "training loss: 2.868100643157959\n",
      "step 38\n",
      "training loss: 2.8827426433563232\n",
      "step 39\n",
      "training loss: 2.8791749477386475\n",
      "step 40\n",
      "training loss: 2.837305784225464\n",
      "validation loss: 2.8240692615509033\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8473927974700928\n",
      "step 42\n",
      "training loss: 2.8571815490722656\n",
      "step 43\n",
      "training loss: 2.859598398208618\n",
      "step 44\n",
      "training loss: 2.8745923042297363\n",
      "step 45\n",
      "training loss: 2.8678646087646484\n",
      "step 46\n",
      "training loss: 2.8746843338012695\n",
      "step 47\n",
      "training loss: 2.875762939453125\n",
      "step 48\n",
      "training loss: 2.8660802841186523\n",
      "step 49\n",
      "training loss: 2.885204553604126\n",
      "step 50\n",
      "training loss: 2.8631370067596436\n",
      "validation loss: 2.857058525085449\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8719940185546875\n",
      "step 52\n",
      "training loss: 2.8825314044952393\n",
      "step 53\n",
      "training loss: 2.875086784362793\n",
      "step 54\n",
      "training loss: 2.871725082397461\n",
      "step 55\n",
      "training loss: 2.8749098777770996\n",
      "step 56\n",
      "training loss: 2.8592734336853027\n",
      "step 57\n",
      "training loss: 2.8368189334869385\n",
      "step 58\n",
      "training loss: 2.8626837730407715\n",
      "step 59\n",
      "training loss: 2.871129274368286\n",
      "step 60\n",
      "training loss: 2.8410253524780273\n",
      "validation loss: 2.865569591522217\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8600730895996094\n",
      "step 62\n",
      "training loss: 2.861341953277588\n",
      "step 63\n",
      "training loss: 2.8512301445007324\n",
      "step 64\n",
      "training loss: 2.8798046112060547\n",
      "step 65\n",
      "training loss: 2.8524675369262695\n",
      "step 66\n",
      "training loss: 2.869236469268799\n",
      "step 67\n",
      "training loss: 2.854362726211548\n",
      "step 68\n",
      "training loss: 2.868605375289917\n",
      "step 69\n",
      "training loss: 2.8224246501922607\n",
      "step 70\n",
      "training loss: 2.881175994873047\n",
      "validation loss: 2.8749964237213135\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.846322774887085\n",
      "step 72\n",
      "training loss: 2.877359390258789\n",
      "step 73\n",
      "training loss: 2.864776611328125\n",
      "step 74\n",
      "training loss: 2.844820737838745\n",
      "step 75\n",
      "training loss: 2.8431663513183594\n",
      "step 76\n",
      "training loss: 2.8626081943511963\n",
      "step 77\n",
      "training loss: 2.85032320022583\n",
      "step 78\n",
      "training loss: 2.8399710655212402\n",
      "step 79\n",
      "training loss: 2.8782432079315186\n",
      "step 80\n",
      "training loss: 2.8688132762908936\n",
      "validation loss: 2.902738094329834\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.860015392303467\n",
      "step 82\n",
      "training loss: 2.841949701309204\n",
      "step 83\n",
      "training loss: 2.855674982070923\n",
      "step 84\n",
      "training loss: 2.857107162475586\n",
      "step 85\n",
      "training loss: 2.8326902389526367\n",
      "step 86\n",
      "training loss: 2.8484559059143066\n",
      "step 87\n",
      "training loss: 2.8499205112457275\n",
      "step 88\n",
      "training loss: 2.843189239501953\n",
      "step 89\n",
      "training loss: 2.8353137969970703\n",
      "step 90\n",
      "training loss: 2.8704745769500732\n",
      "validation loss: 2.8654227256774902\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8392844200134277\n",
      "step 92\n",
      "training loss: 2.8755362033843994\n",
      "step 93\n",
      "training loss: 2.8589508533477783\n",
      "step 94\n",
      "training loss: 2.8375961780548096\n",
      "step 95\n",
      "training loss: 2.872915744781494\n",
      "step 96\n",
      "training loss: 2.8472349643707275\n",
      "step 97\n",
      "training loss: 2.857948064804077\n",
      "step 98\n",
      "training loss: 2.8626039028167725\n",
      "step 99\n",
      "training loss: 2.830359697341919\n",
      "step 100\n",
      "training loss: 2.874732255935669\n",
      "validation loss: 2.860697031021118\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8525519371032715\n",
      "step 102\n",
      "training loss: 2.874856472015381\n",
      "step 103\n",
      "training loss: 2.8697569370269775\n",
      "step 104\n",
      "training loss: 2.8933982849121094\n",
      "step 105\n",
      "training loss: 2.8790347576141357\n",
      "step 106\n",
      "training loss: 2.8319242000579834\n",
      "step 107\n",
      "training loss: 2.872269868850708\n",
      "step 108\n",
      "training loss: 2.8580129146575928\n",
      "step 109\n",
      "training loss: 2.8432817459106445\n",
      "step 110\n",
      "training loss: 2.857632875442505\n",
      "validation loss: 2.827617883682251\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8563010692596436\n",
      "step 112\n",
      "training loss: 2.874709129333496\n",
      "step 113\n",
      "training loss: 2.8586974143981934\n",
      "step 114\n",
      "training loss: 2.873832941055298\n",
      "step 115\n",
      "training loss: 2.875023603439331\n",
      "step 116\n",
      "training loss: 2.8779802322387695\n",
      "step 117\n",
      "training loss: 2.8531746864318848\n",
      "step 118\n",
      "training loss: 2.865633487701416\n",
      "step 119\n",
      "training loss: 2.8624255657196045\n",
      "step 120\n",
      "training loss: 2.8529374599456787\n",
      "validation loss: 2.834036350250244\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.865126609802246\n",
      "step 122\n",
      "training loss: 2.8397350311279297\n",
      "step 123\n",
      "training loss: 2.8658180236816406\n",
      "step 124\n",
      "training loss: 2.871922492980957\n",
      "step 125\n",
      "training loss: 2.8125457763671875\n",
      "step 126\n",
      "training loss: 2.8532228469848633\n",
      "step 127\n",
      "training loss: 2.848473310470581\n",
      "step 128\n",
      "training loss: 2.863515853881836\n",
      "step 129\n",
      "training loss: 2.872394323348999\n",
      "step 130\n",
      "training loss: 2.84317684173584\n",
      "validation loss: 2.8295836448669434\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8639204502105713\n",
      "step 132\n",
      "training loss: 2.8150649070739746\n",
      "step 133\n",
      "training loss: 2.847557306289673\n",
      "step 134\n",
      "training loss: 2.8721442222595215\n",
      "step 135\n",
      "training loss: 2.864356756210327\n",
      "step 136\n",
      "training loss: 2.8586416244506836\n",
      "step 137\n",
      "training loss: 2.8452978134155273\n",
      "step 138\n",
      "training loss: 2.86330246925354\n",
      "step 139\n",
      "training loss: 2.852113962173462\n",
      "step 140\n",
      "training loss: 2.860194444656372\n",
      "validation loss: 2.857208490371704\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8650929927825928\n",
      "step 142\n",
      "training loss: 2.8608109951019287\n",
      "step 143\n",
      "training loss: 2.8549585342407227\n",
      "step 144\n",
      "training loss: 2.858853816986084\n",
      "step 145\n",
      "training loss: 2.8347251415252686\n",
      "step 146\n",
      "training loss: 2.8408777713775635\n",
      "step 147\n",
      "training loss: 2.832035779953003\n",
      "step 148\n",
      "training loss: 2.8660783767700195\n",
      "step 149\n",
      "training loss: 2.8677282333374023\n",
      "step 150\n",
      "training loss: 2.8613784313201904\n",
      "validation loss: 2.868504285812378\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.859586238861084\n",
      "step 152\n",
      "training loss: 2.8519835472106934\n",
      "step 153\n",
      "training loss: 2.8694186210632324\n",
      "step 154\n",
      "training loss: 2.8887033462524414\n",
      "step 155\n",
      "training loss: 2.8102028369903564\n",
      "step 156\n",
      "training loss: 2.8574984073638916\n",
      "step 157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.850273370742798\n",
      "step 158\n",
      "training loss: 2.874471664428711\n",
      "step 159\n",
      "training loss: 2.8586015701293945\n",
      "step 160\n",
      "training loss: 2.878098487854004\n",
      "validation loss: 2.8619844913482666\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8507466316223145\n",
      "step 162\n",
      "training loss: 2.838991641998291\n",
      "step 163\n",
      "training loss: 2.8528153896331787\n",
      "step 164\n",
      "training loss: 2.8636021614074707\n",
      "step 165\n",
      "training loss: 2.8504397869110107\n",
      "step 166\n",
      "training loss: 2.8725180625915527\n",
      "step 167\n",
      "training loss: 2.858463764190674\n",
      "step 168\n",
      "training loss: 2.8240816593170166\n",
      "step 169\n",
      "training loss: 2.850292921066284\n",
      "step 170\n",
      "training loss: 2.853926420211792\n",
      "validation loss: 2.8554160594940186\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.854275941848755\n",
      "step 172\n",
      "training loss: 2.8680126667022705\n",
      "step 173\n",
      "training loss: 2.8637564182281494\n",
      "step 174\n",
      "training loss: 2.8641738891601562\n",
      "step 175\n",
      "training loss: 2.815739154815674\n",
      "step 176\n",
      "training loss: 2.8622963428497314\n",
      "step 177\n",
      "training loss: 2.856571674346924\n",
      "step 178\n",
      "training loss: 2.8618853092193604\n",
      "step 179\n",
      "training loss: 2.862865686416626\n",
      "step 180\n",
      "training loss: 2.870788097381592\n",
      "validation loss: 2.862010955810547\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8681628704071045\n",
      "step 182\n",
      "training loss: 2.8530125617980957\n",
      "step 183\n",
      "training loss: 2.877044916152954\n",
      "step 184\n",
      "training loss: 2.8760228157043457\n",
      "step 185\n",
      "training loss: 2.8639557361602783\n",
      "step 186\n",
      "training loss: 2.8599467277526855\n",
      "step 187\n",
      "training loss: 2.8684840202331543\n",
      "step 188\n",
      "training loss: 2.859395742416382\n",
      "step 189\n",
      "training loss: 2.8569042682647705\n",
      "step 190\n",
      "training loss: 2.8398213386535645\n",
      "validation loss: 2.8412153720855713\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8183655738830566\n",
      "step 192\n",
      "training loss: 2.8677399158477783\n",
      "step 193\n",
      "training loss: 2.8479974269866943\n",
      "step 194\n",
      "training loss: 2.8482279777526855\n",
      "step 195\n",
      "training loss: 2.861229658126831\n",
      "step 196\n",
      "training loss: 2.89100980758667\n",
      "step 197\n",
      "training loss: 2.8412926197052\n",
      "step 198\n",
      "training loss: 2.871685266494751\n",
      "step 199\n",
      "training loss: 2.859907627105713\n",
      "step 200\n",
      "training loss: 2.8696982860565186\n",
      "validation loss: 2.8437716960906982\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.846855640411377\n",
      "step 202\n",
      "training loss: 2.857105255126953\n",
      "step 203\n",
      "training loss: 2.832120656967163\n",
      "step 204\n",
      "training loss: 2.8501479625701904\n",
      "step 205\n",
      "training loss: 2.8592259883880615\n",
      "step 206\n",
      "training loss: 2.844040632247925\n",
      "step 207\n",
      "training loss: 2.886582851409912\n",
      "step 208\n",
      "training loss: 2.8571014404296875\n",
      "step 209\n",
      "training loss: 2.848299264907837\n",
      "step 210\n",
      "training loss: 2.8629069328308105\n",
      "validation loss: 2.829714775085449\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8655641078948975\n",
      "step 212\n",
      "training loss: 2.852747917175293\n",
      "step 213\n",
      "training loss: 2.8395488262176514\n",
      "step 214\n",
      "training loss: 2.8277125358581543\n",
      "step 215\n",
      "training loss: 2.8096425533294678\n",
      "step 216\n",
      "training loss: 2.8744070529937744\n",
      "step 217\n",
      "training loss: 2.8535256385803223\n",
      "step 218\n",
      "training loss: 2.8773865699768066\n",
      "step 219\n",
      "training loss: 2.8520588874816895\n",
      "step 220\n",
      "training loss: 2.860551595687866\n",
      "validation loss: 2.837862014770508\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.879046678543091\n",
      "step 222\n",
      "training loss: 2.8556437492370605\n",
      "step 223\n",
      "training loss: 2.847864866256714\n",
      "step 224\n",
      "training loss: 2.8651158809661865\n",
      "step 225\n",
      "training loss: 2.8424148559570312\n",
      "step 226\n",
      "training loss: 2.8507800102233887\n",
      "step 227\n",
      "training loss: 2.8446497917175293\n",
      "step 228\n",
      "training loss: 2.8479151725769043\n",
      "step 229\n",
      "training loss: 2.835491180419922\n",
      "step 230\n",
      "training loss: 2.8567683696746826\n",
      "validation loss: 2.8711397647857666\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8604376316070557\n",
      "step 232\n",
      "training loss: 2.8437302112579346\n",
      "step 233\n",
      "training loss: 2.8735945224761963\n",
      "step 234\n",
      "training loss: 2.849560260772705\n",
      "step 235\n",
      "training loss: 2.8633689880371094\n",
      "step 236\n",
      "training loss: 2.850520610809326\n",
      "step 237\n",
      "training loss: 2.8656184673309326\n",
      "step 238\n",
      "training loss: 2.790344715118408\n",
      "step 239\n",
      "training loss: 2.8503923416137695\n",
      "step 240\n",
      "training loss: 2.831472158432007\n",
      "validation loss: 2.8745663166046143\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8416285514831543\n",
      "step 242\n",
      "training loss: 2.8691494464874268\n",
      "step 243\n",
      "training loss: 2.870471477508545\n",
      "step 244\n",
      "training loss: 2.8618853092193604\n",
      "step 245\n",
      "training loss: 2.8614954948425293\n",
      "step 246\n",
      "training loss: 2.860860586166382\n",
      "step 247\n",
      "training loss: 2.846973419189453\n",
      "step 248\n",
      "training loss: 2.856553316116333\n",
      "step 249\n",
      "training loss: 2.868499755859375\n",
      "step 250\n",
      "training loss: 2.883876323699951\n",
      "validation loss: 2.8280651569366455\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8544869422912598\n",
      "step 252\n",
      "training loss: 2.8448541164398193\n",
      "step 253\n",
      "training loss: 2.847289562225342\n",
      "step 254\n",
      "training loss: 2.8460137844085693\n",
      "step 255\n",
      "training loss: 2.859003782272339\n",
      "step 256\n",
      "training loss: 2.858644962310791\n",
      "step 257\n",
      "training loss: 2.85064697265625\n",
      "step 258\n",
      "training loss: 2.863527536392212\n",
      "step 259\n",
      "training loss: 2.870072603225708\n",
      "step 260\n",
      "training loss: 2.847419023513794\n",
      "validation loss: 2.8539674282073975\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8727409839630127\n",
      "step 262\n",
      "training loss: 2.876591682434082\n",
      "step 263\n",
      "training loss: 2.8446786403656006\n",
      "step 264\n",
      "training loss: 2.848728895187378\n",
      "step 265\n",
      "training loss: 2.870387554168701\n",
      "step 266\n",
      "training loss: 2.864908456802368\n",
      "step 267\n",
      "training loss: 2.8621745109558105\n",
      "step 268\n",
      "training loss: 2.863328695297241\n",
      "step 269\n",
      "training loss: 2.8662564754486084\n",
      "step 270\n",
      "training loss: 2.856765031814575\n",
      "validation loss: 2.8743128776550293\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8634417057037354\n",
      "----------3.0 min per epoch----------\n",
      "epoch 30\n",
      "step 0\n",
      "training loss: 2.861072540283203\n",
      "validation loss: 2.8681159019470215\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8566811084747314\n",
      "step 2\n",
      "training loss: 2.855419635772705\n",
      "step 3\n",
      "training loss: 2.82047963142395\n",
      "step 4\n",
      "training loss: 2.8548476696014404\n",
      "step 5\n",
      "training loss: 2.8704817295074463\n",
      "step 6\n",
      "training loss: 2.842763900756836\n",
      "step 7\n",
      "training loss: 2.843923330307007\n",
      "step 8\n",
      "training loss: 2.882423162460327\n",
      "step 9\n",
      "training loss: 2.861255407333374\n",
      "step 10\n",
      "training loss: 2.8764331340789795\n",
      "validation loss: 2.859814167022705\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.876850128173828\n",
      "step 12\n",
      "training loss: 2.8635542392730713\n",
      "step 13\n",
      "training loss: 2.845512866973877\n",
      "step 14\n",
      "training loss: 2.875227689743042\n",
      "step 15\n",
      "training loss: 2.8765008449554443\n",
      "step 16\n",
      "training loss: 2.867574691772461\n",
      "step 17\n",
      "training loss: 2.8626034259796143\n",
      "step 18\n",
      "training loss: 2.874490261077881\n",
      "step 19\n",
      "training loss: 2.8832201957702637\n",
      "step 20\n",
      "training loss: 2.7999110221862793\n",
      "validation loss: 2.945606231689453\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8435003757476807\n",
      "step 22\n",
      "training loss: 2.8057379722595215\n",
      "step 23\n",
      "training loss: 2.844839096069336\n",
      "step 24\n",
      "training loss: 2.8778746128082275\n",
      "step 25\n",
      "training loss: 2.868436813354492\n",
      "step 26\n",
      "training loss: 2.882920742034912\n",
      "step 27\n",
      "training loss: 2.8644890785217285\n",
      "step 28\n",
      "training loss: 2.77144455909729\n",
      "step 29\n",
      "training loss: 2.7299394607543945\n",
      "step 30\n",
      "training loss: 2.8100271224975586\n",
      "validation loss: 2.8825061321258545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8833444118499756\n",
      "step 32\n",
      "training loss: 2.8711555004119873\n",
      "step 33\n",
      "training loss: 2.855008125305176\n",
      "step 34\n",
      "training loss: 2.8628480434417725\n",
      "step 35\n",
      "training loss: 2.875487804412842\n",
      "step 36\n",
      "training loss: 2.8788411617279053\n",
      "step 37\n",
      "training loss: 2.8779098987579346\n",
      "step 38\n",
      "training loss: 2.8627429008483887\n",
      "step 39\n",
      "training loss: 2.880401372909546\n",
      "step 40\n",
      "training loss: 2.879438638687134\n",
      "validation loss: 2.833068609237671\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8359017372131348\n",
      "step 42\n",
      "training loss: 2.8422863483428955\n",
      "step 43\n",
      "training loss: 2.8556392192840576\n",
      "step 44\n",
      "training loss: 2.858814239501953\n",
      "step 45\n",
      "training loss: 2.8716771602630615\n",
      "step 46\n",
      "training loss: 2.8700387477874756\n",
      "step 47\n",
      "training loss: 2.872525453567505\n",
      "step 48\n",
      "training loss: 2.876173973083496\n",
      "step 49\n",
      "training loss: 2.8664638996124268\n",
      "step 50\n",
      "training loss: 2.8840842247009277\n",
      "validation loss: 2.8593411445617676\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 51\n",
      "training loss: 2.863931655883789\n",
      "step 52\n",
      "training loss: 2.8718273639678955\n",
      "step 53\n",
      "training loss: 2.881913185119629\n",
      "step 54\n",
      "training loss: 2.875248908996582\n",
      "step 55\n",
      "training loss: 2.8732354640960693\n",
      "step 56\n",
      "training loss: 2.873194694519043\n",
      "step 57\n",
      "training loss: 2.8566746711730957\n",
      "step 58\n",
      "training loss: 2.8358561992645264\n",
      "step 59\n",
      "training loss: 2.8599720001220703\n",
      "step 60\n",
      "training loss: 2.8708224296569824\n",
      "validation loss: 2.890307903289795\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8473222255706787\n",
      "step 62\n",
      "training loss: 2.860208511352539\n",
      "step 63\n",
      "training loss: 2.8601906299591064\n",
      "step 64\n",
      "training loss: 2.848677158355713\n",
      "step 65\n",
      "training loss: 2.8785855770111084\n",
      "step 66\n",
      "training loss: 2.853732109069824\n",
      "step 67\n",
      "training loss: 2.8694558143615723\n",
      "step 68\n",
      "training loss: 2.8526508808135986\n",
      "step 69\n",
      "training loss: 2.8685739040374756\n",
      "step 70\n",
      "training loss: 2.8254506587982178\n",
      "validation loss: 2.8129398822784424\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.882554531097412\n",
      "step 72\n",
      "training loss: 2.8489022254943848\n",
      "step 73\n",
      "training loss: 2.8800275325775146\n",
      "step 74\n",
      "training loss: 2.861734390258789\n",
      "step 75\n",
      "training loss: 2.846184253692627\n",
      "step 76\n",
      "training loss: 2.8404548168182373\n",
      "step 77\n",
      "training loss: 2.8617103099823\n",
      "step 78\n",
      "training loss: 2.8528597354888916\n",
      "step 79\n",
      "training loss: 2.843327760696411\n",
      "step 80\n",
      "training loss: 2.876997709274292\n",
      "validation loss: 2.8534095287323\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.868847370147705\n",
      "step 82\n",
      "training loss: 2.8594956398010254\n",
      "step 83\n",
      "training loss: 2.843661069869995\n",
      "step 84\n",
      "training loss: 2.8565902709960938\n",
      "step 85\n",
      "training loss: 2.8580520153045654\n",
      "step 86\n",
      "training loss: 2.831733465194702\n",
      "step 87\n",
      "training loss: 2.848588228225708\n",
      "step 88\n",
      "training loss: 2.848287343978882\n",
      "step 89\n",
      "training loss: 2.8447840213775635\n",
      "step 90\n",
      "training loss: 2.8356854915618896\n",
      "validation loss: 2.8621463775634766\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8712987899780273\n",
      "step 92\n",
      "training loss: 2.8399510383605957\n",
      "step 93\n",
      "training loss: 2.8762924671173096\n",
      "step 94\n",
      "training loss: 2.8581888675689697\n",
      "step 95\n",
      "training loss: 2.837711811065674\n",
      "step 96\n",
      "training loss: 2.8733839988708496\n",
      "step 97\n",
      "training loss: 2.8457772731781006\n",
      "step 98\n",
      "training loss: 2.8575785160064697\n",
      "step 99\n",
      "training loss: 2.8616178035736084\n",
      "step 100\n",
      "training loss: 2.831035852432251\n",
      "validation loss: 2.865097761154175\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.874920129776001\n",
      "step 102\n",
      "training loss: 2.853353500366211\n",
      "step 103\n",
      "training loss: 2.8742499351501465\n",
      "step 104\n",
      "training loss: 2.87016224861145\n",
      "step 105\n",
      "training loss: 2.893928289413452\n",
      "step 106\n",
      "training loss: 2.8794186115264893\n",
      "step 107\n",
      "training loss: 2.8322432041168213\n",
      "step 108\n",
      "training loss: 2.8692994117736816\n",
      "step 109\n",
      "training loss: 2.856313705444336\n",
      "step 110\n",
      "training loss: 2.844193458557129\n",
      "validation loss: 2.900495767593384\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.856444835662842\n",
      "step 112\n",
      "training loss: 2.85433030128479\n",
      "step 113\n",
      "training loss: 2.874861717224121\n",
      "step 114\n",
      "training loss: 2.8602049350738525\n",
      "step 115\n",
      "training loss: 2.8738481998443604\n",
      "step 116\n",
      "training loss: 2.873676300048828\n",
      "step 117\n",
      "training loss: 2.878807544708252\n",
      "step 118\n",
      "training loss: 2.8569467067718506\n",
      "step 119\n",
      "training loss: 2.865475654602051\n",
      "step 120\n",
      "training loss: 2.861855983734131\n",
      "validation loss: 2.8658218383789062\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.855386257171631\n",
      "step 122\n",
      "training loss: 2.866504669189453\n",
      "step 123\n",
      "training loss: 2.840876579284668\n",
      "step 124\n",
      "training loss: 2.8649919033050537\n",
      "step 125\n",
      "training loss: 2.8729169368743896\n",
      "step 126\n",
      "training loss: 2.8084635734558105\n",
      "step 127\n",
      "training loss: 2.853464365005493\n",
      "step 128\n",
      "training loss: 2.847595691680908\n",
      "step 129\n",
      "training loss: 2.864009141921997\n",
      "step 130\n",
      "training loss: 2.8731231689453125\n",
      "validation loss: 2.8580830097198486\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8437345027923584\n",
      "step 132\n",
      "training loss: 2.8630614280700684\n",
      "step 133\n",
      "training loss: 2.813861131668091\n",
      "step 134\n",
      "training loss: 2.8473610877990723\n",
      "step 135\n",
      "training loss: 2.8733036518096924\n",
      "step 136\n",
      "training loss: 2.863867998123169\n",
      "step 137\n",
      "training loss: 2.8576366901397705\n",
      "step 138\n",
      "training loss: 2.8434031009674072\n",
      "step 139\n",
      "training loss: 2.8647689819335938\n",
      "step 140\n",
      "training loss: 2.8482797145843506\n",
      "validation loss: 2.825155735015869\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8582024574279785\n",
      "step 142\n",
      "training loss: 2.864780902862549\n",
      "step 143\n",
      "training loss: 2.861121892929077\n",
      "step 144\n",
      "training loss: 2.8547024726867676\n",
      "step 145\n",
      "training loss: 2.857908248901367\n",
      "step 146\n",
      "training loss: 2.8332650661468506\n",
      "step 147\n",
      "training loss: 2.839510679244995\n",
      "step 148\n",
      "training loss: 2.831803321838379\n",
      "step 149\n",
      "training loss: 2.864920139312744\n",
      "step 150\n",
      "training loss: 2.8682620525360107\n",
      "validation loss: 2.831070899963379\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.860398769378662\n",
      "step 152\n",
      "training loss: 2.8597326278686523\n",
      "step 153\n",
      "training loss: 2.8519906997680664\n",
      "step 154\n",
      "training loss: 2.866983413696289\n",
      "step 155\n",
      "training loss: 2.888291120529175\n",
      "step 156\n",
      "training loss: 2.806206464767456\n",
      "step 157\n",
      "training loss: 2.8587779998779297\n",
      "step 158\n",
      "training loss: 2.851119041442871\n",
      "step 159\n",
      "training loss: 2.875863790512085\n",
      "step 160\n",
      "training loss: 2.8578755855560303\n",
      "validation loss: 2.8252336978912354\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8765246868133545\n",
      "step 162\n",
      "training loss: 2.8515732288360596\n",
      "step 163\n",
      "training loss: 2.841144561767578\n",
      "step 164\n",
      "training loss: 2.8528575897216797\n",
      "step 165\n",
      "training loss: 2.866734266281128\n",
      "step 166\n",
      "training loss: 2.852271556854248\n",
      "step 167\n",
      "training loss: 2.8747191429138184\n",
      "step 168\n",
      "training loss: 2.859119415283203\n",
      "step 169\n",
      "training loss: 2.8209471702575684\n",
      "step 170\n",
      "training loss: 2.850628137588501\n",
      "validation loss: 2.855738878250122\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8497190475463867\n",
      "step 172\n",
      "training loss: 2.8497390747070312\n",
      "step 173\n",
      "training loss: 2.868619203567505\n",
      "step 174\n",
      "training loss: 2.8639540672302246\n",
      "step 175\n",
      "training loss: 2.8661675453186035\n",
      "step 176\n",
      "training loss: 2.8123972415924072\n",
      "step 177\n",
      "training loss: 2.8627688884735107\n",
      "step 178\n",
      "training loss: 2.8562939167022705\n",
      "step 179\n",
      "training loss: 2.860577344894409\n",
      "step 180\n",
      "training loss: 2.86299467086792\n",
      "validation loss: 2.8655166625976562\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.874504327774048\n",
      "step 182\n",
      "training loss: 2.868912935256958\n",
      "step 183\n",
      "training loss: 2.8525543212890625\n",
      "step 184\n",
      "training loss: 2.876614570617676\n",
      "step 185\n",
      "training loss: 2.8764233589172363\n",
      "step 186\n",
      "training loss: 2.8610262870788574\n",
      "step 187\n",
      "training loss: 2.858980417251587\n",
      "step 188\n",
      "training loss: 2.8682069778442383\n",
      "step 189\n",
      "training loss: 2.859261989593506\n",
      "step 190\n",
      "training loss: 2.8585257530212402\n",
      "validation loss: 2.858823299407959\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.841367244720459\n",
      "step 192\n",
      "training loss: 2.8208606243133545\n",
      "step 193\n",
      "training loss: 2.868032217025757\n",
      "step 194\n",
      "training loss: 2.846611261367798\n",
      "step 195\n",
      "training loss: 2.847163200378418\n",
      "step 196\n",
      "training loss: 2.8658998012542725\n",
      "step 197\n",
      "training loss: 2.890021562576294\n",
      "step 198\n",
      "training loss: 2.8416178226470947\n",
      "step 199\n",
      "training loss: 2.8712520599365234\n",
      "step 200\n",
      "training loss: 2.8587605953216553\n",
      "validation loss: 2.854971408843994\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.869870185852051\n",
      "step 202\n",
      "training loss: 2.847219467163086\n",
      "step 203\n",
      "training loss: 2.8588106632232666\n",
      "step 204\n",
      "training loss: 2.837742805480957\n",
      "step 205\n",
      "training loss: 2.84842586517334\n",
      "step 206\n",
      "training loss: 2.8557322025299072\n",
      "step 207\n",
      "training loss: 2.844346046447754\n",
      "step 208\n",
      "training loss: 2.882471799850464\n",
      "step 209\n",
      "training loss: 2.8582406044006348\n",
      "step 210\n",
      "training loss: 2.8463637828826904\n",
      "validation loss: 2.8621761798858643\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8600988388061523\n",
      "step 212\n",
      "training loss: 2.86733078956604\n",
      "step 213\n",
      "training loss: 2.8527119159698486\n",
      "step 214\n",
      "training loss: 2.8419556617736816\n",
      "step 215\n",
      "training loss: 2.8262808322906494\n",
      "step 216\n",
      "training loss: 2.8110737800598145\n",
      "step 217\n",
      "training loss: 2.8761754035949707\n",
      "step 218\n",
      "training loss: 2.853097438812256\n",
      "step 219\n",
      "training loss: 2.8787755966186523\n",
      "step 220\n",
      "training loss: 2.8526084423065186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.8426921367645264\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.861945152282715\n",
      "step 222\n",
      "training loss: 2.880017042160034\n",
      "step 223\n",
      "training loss: 2.8581323623657227\n",
      "step 224\n",
      "training loss: 2.848902940750122\n",
      "step 225\n",
      "training loss: 2.8639795780181885\n",
      "step 226\n",
      "training loss: 2.8438620567321777\n",
      "step 227\n",
      "training loss: 2.8513739109039307\n",
      "step 228\n",
      "training loss: 2.842378854751587\n",
      "step 229\n",
      "training loss: 2.845222234725952\n",
      "step 230\n",
      "training loss: 2.8342409133911133\n",
      "validation loss: 2.841132164001465\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.857668399810791\n",
      "step 232\n",
      "training loss: 2.8587899208068848\n",
      "step 233\n",
      "training loss: 2.8448233604431152\n",
      "step 234\n",
      "training loss: 2.873558282852173\n",
      "step 235\n",
      "training loss: 2.8481009006500244\n",
      "step 236\n",
      "training loss: 2.860382318496704\n",
      "step 237\n",
      "training loss: 2.849712610244751\n",
      "step 238\n",
      "training loss: 2.8653628826141357\n",
      "step 239\n",
      "training loss: 2.790239095687866\n",
      "step 240\n",
      "training loss: 2.8497371673583984\n",
      "validation loss: 2.8278305530548096\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.830592393875122\n",
      "step 242\n",
      "training loss: 2.842461109161377\n",
      "step 243\n",
      "training loss: 2.8666698932647705\n",
      "step 244\n",
      "training loss: 2.870666027069092\n",
      "step 245\n",
      "training loss: 2.8623530864715576\n",
      "step 246\n",
      "training loss: 2.8599109649658203\n",
      "step 247\n",
      "training loss: 2.8597116470336914\n",
      "step 248\n",
      "training loss: 2.8503620624542236\n",
      "step 249\n",
      "training loss: 2.8549723625183105\n",
      "step 250\n",
      "training loss: 2.870882511138916\n",
      "validation loss: 2.8287971019744873\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.884127378463745\n",
      "step 252\n",
      "training loss: 2.8552908897399902\n",
      "step 253\n",
      "training loss: 2.844872236251831\n",
      "step 254\n",
      "training loss: 2.84788179397583\n",
      "step 255\n",
      "training loss: 2.847442626953125\n",
      "step 256\n",
      "training loss: 2.858123540878296\n",
      "step 257\n",
      "training loss: 2.859900951385498\n",
      "step 258\n",
      "training loss: 2.8524277210235596\n",
      "step 259\n",
      "training loss: 2.864625930786133\n",
      "step 260\n",
      "training loss: 2.87222957611084\n",
      "validation loss: 2.8723199367523193\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8465375900268555\n",
      "step 262\n",
      "training loss: 2.8731584548950195\n",
      "step 263\n",
      "training loss: 2.8758299350738525\n",
      "step 264\n",
      "training loss: 2.844165325164795\n",
      "step 265\n",
      "training loss: 2.8466033935546875\n",
      "step 266\n",
      "training loss: 2.8699183464050293\n",
      "step 267\n",
      "training loss: 2.864283561706543\n",
      "step 268\n",
      "training loss: 2.862391710281372\n",
      "step 269\n",
      "training loss: 2.863196611404419\n",
      "step 270\n",
      "training loss: 2.8656163215637207\n",
      "validation loss: 2.8739585876464844\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.858048677444458\n",
      "----------3.0 min per epoch----------\n",
      "epoch 31\n",
      "step 0\n",
      "training loss: 2.8633038997650146\n",
      "validation loss: 2.8275253772735596\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8611812591552734\n",
      "step 2\n",
      "training loss: 2.8536646366119385\n",
      "step 3\n",
      "training loss: 2.8534321784973145\n",
      "step 4\n",
      "training loss: 2.8190319538116455\n",
      "step 5\n",
      "training loss: 2.847987651824951\n",
      "step 6\n",
      "training loss: 2.8710057735443115\n",
      "step 7\n",
      "training loss: 2.8421380519866943\n",
      "step 8\n",
      "training loss: 2.841517686843872\n",
      "step 9\n",
      "training loss: 2.882864475250244\n",
      "step 10\n",
      "training loss: 2.8604485988616943\n",
      "validation loss: 2.859567880630493\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8778252601623535\n",
      "step 12\n",
      "training loss: 2.8785698413848877\n",
      "step 13\n",
      "training loss: 2.8643929958343506\n",
      "step 14\n",
      "training loss: 2.8447749614715576\n",
      "step 15\n",
      "training loss: 2.8748960494995117\n",
      "step 16\n",
      "training loss: 2.8752431869506836\n",
      "step 17\n",
      "training loss: 2.8664655685424805\n",
      "step 18\n",
      "training loss: 2.863365411758423\n",
      "step 19\n",
      "training loss: 2.8756158351898193\n",
      "step 20\n",
      "training loss: 2.8838882446289062\n",
      "validation loss: 2.8729569911956787\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.803100824356079\n",
      "step 22\n",
      "training loss: 2.8451120853424072\n",
      "step 23\n",
      "training loss: 2.79386830329895\n",
      "step 24\n",
      "training loss: 2.8401145935058594\n",
      "step 25\n",
      "training loss: 2.8746893405914307\n",
      "step 26\n",
      "training loss: 2.8685519695281982\n",
      "step 27\n",
      "training loss: 2.8836114406585693\n",
      "step 28\n",
      "training loss: 2.8685426712036133\n",
      "step 29\n",
      "training loss: 2.76623797416687\n",
      "step 30\n",
      "training loss: 2.716217517852783\n",
      "validation loss: 2.880812168121338\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8232202529907227\n",
      "step 32\n",
      "training loss: 2.8817057609558105\n",
      "step 33\n",
      "training loss: 2.869126796722412\n",
      "step 34\n",
      "training loss: 2.859253406524658\n",
      "step 35\n",
      "training loss: 2.860161781311035\n",
      "step 36\n",
      "training loss: 2.8741867542266846\n",
      "step 37\n",
      "training loss: 2.876801013946533\n",
      "step 38\n",
      "training loss: 2.878072738647461\n",
      "step 39\n",
      "training loss: 2.864992380142212\n",
      "step 40\n",
      "training loss: 2.881584644317627\n",
      "validation loss: 2.8635809421539307\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8802239894866943\n",
      "step 42\n",
      "training loss: 2.837827205657959\n",
      "step 43\n",
      "training loss: 2.846191644668579\n",
      "step 44\n",
      "training loss: 2.856774091720581\n",
      "step 45\n",
      "training loss: 2.856656551361084\n",
      "step 46\n",
      "training loss: 2.8707008361816406\n",
      "step 47\n",
      "training loss: 2.8675429821014404\n",
      "step 48\n",
      "training loss: 2.872988700866699\n",
      "step 49\n",
      "training loss: 2.8748505115509033\n",
      "step 50\n",
      "training loss: 2.8672938346862793\n",
      "validation loss: 2.9431815147399902\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8854517936706543\n",
      "step 52\n",
      "training loss: 2.864269733428955\n",
      "step 53\n",
      "training loss: 2.8713810443878174\n",
      "step 54\n",
      "training loss: 2.880401611328125\n",
      "step 55\n",
      "training loss: 2.8772900104522705\n",
      "step 56\n",
      "training loss: 2.8711071014404297\n",
      "step 57\n",
      "training loss: 2.872910261154175\n",
      "step 58\n",
      "training loss: 2.858001708984375\n",
      "step 59\n",
      "training loss: 2.834566116333008\n",
      "step 60\n",
      "training loss: 2.859947443008423\n",
      "validation loss: 2.8810579776763916\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8702149391174316\n",
      "step 62\n",
      "training loss: 2.844630718231201\n",
      "step 63\n",
      "training loss: 2.860673666000366\n",
      "step 64\n",
      "training loss: 2.8614230155944824\n",
      "step 65\n",
      "training loss: 2.848149061203003\n",
      "step 66\n",
      "training loss: 2.879204273223877\n",
      "step 67\n",
      "training loss: 2.8539319038391113\n",
      "step 68\n",
      "training loss: 2.870333671569824\n",
      "step 69\n",
      "training loss: 2.8535525798797607\n",
      "step 70\n",
      "training loss: 2.868126153945923\n",
      "validation loss: 2.838287353515625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8228893280029297\n",
      "step 72\n",
      "training loss: 2.8836889266967773\n",
      "step 73\n",
      "training loss: 2.846893548965454\n",
      "step 74\n",
      "training loss: 2.8816089630126953\n",
      "step 75\n",
      "training loss: 2.8655588626861572\n",
      "step 76\n",
      "training loss: 2.8464102745056152\n",
      "step 77\n",
      "training loss: 2.843822479248047\n",
      "step 78\n",
      "training loss: 2.862868547439575\n",
      "step 79\n",
      "training loss: 2.8504459857940674\n",
      "step 80\n",
      "training loss: 2.8419189453125\n",
      "validation loss: 2.860628366470337\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8767597675323486\n",
      "step 82\n",
      "training loss: 2.8652162551879883\n",
      "step 83\n",
      "training loss: 2.857865810394287\n",
      "step 84\n",
      "training loss: 2.8433055877685547\n",
      "step 85\n",
      "training loss: 2.857522487640381\n",
      "step 86\n",
      "training loss: 2.858726978302002\n",
      "step 87\n",
      "training loss: 2.8323230743408203\n",
      "step 88\n",
      "training loss: 2.850515842437744\n",
      "step 89\n",
      "training loss: 2.849260091781616\n",
      "step 90\n",
      "training loss: 2.8442206382751465\n",
      "validation loss: 2.897390842437744\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8362371921539307\n",
      "step 92\n",
      "training loss: 2.8706986904144287\n",
      "step 93\n",
      "training loss: 2.8420629501342773\n",
      "step 94\n",
      "training loss: 2.8780126571655273\n",
      "step 95\n",
      "training loss: 2.8596091270446777\n",
      "step 96\n",
      "training loss: 2.8366754055023193\n",
      "step 97\n",
      "training loss: 2.8768808841705322\n",
      "step 98\n",
      "training loss: 2.8488612174987793\n",
      "step 99\n",
      "training loss: 2.8577845096588135\n",
      "step 100\n",
      "training loss: 2.8635921478271484\n",
      "validation loss: 2.8224878311157227\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.831136703491211\n",
      "step 102\n",
      "training loss: 2.8750438690185547\n",
      "step 103\n",
      "training loss: 2.8544564247131348\n",
      "step 104\n",
      "training loss: 2.8753952980041504\n",
      "step 105\n",
      "training loss: 2.870028495788574\n",
      "step 106\n",
      "training loss: 2.892191171646118\n",
      "step 107\n",
      "training loss: 2.8783071041107178\n",
      "step 108\n",
      "training loss: 2.83203387260437\n",
      "step 109\n",
      "training loss: 2.8705992698669434\n",
      "step 110\n",
      "training loss: 2.8554651737213135\n",
      "validation loss: 2.854671001434326\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8438234329223633\n",
      "step 112\n",
      "training loss: 2.858760356903076\n",
      "step 113\n",
      "training loss: 2.85868239402771\n",
      "step 114\n",
      "training loss: 2.8756771087646484\n",
      "step 115\n",
      "training loss: 2.8597099781036377\n",
      "step 116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.874863624572754\n",
      "step 117\n",
      "training loss: 2.874225378036499\n",
      "step 118\n",
      "training loss: 2.878387212753296\n",
      "step 119\n",
      "training loss: 2.8541297912597656\n",
      "step 120\n",
      "training loss: 2.8653066158294678\n",
      "validation loss: 2.8618662357330322\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8616139888763428\n",
      "step 122\n",
      "training loss: 2.853449583053589\n",
      "step 123\n",
      "training loss: 2.8664252758026123\n",
      "step 124\n",
      "training loss: 2.840707540512085\n",
      "step 125\n",
      "training loss: 2.8660101890563965\n",
      "step 126\n",
      "training loss: 2.8730735778808594\n",
      "step 127\n",
      "training loss: 2.8106138706207275\n",
      "step 128\n",
      "training loss: 2.8523874282836914\n",
      "step 129\n",
      "training loss: 2.849522352218628\n",
      "step 130\n",
      "training loss: 2.8602135181427\n",
      "validation loss: 2.8655545711517334\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.872084140777588\n",
      "step 132\n",
      "training loss: 2.842721462249756\n",
      "step 133\n",
      "training loss: 2.8640449047088623\n",
      "step 134\n",
      "training loss: 2.813185930252075\n",
      "step 135\n",
      "training loss: 2.8483877182006836\n",
      "step 136\n",
      "training loss: 2.8738458156585693\n",
      "step 137\n",
      "training loss: 2.8643085956573486\n",
      "step 138\n",
      "training loss: 2.857404947280884\n",
      "step 139\n",
      "training loss: 2.8465678691864014\n",
      "step 140\n",
      "training loss: 2.86287522315979\n",
      "validation loss: 2.9130983352661133\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.85141921043396\n",
      "step 142\n",
      "training loss: 2.863173007965088\n",
      "step 143\n",
      "training loss: 2.8640103340148926\n",
      "step 144\n",
      "training loss: 2.864264965057373\n",
      "step 145\n",
      "training loss: 2.8553028106689453\n",
      "step 146\n",
      "training loss: 2.8571817874908447\n",
      "step 147\n",
      "training loss: 2.8338263034820557\n",
      "step 148\n",
      "training loss: 2.8384320735931396\n",
      "step 149\n",
      "training loss: 2.8325035572052\n",
      "step 150\n",
      "training loss: 2.864426374435425\n",
      "validation loss: 2.864192247390747\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8655524253845215\n",
      "step 152\n",
      "training loss: 2.8611814975738525\n",
      "step 153\n",
      "training loss: 2.857950210571289\n",
      "step 154\n",
      "training loss: 2.849872350692749\n",
      "step 155\n",
      "training loss: 2.865177869796753\n",
      "step 156\n",
      "training loss: 2.889268159866333\n",
      "step 157\n",
      "training loss: 2.8097968101501465\n",
      "step 158\n",
      "training loss: 2.8575241565704346\n",
      "step 159\n",
      "training loss: 2.852198362350464\n",
      "step 160\n",
      "training loss: 2.8738255500793457\n",
      "validation loss: 2.8591055870056152\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.857553482055664\n",
      "step 162\n",
      "training loss: 2.8765275478363037\n",
      "step 163\n",
      "training loss: 2.8515353202819824\n",
      "step 164\n",
      "training loss: 2.838783025741577\n",
      "step 165\n",
      "training loss: 2.8524832725524902\n",
      "step 166\n",
      "training loss: 2.866126775741577\n",
      "step 167\n",
      "training loss: 2.850703001022339\n",
      "step 168\n",
      "training loss: 2.8723528385162354\n",
      "step 169\n",
      "training loss: 2.859393835067749\n",
      "step 170\n",
      "training loss: 2.8243672847747803\n",
      "validation loss: 2.8238563537597656\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8500397205352783\n",
      "step 172\n",
      "training loss: 2.848489999771118\n",
      "step 173\n",
      "training loss: 2.8524556159973145\n",
      "step 174\n",
      "training loss: 2.8701348304748535\n",
      "step 175\n",
      "training loss: 2.8646061420440674\n",
      "step 176\n",
      "training loss: 2.8647820949554443\n",
      "step 177\n",
      "training loss: 2.811894416809082\n",
      "step 178\n",
      "training loss: 2.8624536991119385\n",
      "step 179\n",
      "training loss: 2.855203151702881\n",
      "step 180\n",
      "training loss: 2.863675117492676\n",
      "validation loss: 2.833117961883545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8622703552246094\n",
      "step 182\n",
      "training loss: 2.872826099395752\n",
      "step 183\n",
      "training loss: 2.868201494216919\n",
      "step 184\n",
      "training loss: 2.8522555828094482\n",
      "step 185\n",
      "training loss: 2.8767855167388916\n",
      "step 186\n",
      "training loss: 2.8763232231140137\n",
      "step 187\n",
      "training loss: 2.8643431663513184\n",
      "step 188\n",
      "training loss: 2.859764814376831\n",
      "step 189\n",
      "training loss: 2.869014263153076\n",
      "step 190\n",
      "training loss: 2.8594841957092285\n",
      "validation loss: 2.8264715671539307\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.858844518661499\n",
      "step 192\n",
      "training loss: 2.835390329360962\n",
      "step 193\n",
      "training loss: 2.819178581237793\n",
      "step 194\n",
      "training loss: 2.869210958480835\n",
      "step 195\n",
      "training loss: 2.847991704940796\n",
      "step 196\n",
      "training loss: 2.8476028442382812\n",
      "step 197\n",
      "training loss: 2.8608787059783936\n",
      "step 198\n",
      "training loss: 2.890381336212158\n",
      "step 199\n",
      "training loss: 2.843719482421875\n",
      "step 200\n",
      "training loss: 2.8707101345062256\n",
      "validation loss: 2.856592893600464\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.8602652549743652\n",
      "step 202\n",
      "training loss: 2.8698298931121826\n",
      "step 203\n",
      "training loss: 2.8477303981781006\n",
      "step 204\n",
      "training loss: 2.8589212894439697\n",
      "step 205\n",
      "training loss: 2.837937831878662\n",
      "step 206\n",
      "training loss: 2.851207971572876\n",
      "step 207\n",
      "training loss: 2.856934070587158\n",
      "step 208\n",
      "training loss: 2.84441876411438\n",
      "step 209\n",
      "training loss: 2.8833491802215576\n",
      "step 210\n",
      "training loss: 2.857484817504883\n",
      "validation loss: 2.8667092323303223\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.848947763442993\n",
      "step 212\n",
      "training loss: 2.8609089851379395\n",
      "step 213\n",
      "training loss: 2.8673155307769775\n",
      "step 214\n",
      "training loss: 2.852031707763672\n",
      "step 215\n",
      "training loss: 2.842238426208496\n",
      "step 216\n",
      "training loss: 2.8254494667053223\n",
      "step 217\n",
      "training loss: 2.810974597930908\n",
      "step 218\n",
      "training loss: 2.8736321926116943\n",
      "step 219\n",
      "training loss: 2.8541393280029297\n",
      "step 220\n",
      "training loss: 2.8781368732452393\n",
      "validation loss: 2.85978364944458\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.853334665298462\n",
      "step 222\n",
      "training loss: 2.8623974323272705\n",
      "step 223\n",
      "training loss: 2.879835367202759\n",
      "step 224\n",
      "training loss: 2.8581206798553467\n",
      "step 225\n",
      "training loss: 2.8462233543395996\n",
      "step 226\n",
      "training loss: 2.862614393234253\n",
      "step 227\n",
      "training loss: 2.8435277938842773\n",
      "step 228\n",
      "training loss: 2.853067636489868\n",
      "step 229\n",
      "training loss: 2.843784809112549\n",
      "step 230\n",
      "training loss: 2.8457529544830322\n",
      "validation loss: 2.8572211265563965\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8350229263305664\n",
      "step 232\n",
      "training loss: 2.856457471847534\n",
      "step 233\n",
      "training loss: 2.857693672180176\n",
      "step 234\n",
      "training loss: 2.846958637237549\n",
      "step 235\n",
      "training loss: 2.870616912841797\n",
      "step 236\n",
      "training loss: 2.8484184741973877\n",
      "step 237\n",
      "training loss: 2.8617489337921143\n",
      "step 238\n",
      "training loss: 2.849339008331299\n",
      "step 239\n",
      "training loss: 2.8650786876678467\n",
      "step 240\n",
      "training loss: 2.7880895137786865\n",
      "validation loss: 2.8565499782562256\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8489084243774414\n",
      "step 242\n",
      "training loss: 2.8296332359313965\n",
      "step 243\n",
      "training loss: 2.8470458984375\n",
      "step 244\n",
      "training loss: 2.87001895904541\n",
      "step 245\n",
      "training loss: 2.8685460090637207\n",
      "step 246\n",
      "training loss: 2.8607983589172363\n",
      "step 247\n",
      "training loss: 2.859846591949463\n",
      "step 248\n",
      "training loss: 2.860837459564209\n",
      "step 249\n",
      "training loss: 2.849686622619629\n",
      "step 250\n",
      "training loss: 2.85364031791687\n",
      "validation loss: 2.840028762817383\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8680806159973145\n",
      "step 252\n",
      "training loss: 2.8839008808135986\n",
      "step 253\n",
      "training loss: 2.855329751968384\n",
      "step 254\n",
      "training loss: 2.8449602127075195\n",
      "step 255\n",
      "training loss: 2.846531867980957\n",
      "step 256\n",
      "training loss: 2.846266269683838\n",
      "step 257\n",
      "training loss: 2.859337091445923\n",
      "step 258\n",
      "training loss: 2.85969877243042\n",
      "step 259\n",
      "training loss: 2.850179433822632\n",
      "step 260\n",
      "training loss: 2.864870071411133\n",
      "validation loss: 2.8379828929901123\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8703789710998535\n",
      "step 262\n",
      "training loss: 2.8461337089538574\n",
      "step 263\n",
      "training loss: 2.8730404376983643\n",
      "step 264\n",
      "training loss: 2.874337673187256\n",
      "step 265\n",
      "training loss: 2.844374895095825\n",
      "step 266\n",
      "training loss: 2.8447787761688232\n",
      "step 267\n",
      "training loss: 2.8702964782714844\n",
      "step 268\n",
      "training loss: 2.8636634349823\n",
      "step 269\n",
      "training loss: 2.863605499267578\n",
      "step 270\n",
      "training loss: 2.8626656532287598\n",
      "validation loss: 2.824967861175537\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.865323066711426\n",
      "----------3.0 min per epoch----------\n",
      "epoch 32\n",
      "step 0\n",
      "training loss: 2.857208251953125\n",
      "validation loss: 2.826455593109131\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.863832712173462\n",
      "step 2\n",
      "training loss: 2.860591173171997\n",
      "step 3\n",
      "training loss: 2.8549485206604004\n",
      "step 4\n",
      "training loss: 2.8522019386291504\n",
      "step 5\n",
      "training loss: 2.8215808868408203\n",
      "step 6\n",
      "training loss: 2.8495798110961914\n",
      "step 7\n",
      "training loss: 2.8697168827056885\n",
      "step 8\n",
      "training loss: 2.8416054248809814\n",
      "step 9\n",
      "training loss: 2.845857620239258\n",
      "step 10\n",
      "training loss: 2.883598566055298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.87368106842041\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.860579013824463\n",
      "step 12\n",
      "training loss: 2.8800771236419678\n",
      "step 13\n",
      "training loss: 2.8801708221435547\n",
      "step 14\n",
      "training loss: 2.865360975265503\n",
      "step 15\n",
      "training loss: 2.846527576446533\n",
      "step 16\n",
      "training loss: 2.87396240234375\n",
      "step 17\n",
      "training loss: 2.874035358428955\n",
      "step 18\n",
      "training loss: 2.867892265319824\n",
      "step 19\n",
      "training loss: 2.8614892959594727\n",
      "step 20\n",
      "training loss: 2.875070095062256\n",
      "validation loss: 2.878805160522461\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.885056495666504\n",
      "step 22\n",
      "training loss: 2.802263021469116\n",
      "step 23\n",
      "training loss: 2.847935199737549\n",
      "step 24\n",
      "training loss: 2.795982837677002\n",
      "step 25\n",
      "training loss: 2.8382928371429443\n",
      "step 26\n",
      "training loss: 2.8777849674224854\n",
      "step 27\n",
      "training loss: 2.8714396953582764\n",
      "step 28\n",
      "training loss: 2.8796684741973877\n",
      "step 29\n",
      "training loss: 2.863495349884033\n",
      "step 30\n",
      "training loss: 2.767885446548462\n",
      "validation loss: 2.8424696922302246\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.7110109329223633\n",
      "step 32\n",
      "training loss: 2.8169796466827393\n",
      "step 33\n",
      "training loss: 2.88649320602417\n",
      "step 34\n",
      "training loss: 2.8693735599517822\n",
      "step 35\n",
      "training loss: 2.8597748279571533\n",
      "step 36\n",
      "training loss: 2.861935615539551\n",
      "step 37\n",
      "training loss: 2.8761370182037354\n",
      "step 38\n",
      "training loss: 2.877393960952759\n",
      "step 39\n",
      "training loss: 2.8797366619110107\n",
      "step 40\n",
      "training loss: 2.8673834800720215\n",
      "validation loss: 2.8568155765533447\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.881573438644409\n",
      "step 42\n",
      "training loss: 2.8791143894195557\n",
      "step 43\n",
      "training loss: 2.8375837802886963\n",
      "step 44\n",
      "training loss: 2.845111608505249\n",
      "step 45\n",
      "training loss: 2.8580524921417236\n",
      "step 46\n",
      "training loss: 2.8583858013153076\n",
      "step 47\n",
      "training loss: 2.874189615249634\n",
      "step 48\n",
      "training loss: 2.869136095046997\n",
      "step 49\n",
      "training loss: 2.872734785079956\n",
      "step 50\n",
      "training loss: 2.87593674659729\n",
      "validation loss: 2.885404348373413\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8680641651153564\n",
      "step 52\n",
      "training loss: 2.886890411376953\n",
      "step 53\n",
      "training loss: 2.866096258163452\n",
      "step 54\n",
      "training loss: 2.870331287384033\n",
      "step 55\n",
      "training loss: 2.8818511962890625\n",
      "step 56\n",
      "training loss: 2.8760898113250732\n",
      "step 57\n",
      "training loss: 2.872391700744629\n",
      "step 58\n",
      "training loss: 2.8736257553100586\n",
      "step 59\n",
      "training loss: 2.858731508255005\n",
      "step 60\n",
      "training loss: 2.834217071533203\n",
      "validation loss: 2.8795623779296875\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8602864742279053\n",
      "step 62\n",
      "training loss: 2.873073101043701\n",
      "step 63\n",
      "training loss: 2.8437511920928955\n",
      "step 64\n",
      "training loss: 2.8620452880859375\n",
      "step 65\n",
      "training loss: 2.8606460094451904\n",
      "step 66\n",
      "training loss: 2.8482484817504883\n",
      "step 67\n",
      "training loss: 2.882232666015625\n",
      "step 68\n",
      "training loss: 2.853370189666748\n",
      "step 69\n",
      "training loss: 2.870065689086914\n",
      "step 70\n",
      "training loss: 2.855356454849243\n",
      "validation loss: 2.866046190261841\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8686575889587402\n",
      "step 72\n",
      "training loss: 2.8248980045318604\n",
      "step 73\n",
      "training loss: 2.8822498321533203\n",
      "step 74\n",
      "training loss: 2.8493027687072754\n",
      "step 75\n",
      "training loss: 2.8821823596954346\n",
      "step 76\n",
      "training loss: 2.864513397216797\n",
      "step 77\n",
      "training loss: 2.845303535461426\n",
      "step 78\n",
      "training loss: 2.8410956859588623\n",
      "step 79\n",
      "training loss: 2.8613789081573486\n",
      "step 80\n",
      "training loss: 2.8514368534088135\n",
      "validation loss: 2.9439120292663574\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8416967391967773\n",
      "step 82\n",
      "training loss: 2.8801968097686768\n",
      "step 83\n",
      "training loss: 2.8660695552825928\n",
      "step 84\n",
      "training loss: 2.857940435409546\n",
      "step 85\n",
      "training loss: 2.842272996902466\n",
      "step 86\n",
      "training loss: 2.854994773864746\n",
      "step 87\n",
      "training loss: 2.8577721118927\n",
      "step 88\n",
      "training loss: 2.833065986633301\n",
      "step 89\n",
      "training loss: 2.849331855773926\n",
      "step 90\n",
      "training loss: 2.850081205368042\n",
      "validation loss: 2.8780112266540527\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8447537422180176\n",
      "step 92\n",
      "training loss: 2.835712432861328\n",
      "step 93\n",
      "training loss: 2.870532274246216\n",
      "step 94\n",
      "training loss: 2.842480421066284\n",
      "step 95\n",
      "training loss: 2.8769164085388184\n",
      "step 96\n",
      "training loss: 2.858020782470703\n",
      "step 97\n",
      "training loss: 2.8376517295837402\n",
      "step 98\n",
      "training loss: 2.8734164237976074\n",
      "step 99\n",
      "training loss: 2.8488035202026367\n",
      "step 100\n",
      "training loss: 2.8565051555633545\n",
      "validation loss: 2.8400497436523438\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.863229990005493\n",
      "step 102\n",
      "training loss: 2.833847761154175\n",
      "step 103\n",
      "training loss: 2.875154733657837\n",
      "step 104\n",
      "training loss: 2.8550968170166016\n",
      "step 105\n",
      "training loss: 2.875377893447876\n",
      "step 106\n",
      "training loss: 2.8690643310546875\n",
      "step 107\n",
      "training loss: 2.892794370651245\n",
      "step 108\n",
      "training loss: 2.878884792327881\n",
      "step 109\n",
      "training loss: 2.8333041667938232\n",
      "step 110\n",
      "training loss: 2.8698034286499023\n",
      "validation loss: 2.8628246784210205\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8578596115112305\n",
      "step 112\n",
      "training loss: 2.8472678661346436\n",
      "step 113\n",
      "training loss: 2.8580245971679688\n",
      "step 114\n",
      "training loss: 2.856417417526245\n",
      "step 115\n",
      "training loss: 2.874691963195801\n",
      "step 116\n",
      "training loss: 2.857365369796753\n",
      "step 117\n",
      "training loss: 2.875741958618164\n",
      "step 118\n",
      "training loss: 2.8734307289123535\n",
      "step 119\n",
      "training loss: 2.879059314727783\n",
      "step 120\n",
      "training loss: 2.854836940765381\n",
      "validation loss: 2.896211624145508\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8666892051696777\n",
      "step 122\n",
      "training loss: 2.8625600337982178\n",
      "step 123\n",
      "training loss: 2.856077194213867\n",
      "step 124\n",
      "training loss: 2.8664474487304688\n",
      "step 125\n",
      "training loss: 2.841383695602417\n",
      "step 126\n",
      "training loss: 2.867459297180176\n",
      "step 127\n",
      "training loss: 2.8721275329589844\n",
      "step 128\n",
      "training loss: 2.814858913421631\n",
      "step 129\n",
      "training loss: 2.8542988300323486\n",
      "step 130\n",
      "training loss: 2.8508036136627197\n",
      "validation loss: 2.8253867626190186\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8636269569396973\n",
      "step 132\n",
      "training loss: 2.8719358444213867\n",
      "step 133\n",
      "training loss: 2.8422446250915527\n",
      "step 134\n",
      "training loss: 2.8622241020202637\n",
      "step 135\n",
      "training loss: 2.814964532852173\n",
      "step 136\n",
      "training loss: 2.8478341102600098\n",
      "step 137\n",
      "training loss: 2.874213457107544\n",
      "step 138\n",
      "training loss: 2.866264820098877\n",
      "step 139\n",
      "training loss: 2.8563716411590576\n",
      "step 140\n",
      "training loss: 2.847485065460205\n",
      "validation loss: 2.8542444705963135\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8626880645751953\n",
      "step 142\n",
      "training loss: 2.852377414703369\n",
      "step 143\n",
      "training loss: 2.864053726196289\n",
      "step 144\n",
      "training loss: 2.8642780780792236\n",
      "step 145\n",
      "training loss: 2.863302230834961\n",
      "step 146\n",
      "training loss: 2.8574094772338867\n",
      "step 147\n",
      "training loss: 2.8593289852142334\n",
      "step 148\n",
      "training loss: 2.8343183994293213\n",
      "step 149\n",
      "training loss: 2.840067148208618\n",
      "step 150\n",
      "training loss: 2.8298397064208984\n",
      "validation loss: 2.8624427318573\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.864506721496582\n",
      "step 152\n",
      "training loss: 2.8684048652648926\n",
      "step 153\n",
      "training loss: 2.8610124588012695\n",
      "step 154\n",
      "training loss: 2.8586888313293457\n",
      "step 155\n",
      "training loss: 2.852721691131592\n",
      "step 156\n",
      "training loss: 2.8689770698547363\n",
      "step 157\n",
      "training loss: 2.8867673873901367\n",
      "step 158\n",
      "training loss: 2.8108270168304443\n",
      "step 159\n",
      "training loss: 2.8584048748016357\n",
      "step 160\n",
      "training loss: 2.8511457443237305\n",
      "validation loss: 2.868631362915039\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.874688148498535\n",
      "step 162\n",
      "training loss: 2.8567585945129395\n",
      "step 163\n",
      "training loss: 2.87849497795105\n",
      "step 164\n",
      "training loss: 2.8517353534698486\n",
      "step 165\n",
      "training loss: 2.8418610095977783\n",
      "step 166\n",
      "training loss: 2.8524162769317627\n",
      "step 167\n",
      "training loss: 2.8654541969299316\n",
      "step 168\n",
      "training loss: 2.851119041442871\n",
      "step 169\n",
      "training loss: 2.8738436698913574\n",
      "step 170\n",
      "training loss: 2.8609557151794434\n",
      "validation loss: 2.90907621383667\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.825070858001709\n",
      "step 172\n",
      "training loss: 2.8514299392700195\n",
      "step 173\n",
      "training loss: 2.851328134536743\n",
      "step 174\n",
      "training loss: 2.8506245613098145\n",
      "step 175\n",
      "training loss: 2.871612548828125\n",
      "step 176\n",
      "training loss: 2.864758014678955\n",
      "step 177\n",
      "training loss: 2.8655896186828613\n",
      "step 178\n",
      "training loss: 2.8127665519714355\n",
      "step 179\n",
      "training loss: 2.8644309043884277\n",
      "step 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8570356369018555\n",
      "validation loss: 2.864009380340576\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.863123655319214\n",
      "step 182\n",
      "training loss: 2.8643739223480225\n",
      "step 183\n",
      "training loss: 2.8739476203918457\n",
      "step 184\n",
      "training loss: 2.8676276206970215\n",
      "step 185\n",
      "training loss: 2.8524303436279297\n",
      "step 186\n",
      "training loss: 2.878549575805664\n",
      "step 187\n",
      "training loss: 2.874939441680908\n",
      "step 188\n",
      "training loss: 2.8644325733184814\n",
      "step 189\n",
      "training loss: 2.860069990158081\n",
      "step 190\n",
      "training loss: 2.8680527210235596\n",
      "validation loss: 2.8591787815093994\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8582799434661865\n",
      "step 192\n",
      "training loss: 2.858106851577759\n",
      "step 193\n",
      "training loss: 2.841233968734741\n",
      "step 194\n",
      "training loss: 2.822082757949829\n",
      "step 195\n",
      "training loss: 2.868927240371704\n",
      "step 196\n",
      "training loss: 2.849254608154297\n",
      "step 197\n",
      "training loss: 2.8474209308624268\n",
      "step 198\n",
      "training loss: 2.862457752227783\n",
      "step 199\n",
      "training loss: 2.8919003009796143\n",
      "step 200\n",
      "training loss: 2.841648817062378\n",
      "validation loss: 2.823490858078003\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.870415687561035\n",
      "step 202\n",
      "training loss: 2.862649917602539\n",
      "step 203\n",
      "training loss: 2.8686115741729736\n",
      "step 204\n",
      "training loss: 2.848870277404785\n",
      "step 205\n",
      "training loss: 2.8568050861358643\n",
      "step 206\n",
      "training loss: 2.834873676300049\n",
      "step 207\n",
      "training loss: 2.8504860401153564\n",
      "step 208\n",
      "training loss: 2.858764886856079\n",
      "step 209\n",
      "training loss: 2.8445067405700684\n",
      "step 210\n",
      "training loss: 2.884410858154297\n",
      "validation loss: 2.831129789352417\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8579633235931396\n",
      "step 212\n",
      "training loss: 2.847093105316162\n",
      "step 213\n",
      "training loss: 2.861109733581543\n",
      "step 214\n",
      "training loss: 2.868185520172119\n",
      "step 215\n",
      "training loss: 2.851816177368164\n",
      "step 216\n",
      "training loss: 2.8402860164642334\n",
      "step 217\n",
      "training loss: 2.8289804458618164\n",
      "step 218\n",
      "training loss: 2.8133840560913086\n",
      "step 219\n",
      "training loss: 2.875577211380005\n",
      "step 220\n",
      "training loss: 2.8530280590057373\n",
      "validation loss: 2.829540491104126\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.878953456878662\n",
      "step 222\n",
      "training loss: 2.853799343109131\n",
      "step 223\n",
      "training loss: 2.8633828163146973\n",
      "step 224\n",
      "training loss: 2.880274534225464\n",
      "step 225\n",
      "training loss: 2.858309030532837\n",
      "step 226\n",
      "training loss: 2.8510029315948486\n",
      "step 227\n",
      "training loss: 2.864366292953491\n",
      "step 228\n",
      "training loss: 2.842554807662964\n",
      "step 229\n",
      "training loss: 2.853423595428467\n",
      "step 230\n",
      "training loss: 2.8469972610473633\n",
      "validation loss: 2.85687518119812\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8475849628448486\n",
      "step 232\n",
      "training loss: 2.8327548503875732\n",
      "step 233\n",
      "training loss: 2.857872724533081\n",
      "step 234\n",
      "training loss: 2.858785629272461\n",
      "step 235\n",
      "training loss: 2.844764471054077\n",
      "step 236\n",
      "training loss: 2.8728082180023193\n",
      "step 237\n",
      "training loss: 2.8507375717163086\n",
      "step 238\n",
      "training loss: 2.8608133792877197\n",
      "step 239\n",
      "training loss: 2.8526480197906494\n",
      "step 240\n",
      "training loss: 2.865433931350708\n",
      "validation loss: 2.8626630306243896\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.7918448448181152\n",
      "step 242\n",
      "training loss: 2.850037097930908\n",
      "step 243\n",
      "training loss: 2.832364082336426\n",
      "step 244\n",
      "training loss: 2.8394925594329834\n",
      "step 245\n",
      "training loss: 2.8677713871002197\n",
      "step 246\n",
      "training loss: 2.870028018951416\n",
      "step 247\n",
      "training loss: 2.8631632328033447\n",
      "step 248\n",
      "training loss: 2.8603594303131104\n",
      "step 249\n",
      "training loss: 2.860119342803955\n",
      "step 250\n",
      "training loss: 2.849214792251587\n",
      "validation loss: 2.8584771156311035\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8552865982055664\n",
      "step 252\n",
      "training loss: 2.8673760890960693\n",
      "step 253\n",
      "training loss: 2.883636713027954\n",
      "step 254\n",
      "training loss: 2.8556599617004395\n",
      "step 255\n",
      "training loss: 2.8456625938415527\n",
      "step 256\n",
      "training loss: 2.8483684062957764\n",
      "step 257\n",
      "training loss: 2.847416639328003\n",
      "step 258\n",
      "training loss: 2.8576629161834717\n",
      "step 259\n",
      "training loss: 2.8611974716186523\n",
      "step 260\n",
      "training loss: 2.851957321166992\n",
      "validation loss: 2.856754779815674\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.864788770675659\n",
      "step 262\n",
      "training loss: 2.872166156768799\n",
      "step 263\n",
      "training loss: 2.848470449447632\n",
      "step 264\n",
      "training loss: 2.8736915588378906\n",
      "step 265\n",
      "training loss: 2.876713514328003\n",
      "step 266\n",
      "training loss: 2.8468680381774902\n",
      "step 267\n",
      "training loss: 2.8486387729644775\n",
      "step 268\n",
      "training loss: 2.869906425476074\n",
      "step 269\n",
      "training loss: 2.8635876178741455\n",
      "step 270\n",
      "training loss: 2.862131357192993\n",
      "validation loss: 2.8584048748016357\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8650457859039307\n",
      "----------3.0 min per epoch----------\n",
      "epoch 33\n",
      "step 0\n",
      "training loss: 2.8666439056396484\n",
      "validation loss: 2.837864875793457\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8571348190307617\n",
      "step 2\n",
      "training loss: 2.8651745319366455\n",
      "step 3\n",
      "training loss: 2.860788106918335\n",
      "step 4\n",
      "training loss: 2.853414297103882\n",
      "step 5\n",
      "training loss: 2.8537139892578125\n",
      "step 6\n",
      "training loss: 2.817384719848633\n",
      "step 7\n",
      "training loss: 2.8539204597473145\n",
      "step 8\n",
      "training loss: 2.8702855110168457\n",
      "step 9\n",
      "training loss: 2.8447935581207275\n",
      "step 10\n",
      "training loss: 2.8469502925872803\n",
      "validation loss: 2.8381505012512207\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8833394050598145\n",
      "step 12\n",
      "training loss: 2.8566653728485107\n",
      "step 13\n",
      "training loss: 2.8803999423980713\n",
      "step 14\n",
      "training loss: 2.878757953643799\n",
      "step 15\n",
      "training loss: 2.865309238433838\n",
      "step 16\n",
      "training loss: 2.8461122512817383\n",
      "step 17\n",
      "training loss: 2.87485671043396\n",
      "step 18\n",
      "training loss: 2.8756461143493652\n",
      "step 19\n",
      "training loss: 2.867861032485962\n",
      "step 20\n",
      "training loss: 2.8624675273895264\n",
      "validation loss: 2.830069065093994\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.874915599822998\n",
      "step 22\n",
      "training loss: 2.882582187652588\n",
      "step 23\n",
      "training loss: 2.802788257598877\n",
      "step 24\n",
      "training loss: 2.8463943004608154\n",
      "step 25\n",
      "training loss: 2.7955355644226074\n",
      "step 26\n",
      "training loss: 2.8350625038146973\n",
      "step 27\n",
      "training loss: 2.8759591579437256\n",
      "step 28\n",
      "training loss: 2.872183322906494\n",
      "step 29\n",
      "training loss: 2.8804805278778076\n",
      "step 30\n",
      "training loss: 2.8662796020507812\n",
      "validation loss: 2.8238792419433594\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.7645998001098633\n",
      "step 32\n",
      "training loss: 2.7074568271636963\n",
      "step 33\n",
      "training loss: 2.815048933029175\n",
      "step 34\n",
      "training loss: 2.8809874057769775\n",
      "step 35\n",
      "training loss: 2.8689916133880615\n",
      "step 36\n",
      "training loss: 2.8551621437072754\n",
      "step 37\n",
      "training loss: 2.863524913787842\n",
      "step 38\n",
      "training loss: 2.8774945735931396\n",
      "step 39\n",
      "training loss: 2.8758633136749268\n",
      "step 40\n",
      "training loss: 2.8802297115325928\n",
      "validation loss: 2.883676528930664\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.864147663116455\n",
      "step 42\n",
      "training loss: 2.8813390731811523\n",
      "step 43\n",
      "training loss: 2.8795039653778076\n",
      "step 44\n",
      "training loss: 2.8394837379455566\n",
      "step 45\n",
      "training loss: 2.847442865371704\n",
      "step 46\n",
      "training loss: 2.855422019958496\n",
      "step 47\n",
      "training loss: 2.857689380645752\n",
      "step 48\n",
      "training loss: 2.873034954071045\n",
      "step 49\n",
      "training loss: 2.87353253364563\n",
      "step 50\n",
      "training loss: 2.8732588291168213\n",
      "validation loss: 2.878800868988037\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8770861625671387\n",
      "step 52\n",
      "training loss: 2.8667445182800293\n",
      "step 53\n",
      "training loss: 2.8858325481414795\n",
      "step 54\n",
      "training loss: 2.8675551414489746\n",
      "step 55\n",
      "training loss: 2.8713345527648926\n",
      "step 56\n",
      "training loss: 2.880838394165039\n",
      "step 57\n",
      "training loss: 2.874821186065674\n",
      "step 58\n",
      "training loss: 2.8711540699005127\n",
      "step 59\n",
      "training loss: 2.8744125366210938\n",
      "step 60\n",
      "training loss: 2.8577659130096436\n",
      "validation loss: 2.833691358566284\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.835296630859375\n",
      "step 62\n",
      "training loss: 2.860527753829956\n",
      "step 63\n",
      "training loss: 2.8738808631896973\n",
      "step 64\n",
      "training loss: 2.8438804149627686\n",
      "step 65\n",
      "training loss: 2.860924243927002\n",
      "step 66\n",
      "training loss: 2.861107349395752\n",
      "step 67\n",
      "training loss: 2.8516013622283936\n",
      "step 68\n",
      "training loss: 2.8812952041625977\n",
      "step 69\n",
      "training loss: 2.857478141784668\n",
      "step 70\n",
      "training loss: 2.870556354522705\n",
      "validation loss: 2.8540191650390625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.855318546295166\n",
      "step 72\n",
      "training loss: 2.8692710399627686\n",
      "step 73\n",
      "training loss: 2.822071075439453\n",
      "step 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.883711576461792\n",
      "step 75\n",
      "training loss: 2.847109794616699\n",
      "step 76\n",
      "training loss: 2.8804426193237305\n",
      "step 77\n",
      "training loss: 2.864095687866211\n",
      "step 78\n",
      "training loss: 2.846803903579712\n",
      "step 79\n",
      "training loss: 2.844202995300293\n",
      "step 80\n",
      "training loss: 2.8631017208099365\n",
      "validation loss: 2.8852946758270264\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8514721393585205\n",
      "step 82\n",
      "training loss: 2.8448851108551025\n",
      "step 83\n",
      "training loss: 2.876770257949829\n",
      "step 84\n",
      "training loss: 2.867173433303833\n",
      "step 85\n",
      "training loss: 2.859238862991333\n",
      "step 86\n",
      "training loss: 2.8443145751953125\n",
      "step 87\n",
      "training loss: 2.857072353363037\n",
      "step 88\n",
      "training loss: 2.858262062072754\n",
      "step 89\n",
      "training loss: 2.8342983722686768\n",
      "step 90\n",
      "training loss: 2.849125623703003\n",
      "validation loss: 2.8723459243774414\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8478708267211914\n",
      "step 92\n",
      "training loss: 2.8418078422546387\n",
      "step 93\n",
      "training loss: 2.8351104259490967\n",
      "step 94\n",
      "training loss: 2.870709180831909\n",
      "step 95\n",
      "training loss: 2.839236259460449\n",
      "step 96\n",
      "training loss: 2.8783938884735107\n",
      "step 97\n",
      "training loss: 2.859955072402954\n",
      "step 98\n",
      "training loss: 2.8394362926483154\n",
      "step 99\n",
      "training loss: 2.875258207321167\n",
      "step 100\n",
      "training loss: 2.8506739139556885\n",
      "validation loss: 2.8634419441223145\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.858854055404663\n",
      "step 102\n",
      "training loss: 2.863720417022705\n",
      "step 103\n",
      "training loss: 2.830233573913574\n",
      "step 104\n",
      "training loss: 2.87565279006958\n",
      "step 105\n",
      "training loss: 2.855811357498169\n",
      "step 106\n",
      "training loss: 2.8749501705169678\n",
      "step 107\n",
      "training loss: 2.8692753314971924\n",
      "step 108\n",
      "training loss: 2.8933818340301514\n",
      "step 109\n",
      "training loss: 2.8787214756011963\n",
      "step 110\n",
      "training loss: 2.8368289470672607\n",
      "validation loss: 2.937692642211914\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8693394660949707\n",
      "step 112\n",
      "training loss: 2.855954885482788\n",
      "step 113\n",
      "training loss: 2.844766616821289\n",
      "step 114\n",
      "training loss: 2.8570189476013184\n",
      "step 115\n",
      "training loss: 2.8546361923217773\n",
      "step 116\n",
      "training loss: 2.8785009384155273\n",
      "step 117\n",
      "training loss: 2.8581957817077637\n",
      "step 118\n",
      "training loss: 2.8776800632476807\n",
      "step 119\n",
      "training loss: 2.8731865882873535\n",
      "step 120\n",
      "training loss: 2.8781278133392334\n",
      "validation loss: 2.875624418258667\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8524837493896484\n",
      "step 122\n",
      "training loss: 2.8663737773895264\n",
      "step 123\n",
      "training loss: 2.8626303672790527\n",
      "step 124\n",
      "training loss: 2.8546478748321533\n",
      "step 125\n",
      "training loss: 2.8672358989715576\n",
      "step 126\n",
      "training loss: 2.843696355819702\n",
      "step 127\n",
      "training loss: 2.8669471740722656\n",
      "step 128\n",
      "training loss: 2.872607707977295\n",
      "step 129\n",
      "training loss: 2.8126413822174072\n",
      "step 130\n",
      "training loss: 2.8536250591278076\n",
      "validation loss: 2.8449513912200928\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8478221893310547\n",
      "step 132\n",
      "training loss: 2.862515449523926\n",
      "step 133\n",
      "training loss: 2.8709256649017334\n",
      "step 134\n",
      "training loss: 2.8437416553497314\n",
      "step 135\n",
      "training loss: 2.864159107208252\n",
      "step 136\n",
      "training loss: 2.812370538711548\n",
      "step 137\n",
      "training loss: 2.8467671871185303\n",
      "step 138\n",
      "training loss: 2.8731608390808105\n",
      "step 139\n",
      "training loss: 2.863112211227417\n",
      "step 140\n",
      "training loss: 2.8585867881774902\n",
      "validation loss: 2.854689359664917\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8473966121673584\n",
      "step 142\n",
      "training loss: 2.8656649589538574\n",
      "step 143\n",
      "training loss: 2.8515260219573975\n",
      "step 144\n",
      "training loss: 2.8627772331237793\n",
      "step 145\n",
      "training loss: 2.865689516067505\n",
      "step 146\n",
      "training loss: 2.8629446029663086\n",
      "step 147\n",
      "training loss: 2.8564555644989014\n",
      "step 148\n",
      "training loss: 2.858023166656494\n",
      "step 149\n",
      "training loss: 2.8361170291900635\n",
      "step 150\n",
      "training loss: 2.840039014816284\n",
      "validation loss: 2.8957924842834473\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.829700231552124\n",
      "step 152\n",
      "training loss: 2.8648946285247803\n",
      "step 153\n",
      "training loss: 2.8709359169006348\n",
      "step 154\n",
      "training loss: 2.8604636192321777\n",
      "step 155\n",
      "training loss: 2.861725091934204\n",
      "step 156\n",
      "training loss: 2.8532299995422363\n",
      "step 157\n",
      "training loss: 2.8684399127960205\n",
      "step 158\n",
      "training loss: 2.8899285793304443\n",
      "step 159\n",
      "training loss: 2.8081438541412354\n",
      "step 160\n",
      "training loss: 2.8588173389434814\n",
      "validation loss: 2.8206071853637695\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.850710868835449\n",
      "step 162\n",
      "training loss: 2.873898506164551\n",
      "step 163\n",
      "training loss: 2.8562872409820557\n",
      "step 164\n",
      "training loss: 2.878397226333618\n",
      "step 165\n",
      "training loss: 2.8517751693725586\n",
      "step 166\n",
      "training loss: 2.8408145904541016\n",
      "step 167\n",
      "training loss: 2.852727174758911\n",
      "step 168\n",
      "training loss: 2.86333966255188\n",
      "step 169\n",
      "training loss: 2.8518893718719482\n",
      "step 170\n",
      "training loss: 2.872997522354126\n",
      "validation loss: 2.8548293113708496\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.859513998031616\n",
      "step 172\n",
      "training loss: 2.8236682415008545\n",
      "step 173\n",
      "training loss: 2.8500468730926514\n",
      "step 174\n",
      "training loss: 2.8518118858337402\n",
      "step 175\n",
      "training loss: 2.850158452987671\n",
      "step 176\n",
      "training loss: 2.8690173625946045\n",
      "step 177\n",
      "training loss: 2.8634214401245117\n",
      "step 178\n",
      "training loss: 2.865675449371338\n",
      "step 179\n",
      "training loss: 2.811105966567993\n",
      "step 180\n",
      "training loss: 2.861555337905884\n",
      "validation loss: 2.8587775230407715\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8566412925720215\n",
      "step 182\n",
      "training loss: 2.862748622894287\n",
      "step 183\n",
      "training loss: 2.8628439903259277\n",
      "step 184\n",
      "training loss: 2.8711609840393066\n",
      "step 185\n",
      "training loss: 2.8689608573913574\n",
      "step 186\n",
      "training loss: 2.851724147796631\n",
      "step 187\n",
      "training loss: 2.8764281272888184\n",
      "step 188\n",
      "training loss: 2.8757753372192383\n",
      "step 189\n",
      "training loss: 2.8634378910064697\n",
      "step 190\n",
      "training loss: 2.8588502407073975\n",
      "validation loss: 2.864819049835205\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.868612051010132\n",
      "step 192\n",
      "training loss: 2.8589720726013184\n",
      "step 193\n",
      "training loss: 2.8569858074188232\n",
      "step 194\n",
      "training loss: 2.838162660598755\n",
      "step 195\n",
      "training loss: 2.8192436695098877\n",
      "step 196\n",
      "training loss: 2.8694851398468018\n",
      "step 197\n",
      "training loss: 2.846933603286743\n",
      "step 198\n",
      "training loss: 2.845944404602051\n",
      "step 199\n",
      "training loss: 2.861673593521118\n",
      "step 200\n",
      "training loss: 2.8896517753601074\n",
      "validation loss: 2.9083707332611084\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.840420722961426\n",
      "step 202\n",
      "training loss: 2.8688526153564453\n",
      "step 203\n",
      "training loss: 2.8593051433563232\n",
      "step 204\n",
      "training loss: 2.867584228515625\n",
      "step 205\n",
      "training loss: 2.848175287246704\n",
      "step 206\n",
      "training loss: 2.8589060306549072\n",
      "step 207\n",
      "training loss: 2.836322546005249\n",
      "step 208\n",
      "training loss: 2.8485395908355713\n",
      "step 209\n",
      "training loss: 2.8573808670043945\n",
      "step 210\n",
      "training loss: 2.842609405517578\n",
      "validation loss: 2.8646323680877686\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.883518934249878\n",
      "step 212\n",
      "training loss: 2.857527732849121\n",
      "step 213\n",
      "training loss: 2.847440242767334\n",
      "step 214\n",
      "training loss: 2.8603053092956543\n",
      "step 215\n",
      "training loss: 2.8670289516448975\n",
      "step 216\n",
      "training loss: 2.8523318767547607\n",
      "step 217\n",
      "training loss: 2.839923143386841\n",
      "step 218\n",
      "training loss: 2.8287696838378906\n",
      "step 219\n",
      "training loss: 2.812006711959839\n",
      "step 220\n",
      "training loss: 2.8748676776885986\n",
      "validation loss: 2.8599743843078613\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8567588329315186\n",
      "step 222\n",
      "training loss: 2.8776187896728516\n",
      "step 223\n",
      "training loss: 2.853515148162842\n",
      "step 224\n",
      "training loss: 2.8604419231414795\n",
      "step 225\n",
      "training loss: 2.880444288253784\n",
      "step 226\n",
      "training loss: 2.8578383922576904\n",
      "step 227\n",
      "training loss: 2.8482825756073\n",
      "step 228\n",
      "training loss: 2.863396406173706\n",
      "step 229\n",
      "training loss: 2.841878652572632\n",
      "step 230\n",
      "training loss: 2.8529152870178223\n",
      "validation loss: 2.823704957962036\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.84521746635437\n",
      "step 232\n",
      "training loss: 2.8450100421905518\n",
      "step 233\n",
      "training loss: 2.8337299823760986\n",
      "step 234\n",
      "training loss: 2.856210947036743\n",
      "step 235\n",
      "training loss: 2.857905864715576\n",
      "step 236\n",
      "training loss: 2.844510555267334\n",
      "step 237\n",
      "training loss: 2.8728253841400146\n",
      "step 238\n",
      "training loss: 2.8483564853668213\n",
      "step 239\n",
      "training loss: 2.8596277236938477\n",
      "step 240\n",
      "training loss: 2.847618579864502\n",
      "validation loss: 2.831397294998169\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8645637035369873\n",
      "step 242\n",
      "training loss: 2.791351318359375\n",
      "step 243\n",
      "training loss: 2.847930908203125\n",
      "step 244\n",
      "training loss: 2.834197521209717\n",
      "step 245\n",
      "training loss: 2.8445045948028564\n",
      "step 246\n",
      "training loss: 2.868563175201416\n",
      "step 247\n",
      "training loss: 2.8712735176086426\n",
      "step 248\n",
      "training loss: 2.858983039855957\n",
      "step 249\n",
      "training loss: 2.8622920513153076\n",
      "step 250\n",
      "training loss: 2.858715295791626\n",
      "validation loss: 2.8166136741638184\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8470559120178223\n",
      "step 252\n",
      "training loss: 2.853198528289795\n",
      "step 253\n",
      "training loss: 2.8701882362365723\n",
      "step 254\n",
      "training loss: 2.8860342502593994\n",
      "step 255\n",
      "training loss: 2.8541109561920166\n",
      "step 256\n",
      "training loss: 2.8425161838531494\n",
      "step 257\n",
      "training loss: 2.8474843502044678\n",
      "step 258\n",
      "training loss: 2.844682216644287\n",
      "step 259\n",
      "training loss: 2.8603806495666504\n",
      "step 260\n",
      "training loss: 2.860314130783081\n",
      "validation loss: 2.855701208114624\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8493895530700684\n",
      "step 262\n",
      "training loss: 2.8626465797424316\n",
      "step 263\n",
      "training loss: 2.8721296787261963\n",
      "step 264\n",
      "training loss: 2.845698595046997\n",
      "step 265\n",
      "training loss: 2.8709540367126465\n",
      "step 266\n",
      "training loss: 2.8758814334869385\n",
      "step 267\n",
      "training loss: 2.845059394836426\n",
      "step 268\n",
      "training loss: 2.8472025394439697\n",
      "step 269\n",
      "training loss: 2.8710484504699707\n",
      "step 270\n",
      "training loss: 2.8629097938537598\n",
      "validation loss: 2.861384391784668\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8609588146209717\n",
      "----------3.0 min per epoch----------\n",
      "epoch 34\n",
      "step 0\n",
      "training loss: 2.8631722927093506\n",
      "validation loss: 2.856539011001587\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.86651349067688\n",
      "step 2\n",
      "training loss: 2.855952262878418\n",
      "step 3\n",
      "training loss: 2.864027976989746\n",
      "step 4\n",
      "training loss: 2.8600218296051025\n",
      "step 5\n",
      "training loss: 2.8512284755706787\n",
      "step 6\n",
      "training loss: 2.849210023880005\n",
      "step 7\n",
      "training loss: 2.8147740364074707\n",
      "step 8\n",
      "training loss: 2.8511786460876465\n",
      "step 9\n",
      "training loss: 2.870823621749878\n",
      "step 10\n",
      "training loss: 2.8384270668029785\n",
      "validation loss: 2.857736110687256\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8419909477233887\n",
      "step 12\n",
      "training loss: 2.8813107013702393\n",
      "step 13\n",
      "training loss: 2.8570969104766846\n",
      "step 14\n",
      "training loss: 2.877474069595337\n",
      "step 15\n",
      "training loss: 2.878082036972046\n",
      "step 16\n",
      "training loss: 2.8659441471099854\n",
      "step 17\n",
      "training loss: 2.844054937362671\n",
      "step 18\n",
      "training loss: 2.8749852180480957\n",
      "step 19\n",
      "training loss: 2.8757712841033936\n",
      "step 20\n",
      "training loss: 2.867128610610962\n",
      "validation loss: 2.857682228088379\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.863781213760376\n",
      "step 22\n",
      "training loss: 2.873750686645508\n",
      "step 23\n",
      "training loss: 2.883582830429077\n",
      "step 24\n",
      "training loss: 2.80184268951416\n",
      "step 25\n",
      "training loss: 2.8443808555603027\n",
      "step 26\n",
      "training loss: 2.7857632637023926\n",
      "step 27\n",
      "training loss: 2.836397409439087\n",
      "step 28\n",
      "training loss: 2.879040002822876\n",
      "step 29\n",
      "training loss: 2.8695108890533447\n",
      "step 30\n",
      "training loss: 2.880530834197998\n",
      "validation loss: 2.8391592502593994\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8611059188842773\n",
      "step 32\n",
      "training loss: 2.75093150138855\n",
      "step 33\n",
      "training loss: 2.6930198669433594\n",
      "step 34\n",
      "training loss: 2.8124654293060303\n",
      "step 35\n",
      "training loss: 2.8850789070129395\n",
      "step 36\n",
      "training loss: 2.8691630363464355\n",
      "step 37\n",
      "training loss: 2.8539175987243652\n",
      "step 38\n",
      "training loss: 2.8599891662597656\n",
      "step 39\n",
      "training loss: 2.873082399368286\n",
      "step 40\n",
      "training loss: 2.8759331703186035\n",
      "validation loss: 2.8463480472564697\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8796207904815674\n",
      "step 42\n",
      "training loss: 2.864288806915283\n",
      "step 43\n",
      "training loss: 2.8807623386383057\n",
      "step 44\n",
      "training loss: 2.878021240234375\n",
      "step 45\n",
      "training loss: 2.837448835372925\n",
      "step 46\n",
      "training loss: 2.84002423286438\n",
      "step 47\n",
      "training loss: 2.855131149291992\n",
      "step 48\n",
      "training loss: 2.858818292617798\n",
      "step 49\n",
      "training loss: 2.87324857711792\n",
      "step 50\n",
      "training loss: 2.8686583042144775\n",
      "validation loss: 2.8379404544830322\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8719279766082764\n",
      "step 52\n",
      "training loss: 2.879002094268799\n",
      "step 53\n",
      "training loss: 2.8680241107940674\n",
      "step 54\n",
      "training loss: 2.8855934143066406\n",
      "step 55\n",
      "training loss: 2.8630995750427246\n",
      "step 56\n",
      "training loss: 2.8724875450134277\n",
      "step 57\n",
      "training loss: 2.8799550533294678\n",
      "step 58\n",
      "training loss: 2.8755147457122803\n",
      "step 59\n",
      "training loss: 2.872910976409912\n",
      "step 60\n",
      "training loss: 2.8750710487365723\n",
      "validation loss: 2.8396377563476562\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.859496593475342\n",
      "step 62\n",
      "training loss: 2.8355250358581543\n",
      "step 63\n",
      "training loss: 2.862917900085449\n",
      "step 64\n",
      "training loss: 2.8710691928863525\n",
      "step 65\n",
      "training loss: 2.838517189025879\n",
      "step 66\n",
      "training loss: 2.8594841957092285\n",
      "step 67\n",
      "training loss: 2.859957695007324\n",
      "step 68\n",
      "training loss: 2.846975803375244\n",
      "step 69\n",
      "training loss: 2.8803505897521973\n",
      "step 70\n",
      "training loss: 2.85363507270813\n",
      "validation loss: 2.873718738555908\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.870154619216919\n",
      "step 72\n",
      "training loss: 2.853484869003296\n",
      "step 73\n",
      "training loss: 2.8683767318725586\n",
      "step 74\n",
      "training loss: 2.824946403503418\n",
      "step 75\n",
      "training loss: 2.8833892345428467\n",
      "step 76\n",
      "training loss: 2.8447115421295166\n",
      "step 77\n",
      "training loss: 2.8786516189575195\n",
      "step 78\n",
      "training loss: 2.8645529747009277\n",
      "step 79\n",
      "training loss: 2.8440287113189697\n",
      "step 80\n",
      "training loss: 2.8435449600219727\n",
      "validation loss: 2.87347149848938\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8608288764953613\n",
      "step 82\n",
      "training loss: 2.8500614166259766\n",
      "step 83\n",
      "training loss: 2.8387532234191895\n",
      "step 84\n",
      "training loss: 2.8788986206054688\n",
      "step 85\n",
      "training loss: 2.864128828048706\n",
      "step 86\n",
      "training loss: 2.857327938079834\n",
      "step 87\n",
      "training loss: 2.842789888381958\n",
      "step 88\n",
      "training loss: 2.8568267822265625\n",
      "step 89\n",
      "training loss: 2.8571584224700928\n",
      "step 90\n",
      "training loss: 2.8323845863342285\n",
      "validation loss: 2.828701972961426\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8478355407714844\n",
      "step 92\n",
      "training loss: 2.846604108810425\n",
      "step 93\n",
      "training loss: 2.8435802459716797\n",
      "step 94\n",
      "training loss: 2.835005283355713\n",
      "step 95\n",
      "training loss: 2.8697776794433594\n",
      "step 96\n",
      "training loss: 2.842442750930786\n",
      "step 97\n",
      "training loss: 2.8766331672668457\n",
      "step 98\n",
      "training loss: 2.8567140102386475\n",
      "step 99\n",
      "training loss: 2.8371548652648926\n",
      "step 100\n",
      "training loss: 2.87418532371521\n",
      "validation loss: 2.8507750034332275\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.850337505340576\n",
      "step 102\n",
      "training loss: 2.857978105545044\n",
      "step 103\n",
      "training loss: 2.8632149696350098\n",
      "step 104\n",
      "training loss: 2.830716133117676\n",
      "step 105\n",
      "training loss: 2.8753302097320557\n",
      "step 106\n",
      "training loss: 2.851999521255493\n",
      "step 107\n",
      "training loss: 2.8748562335968018\n",
      "step 108\n",
      "training loss: 2.870439291000366\n",
      "step 109\n",
      "training loss: 2.8921051025390625\n",
      "step 110\n",
      "training loss: 2.876997470855713\n",
      "validation loss: 2.873396158218384\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8346221446990967\n",
      "step 112\n",
      "training loss: 2.870333194732666\n",
      "step 113\n",
      "training loss: 2.854471206665039\n",
      "step 114\n",
      "training loss: 2.8458054065704346\n",
      "step 115\n",
      "training loss: 2.854748010635376\n",
      "step 116\n",
      "training loss: 2.855962038040161\n",
      "step 117\n",
      "training loss: 2.8766162395477295\n",
      "step 118\n",
      "training loss: 2.860398054122925\n",
      "step 119\n",
      "training loss: 2.8748021125793457\n",
      "step 120\n",
      "training loss: 2.8746302127838135\n",
      "validation loss: 2.8730616569519043\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.878671407699585\n",
      "step 122\n",
      "training loss: 2.8544931411743164\n",
      "step 123\n",
      "training loss: 2.8665361404418945\n",
      "step 124\n",
      "training loss: 2.862971305847168\n",
      "step 125\n",
      "training loss: 2.853874683380127\n",
      "step 126\n",
      "training loss: 2.8656527996063232\n",
      "step 127\n",
      "training loss: 2.841484785079956\n",
      "step 128\n",
      "training loss: 2.8662962913513184\n",
      "step 129\n",
      "training loss: 2.87080979347229\n",
      "step 130\n",
      "training loss: 2.8131866455078125\n",
      "validation loss: 2.865945339202881\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8534719944000244\n",
      "step 132\n",
      "training loss: 2.849477767944336\n",
      "step 133\n",
      "training loss: 2.862006187438965\n",
      "step 134\n",
      "training loss: 2.8718273639678955\n",
      "step 135\n",
      "training loss: 2.8429267406463623\n",
      "step 136\n",
      "training loss: 2.863450050354004\n",
      "step 137\n",
      "training loss: 2.8125064373016357\n",
      "step 138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8478312492370605\n",
      "step 139\n",
      "training loss: 2.872283935546875\n",
      "step 140\n",
      "training loss: 2.8654415607452393\n",
      "validation loss: 2.937892436981201\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.855825424194336\n",
      "step 142\n",
      "training loss: 2.8466930389404297\n",
      "step 143\n",
      "training loss: 2.8653481006622314\n",
      "step 144\n",
      "training loss: 2.8513267040252686\n",
      "step 145\n",
      "training loss: 2.859710693359375\n",
      "step 146\n",
      "training loss: 2.8664968013763428\n",
      "step 147\n",
      "training loss: 2.8612515926361084\n",
      "step 148\n",
      "training loss: 2.8566439151763916\n",
      "step 149\n",
      "training loss: 2.8580026626586914\n",
      "step 150\n",
      "training loss: 2.8324201107025146\n",
      "validation loss: 2.875467300415039\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8401505947113037\n",
      "step 152\n",
      "training loss: 2.832828998565674\n",
      "step 153\n",
      "training loss: 2.8660292625427246\n",
      "step 154\n",
      "training loss: 2.8674654960632324\n",
      "step 155\n",
      "training loss: 2.8616738319396973\n",
      "step 156\n",
      "training loss: 2.860337495803833\n",
      "step 157\n",
      "training loss: 2.8516159057617188\n",
      "step 158\n",
      "training loss: 2.8685107231140137\n",
      "step 159\n",
      "training loss: 2.88858962059021\n",
      "step 160\n",
      "training loss: 2.8077211380004883\n",
      "validation loss: 2.8450675010681152\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8580520153045654\n",
      "step 162\n",
      "training loss: 2.851797103881836\n",
      "step 163\n",
      "training loss: 2.8739378452301025\n",
      "step 164\n",
      "training loss: 2.8559744358062744\n",
      "step 165\n",
      "training loss: 2.8774209022521973\n",
      "step 166\n",
      "training loss: 2.8528313636779785\n",
      "step 167\n",
      "training loss: 2.8398399353027344\n",
      "step 168\n",
      "training loss: 2.851854085922241\n",
      "step 169\n",
      "training loss: 2.8650355339050293\n",
      "step 170\n",
      "training loss: 2.851278305053711\n",
      "validation loss: 2.861638069152832\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.87262225151062\n",
      "step 172\n",
      "training loss: 2.8595902919769287\n",
      "step 173\n",
      "training loss: 2.821593999862671\n",
      "step 174\n",
      "training loss: 2.8493475914001465\n",
      "step 175\n",
      "training loss: 2.8521270751953125\n",
      "step 176\n",
      "training loss: 2.8502774238586426\n",
      "step 177\n",
      "training loss: 2.8684659004211426\n",
      "step 178\n",
      "training loss: 2.861898422241211\n",
      "step 179\n",
      "training loss: 2.865297555923462\n",
      "step 180\n",
      "training loss: 2.811436176300049\n",
      "validation loss: 2.886549711227417\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8615636825561523\n",
      "step 182\n",
      "training loss: 2.853759765625\n",
      "step 183\n",
      "training loss: 2.858717679977417\n",
      "step 184\n",
      "training loss: 2.859647274017334\n",
      "step 185\n",
      "training loss: 2.8749566078186035\n",
      "step 186\n",
      "training loss: 2.8687052726745605\n",
      "step 187\n",
      "training loss: 2.8513598442077637\n",
      "step 188\n",
      "training loss: 2.877134084701538\n",
      "step 189\n",
      "training loss: 2.8733930587768555\n",
      "step 190\n",
      "training loss: 2.8626232147216797\n",
      "validation loss: 2.816948175430298\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.856807231903076\n",
      "step 192\n",
      "training loss: 2.8676586151123047\n",
      "step 193\n",
      "training loss: 2.859548807144165\n",
      "step 194\n",
      "training loss: 2.8559072017669678\n",
      "step 195\n",
      "training loss: 2.8385493755340576\n",
      "step 196\n",
      "training loss: 2.8216421604156494\n",
      "step 197\n",
      "training loss: 2.8700928688049316\n",
      "step 198\n",
      "training loss: 2.846031665802002\n",
      "step 199\n",
      "training loss: 2.84507155418396\n",
      "step 200\n",
      "training loss: 2.859105348587036\n",
      "validation loss: 2.8555848598480225\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.890010118484497\n",
      "step 202\n",
      "training loss: 2.84371280670166\n",
      "step 203\n",
      "training loss: 2.8698182106018066\n",
      "step 204\n",
      "training loss: 2.8630471229553223\n",
      "step 205\n",
      "training loss: 2.8696751594543457\n",
      "step 206\n",
      "training loss: 2.8490521907806396\n",
      "step 207\n",
      "training loss: 2.857581853866577\n",
      "step 208\n",
      "training loss: 2.8349103927612305\n",
      "step 209\n",
      "training loss: 2.848837375640869\n",
      "step 210\n",
      "training loss: 2.856351613998413\n",
      "validation loss: 2.8634560108184814\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8417396545410156\n",
      "step 212\n",
      "training loss: 2.8856518268585205\n",
      "step 213\n",
      "training loss: 2.8597214221954346\n",
      "step 214\n",
      "training loss: 2.8482346534729004\n",
      "step 215\n",
      "training loss: 2.8612759113311768\n",
      "step 216\n",
      "training loss: 2.866014242172241\n",
      "step 217\n",
      "training loss: 2.852003812789917\n",
      "step 218\n",
      "training loss: 2.8417227268218994\n",
      "step 219\n",
      "training loss: 2.827899217605591\n",
      "step 220\n",
      "training loss: 2.8122692108154297\n",
      "validation loss: 2.8682897090911865\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8751437664031982\n",
      "step 222\n",
      "training loss: 2.8516783714294434\n",
      "step 223\n",
      "training loss: 2.8772006034851074\n",
      "step 224\n",
      "training loss: 2.853959560394287\n",
      "step 225\n",
      "training loss: 2.8601765632629395\n",
      "step 226\n",
      "training loss: 2.8829033374786377\n",
      "step 227\n",
      "training loss: 2.8559319972991943\n",
      "step 228\n",
      "training loss: 2.847527027130127\n",
      "step 229\n",
      "training loss: 2.8629345893859863\n",
      "step 230\n",
      "training loss: 2.842271327972412\n",
      "validation loss: 2.9009578227996826\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8534293174743652\n",
      "step 232\n",
      "training loss: 2.8434994220733643\n",
      "step 233\n",
      "training loss: 2.8448266983032227\n",
      "step 234\n",
      "training loss: 2.833679676055908\n",
      "step 235\n",
      "training loss: 2.8562591075897217\n",
      "step 236\n",
      "training loss: 2.859127998352051\n",
      "step 237\n",
      "training loss: 2.846665382385254\n",
      "step 238\n",
      "training loss: 2.8744187355041504\n",
      "step 239\n",
      "training loss: 2.8471615314483643\n",
      "step 240\n",
      "training loss: 2.8607754707336426\n",
      "validation loss: 2.863970994949341\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.846364736557007\n",
      "step 242\n",
      "training loss: 2.864896774291992\n",
      "step 243\n",
      "training loss: 2.7922112941741943\n",
      "step 244\n",
      "training loss: 2.847468852996826\n",
      "step 245\n",
      "training loss: 2.8304800987243652\n",
      "step 246\n",
      "training loss: 2.8388864994049072\n",
      "step 247\n",
      "training loss: 2.867142915725708\n",
      "step 248\n",
      "training loss: 2.868773937225342\n",
      "step 249\n",
      "training loss: 2.861154317855835\n",
      "step 250\n",
      "training loss: 2.860433340072632\n",
      "validation loss: 2.8546276092529297\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8579673767089844\n",
      "step 252\n",
      "training loss: 2.8480257987976074\n",
      "step 253\n",
      "training loss: 2.8546345233917236\n",
      "step 254\n",
      "training loss: 2.871432065963745\n",
      "step 255\n",
      "training loss: 2.8844809532165527\n",
      "step 256\n",
      "training loss: 2.855240821838379\n",
      "step 257\n",
      "training loss: 2.8454277515411377\n",
      "step 258\n",
      "training loss: 2.847202777862549\n",
      "step 259\n",
      "training loss: 2.846456289291382\n",
      "step 260\n",
      "training loss: 2.8581748008728027\n",
      "validation loss: 2.817701816558838\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8603055477142334\n",
      "step 262\n",
      "training loss: 2.851047992706299\n",
      "step 263\n",
      "training loss: 2.864408254623413\n",
      "step 264\n",
      "training loss: 2.871880054473877\n",
      "step 265\n",
      "training loss: 2.846189260482788\n",
      "step 266\n",
      "training loss: 2.8735620975494385\n",
      "step 267\n",
      "training loss: 2.8755812644958496\n",
      "step 268\n",
      "training loss: 2.8453407287597656\n",
      "step 269\n",
      "training loss: 2.8478481769561768\n",
      "step 270\n",
      "training loss: 2.8683996200561523\n",
      "validation loss: 2.832709550857544\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.863433599472046\n",
      "----------3.0 min per epoch----------\n",
      "epoch 35\n",
      "step 0\n",
      "training loss: 2.8622400760650635\n",
      "validation loss: 2.817356824874878\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8637278079986572\n",
      "step 2\n",
      "training loss: 2.866532564163208\n",
      "step 3\n",
      "training loss: 2.8568735122680664\n",
      "step 4\n",
      "training loss: 2.863417387008667\n",
      "step 5\n",
      "training loss: 2.8601248264312744\n",
      "step 6\n",
      "training loss: 2.850001096725464\n",
      "step 7\n",
      "training loss: 2.8503661155700684\n",
      "step 8\n",
      "training loss: 2.812525987625122\n",
      "step 9\n",
      "training loss: 2.8479864597320557\n",
      "step 10\n",
      "training loss: 2.8705015182495117\n",
      "validation loss: 2.8562827110290527\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.840773820877075\n",
      "step 12\n",
      "training loss: 2.8404173851013184\n",
      "step 13\n",
      "training loss: 2.8835971355438232\n",
      "step 14\n",
      "training loss: 2.857356309890747\n",
      "step 15\n",
      "training loss: 2.8786652088165283\n",
      "step 16\n",
      "training loss: 2.8798727989196777\n",
      "step 17\n",
      "training loss: 2.8653950691223145\n",
      "step 18\n",
      "training loss: 2.849085807800293\n",
      "step 19\n",
      "training loss: 2.877845525741577\n",
      "step 20\n",
      "training loss: 2.875147581100464\n",
      "validation loss: 2.8684799671173096\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8677446842193604\n",
      "step 22\n",
      "training loss: 2.8626742362976074\n",
      "step 23\n",
      "training loss: 2.8749914169311523\n",
      "step 24\n",
      "training loss: 2.8824386596679688\n",
      "step 25\n",
      "training loss: 2.801809549331665\n",
      "step 26\n",
      "training loss: 2.8450918197631836\n",
      "step 27\n",
      "training loss: 2.7830376625061035\n",
      "step 28\n",
      "training loss: 2.840318441390991\n",
      "step 29\n",
      "training loss: 2.880483627319336\n",
      "step 30\n",
      "training loss: 2.867396116256714\n",
      "validation loss: 2.8683338165283203\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31\n",
      "training loss: 2.8857545852661133\n",
      "step 32\n",
      "training loss: 2.8693959712982178\n",
      "step 33\n",
      "training loss: 2.7539725303649902\n",
      "step 34\n",
      "training loss: 2.6901683807373047\n",
      "step 35\n",
      "training loss: 2.7986226081848145\n",
      "step 36\n",
      "training loss: 2.8849990367889404\n",
      "step 37\n",
      "training loss: 2.8714756965637207\n",
      "step 38\n",
      "training loss: 2.8540751934051514\n",
      "step 39\n",
      "training loss: 2.863403797149658\n",
      "step 40\n",
      "training loss: 2.874438524246216\n",
      "validation loss: 2.8657476902008057\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.875702381134033\n",
      "step 42\n",
      "training loss: 2.8774163722991943\n",
      "step 43\n",
      "training loss: 2.8618595600128174\n",
      "step 44\n",
      "training loss: 2.88016676902771\n",
      "step 45\n",
      "training loss: 2.8805012702941895\n",
      "step 46\n",
      "training loss: 2.83479642868042\n",
      "step 47\n",
      "training loss: 2.8502070903778076\n",
      "step 48\n",
      "training loss: 2.8562493324279785\n",
      "step 49\n",
      "training loss: 2.8595452308654785\n",
      "step 50\n",
      "training loss: 2.8729255199432373\n",
      "validation loss: 2.8657212257385254\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8669586181640625\n",
      "step 52\n",
      "training loss: 2.8710834980010986\n",
      "step 53\n",
      "training loss: 2.8750500679016113\n",
      "step 54\n",
      "training loss: 2.8673880100250244\n",
      "step 55\n",
      "training loss: 2.8868746757507324\n",
      "step 56\n",
      "training loss: 2.86599063873291\n",
      "step 57\n",
      "training loss: 2.8716061115264893\n",
      "step 58\n",
      "training loss: 2.881375312805176\n",
      "step 59\n",
      "training loss: 2.8741838932037354\n",
      "step 60\n",
      "training loss: 2.8737831115722656\n",
      "validation loss: 2.8447391986846924\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.876889944076538\n",
      "step 62\n",
      "training loss: 2.8586580753326416\n",
      "step 63\n",
      "training loss: 2.8353331089019775\n",
      "step 64\n",
      "training loss: 2.8606913089752197\n",
      "step 65\n",
      "training loss: 2.870662212371826\n",
      "step 66\n",
      "training loss: 2.8438799381256104\n",
      "step 67\n",
      "training loss: 2.8590312004089355\n",
      "step 68\n",
      "training loss: 2.8609511852264404\n",
      "step 69\n",
      "training loss: 2.8494324684143066\n",
      "step 70\n",
      "training loss: 2.882050037384033\n",
      "validation loss: 2.846090078353882\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.853825807571411\n",
      "step 72\n",
      "training loss: 2.8710122108459473\n",
      "step 73\n",
      "training loss: 2.8540451526641846\n",
      "step 74\n",
      "training loss: 2.871372699737549\n",
      "step 75\n",
      "training loss: 2.8199548721313477\n",
      "step 76\n",
      "training loss: 2.8843295574188232\n",
      "step 77\n",
      "training loss: 2.8481152057647705\n",
      "step 78\n",
      "training loss: 2.8798391819000244\n",
      "step 79\n",
      "training loss: 2.8646750450134277\n",
      "step 80\n",
      "training loss: 2.843296527862549\n",
      "validation loss: 2.830657482147217\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.842869997024536\n",
      "step 82\n",
      "training loss: 2.861144542694092\n",
      "step 83\n",
      "training loss: 2.84848952293396\n",
      "step 84\n",
      "training loss: 2.8387815952301025\n",
      "step 85\n",
      "training loss: 2.875887632369995\n",
      "step 86\n",
      "training loss: 2.865347385406494\n",
      "step 87\n",
      "training loss: 2.8570117950439453\n",
      "step 88\n",
      "training loss: 2.843132495880127\n",
      "step 89\n",
      "training loss: 2.854449987411499\n",
      "step 90\n",
      "training loss: 2.857900381088257\n",
      "validation loss: 2.8322319984436035\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8308792114257812\n",
      "step 92\n",
      "training loss: 2.8480136394500732\n",
      "step 93\n",
      "training loss: 2.846458911895752\n",
      "step 94\n",
      "training loss: 2.842169761657715\n",
      "step 95\n",
      "training loss: 2.836276054382324\n",
      "step 96\n",
      "training loss: 2.870962142944336\n",
      "step 97\n",
      "training loss: 2.840930223464966\n",
      "step 98\n",
      "training loss: 2.876065254211426\n",
      "step 99\n",
      "training loss: 2.857837677001953\n",
      "step 100\n",
      "training loss: 2.836552858352661\n",
      "validation loss: 2.871673107147217\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8735082149505615\n",
      "step 102\n",
      "training loss: 2.849642515182495\n",
      "step 103\n",
      "training loss: 2.858098030090332\n",
      "step 104\n",
      "training loss: 2.862800359725952\n",
      "step 105\n",
      "training loss: 2.828507661819458\n",
      "step 106\n",
      "training loss: 2.8720879554748535\n",
      "step 107\n",
      "training loss: 2.854454517364502\n",
      "step 108\n",
      "training loss: 2.873905658721924\n",
      "step 109\n",
      "training loss: 2.8700525760650635\n",
      "step 110\n",
      "training loss: 2.8915085792541504\n",
      "validation loss: 2.8747196197509766\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8761932849884033\n",
      "step 112\n",
      "training loss: 2.8318324089050293\n",
      "step 113\n",
      "training loss: 2.869190216064453\n",
      "step 114\n",
      "training loss: 2.8548521995544434\n",
      "step 115\n",
      "training loss: 2.843930959701538\n",
      "step 116\n",
      "training loss: 2.8557543754577637\n",
      "step 117\n",
      "training loss: 2.8569047451019287\n",
      "step 118\n",
      "training loss: 2.875638961791992\n",
      "step 119\n",
      "training loss: 2.8575408458709717\n",
      "step 120\n",
      "training loss: 2.8739147186279297\n",
      "validation loss: 2.8289058208465576\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.873659372329712\n",
      "step 122\n",
      "training loss: 2.8780815601348877\n",
      "step 123\n",
      "training loss: 2.851480007171631\n",
      "step 124\n",
      "training loss: 2.8674514293670654\n",
      "step 125\n",
      "training loss: 2.8586761951446533\n",
      "step 126\n",
      "training loss: 2.853496551513672\n",
      "step 127\n",
      "training loss: 2.8658149242401123\n",
      "step 128\n",
      "training loss: 2.8384299278259277\n",
      "step 129\n",
      "training loss: 2.8664474487304688\n",
      "step 130\n",
      "training loss: 2.8728907108306885\n",
      "validation loss: 2.8512091636657715\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.811002731323242\n",
      "step 132\n",
      "training loss: 2.8532872200012207\n",
      "step 133\n",
      "training loss: 2.8473575115203857\n",
      "step 134\n",
      "training loss: 2.861677885055542\n",
      "step 135\n",
      "training loss: 2.8711423873901367\n",
      "step 136\n",
      "training loss: 2.8417136669158936\n",
      "step 137\n",
      "training loss: 2.8651020526885986\n",
      "step 138\n",
      "training loss: 2.8133251667022705\n",
      "step 139\n",
      "training loss: 2.8484699726104736\n",
      "step 140\n",
      "training loss: 2.873126268386841\n",
      "validation loss: 2.8718016147613525\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.86445689201355\n",
      "step 142\n",
      "training loss: 2.8574531078338623\n",
      "step 143\n",
      "training loss: 2.8467164039611816\n",
      "step 144\n",
      "training loss: 2.864137887954712\n",
      "step 145\n",
      "training loss: 2.8507401943206787\n",
      "step 146\n",
      "training loss: 2.8610799312591553\n",
      "step 147\n",
      "training loss: 2.863877296447754\n",
      "step 148\n",
      "training loss: 2.8619706630706787\n",
      "step 149\n",
      "training loss: 2.8579025268554688\n",
      "step 150\n",
      "training loss: 2.85650372505188\n",
      "validation loss: 2.87576961517334\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.832432746887207\n",
      "step 152\n",
      "training loss: 2.840510606765747\n",
      "step 153\n",
      "training loss: 2.8281562328338623\n",
      "step 154\n",
      "training loss: 2.866650342941284\n",
      "step 155\n",
      "training loss: 2.8680500984191895\n",
      "step 156\n",
      "training loss: 2.860790491104126\n",
      "step 157\n",
      "training loss: 2.859997034072876\n",
      "step 158\n",
      "training loss: 2.8524961471557617\n",
      "step 159\n",
      "training loss: 2.8663899898529053\n",
      "step 160\n",
      "training loss: 2.8893370628356934\n",
      "validation loss: 2.8587777614593506\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8079140186309814\n",
      "step 162\n",
      "training loss: 2.8605027198791504\n",
      "step 163\n",
      "training loss: 2.8501598834991455\n",
      "step 164\n",
      "training loss: 2.876032590866089\n",
      "step 165\n",
      "training loss: 2.8540761470794678\n",
      "step 166\n",
      "training loss: 2.877131462097168\n",
      "step 167\n",
      "training loss: 2.852128744125366\n",
      "step 168\n",
      "training loss: 2.8394153118133545\n",
      "step 169\n",
      "training loss: 2.851533889770508\n",
      "step 170\n",
      "training loss: 2.862963914871216\n",
      "validation loss: 2.9370224475860596\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.849763870239258\n",
      "step 172\n",
      "training loss: 2.8734493255615234\n",
      "step 173\n",
      "training loss: 2.858286142349243\n",
      "step 174\n",
      "training loss: 2.8242311477661133\n",
      "step 175\n",
      "training loss: 2.848754644393921\n",
      "step 176\n",
      "training loss: 2.8513681888580322\n",
      "step 177\n",
      "training loss: 2.8502111434936523\n",
      "step 178\n",
      "training loss: 2.867827892303467\n",
      "step 179\n",
      "training loss: 2.863964319229126\n",
      "step 180\n",
      "training loss: 2.864699363708496\n",
      "validation loss: 2.874513626098633\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.8148515224456787\n",
      "step 182\n",
      "training loss: 2.8632819652557373\n",
      "step 183\n",
      "training loss: 2.856609344482422\n",
      "step 184\n",
      "training loss: 2.863281011581421\n",
      "step 185\n",
      "training loss: 2.8606204986572266\n",
      "step 186\n",
      "training loss: 2.8732500076293945\n",
      "step 187\n",
      "training loss: 2.86673903465271\n",
      "step 188\n",
      "training loss: 2.851417303085327\n",
      "step 189\n",
      "training loss: 2.878082752227783\n",
      "step 190\n",
      "training loss: 2.8756635189056396\n",
      "validation loss: 2.842623233795166\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.866093397140503\n",
      "step 192\n",
      "training loss: 2.8586771488189697\n",
      "step 193\n",
      "training loss: 2.868185043334961\n",
      "step 194\n",
      "training loss: 2.856015920639038\n",
      "step 195\n",
      "training loss: 2.856945276260376\n",
      "step 196\n",
      "training loss: 2.8389699459075928\n",
      "step 197\n",
      "training loss: 2.8254451751708984\n",
      "step 198\n",
      "training loss: 2.8678417205810547\n",
      "step 199\n",
      "training loss: 2.847614049911499\n",
      "step 200\n",
      "training loss: 2.846524953842163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.8559865951538086\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.86027455329895\n",
      "step 202\n",
      "training loss: 2.889039993286133\n",
      "step 203\n",
      "training loss: 2.842430830001831\n",
      "step 204\n",
      "training loss: 2.8709945678710938\n",
      "step 205\n",
      "training loss: 2.8613836765289307\n",
      "step 206\n",
      "training loss: 2.870210647583008\n",
      "step 207\n",
      "training loss: 2.8474273681640625\n",
      "step 208\n",
      "training loss: 2.8596408367156982\n",
      "step 209\n",
      "training loss: 2.834928035736084\n",
      "step 210\n",
      "training loss: 2.848529577255249\n",
      "validation loss: 2.8925633430480957\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8568694591522217\n",
      "step 212\n",
      "training loss: 2.8428568840026855\n",
      "step 213\n",
      "training loss: 2.884899854660034\n",
      "step 214\n",
      "training loss: 2.8578341007232666\n",
      "step 215\n",
      "training loss: 2.849872350692749\n",
      "step 216\n",
      "training loss: 2.8597514629364014\n",
      "step 217\n",
      "training loss: 2.8641927242279053\n",
      "step 218\n",
      "training loss: 2.849849224090576\n",
      "step 219\n",
      "training loss: 2.8412485122680664\n",
      "step 220\n",
      "training loss: 2.8276805877685547\n",
      "validation loss: 2.8218271732330322\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.812140464782715\n",
      "step 222\n",
      "training loss: 2.8730154037475586\n",
      "step 223\n",
      "training loss: 2.8515148162841797\n",
      "step 224\n",
      "training loss: 2.8776681423187256\n",
      "step 225\n",
      "training loss: 2.8521313667297363\n",
      "step 226\n",
      "training loss: 2.861172914505005\n",
      "step 227\n",
      "training loss: 2.8773627281188965\n",
      "step 228\n",
      "training loss: 2.856849193572998\n",
      "step 229\n",
      "training loss: 2.846801280975342\n",
      "step 230\n",
      "training loss: 2.8611555099487305\n",
      "validation loss: 2.8531203269958496\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.841064453125\n",
      "step 232\n",
      "training loss: 2.8526875972747803\n",
      "step 233\n",
      "training loss: 2.8434062004089355\n",
      "step 234\n",
      "training loss: 2.844364881515503\n",
      "step 235\n",
      "training loss: 2.8344461917877197\n",
      "step 236\n",
      "training loss: 2.857187271118164\n",
      "step 237\n",
      "training loss: 2.859251022338867\n",
      "step 238\n",
      "training loss: 2.843461513519287\n",
      "step 239\n",
      "training loss: 2.871384620666504\n",
      "step 240\n",
      "training loss: 2.8503661155700684\n",
      "validation loss: 2.85727596282959\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.861464262008667\n",
      "step 242\n",
      "training loss: 2.844961166381836\n",
      "step 243\n",
      "training loss: 2.8644726276397705\n",
      "step 244\n",
      "training loss: 2.7901175022125244\n",
      "step 245\n",
      "training loss: 2.8468081951141357\n",
      "step 246\n",
      "training loss: 2.830922842025757\n",
      "step 247\n",
      "training loss: 2.840491533279419\n",
      "step 248\n",
      "training loss: 2.868121385574341\n",
      "step 249\n",
      "training loss: 2.8703105449676514\n",
      "step 250\n",
      "training loss: 2.859206199645996\n",
      "validation loss: 2.8656837940216064\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8583545684814453\n",
      "step 252\n",
      "training loss: 2.8573405742645264\n",
      "step 253\n",
      "training loss: 2.8479080200195312\n",
      "step 254\n",
      "training loss: 2.854912519454956\n",
      "step 255\n",
      "training loss: 2.869980812072754\n",
      "step 256\n",
      "training loss: 2.884992837905884\n",
      "step 257\n",
      "training loss: 2.8542933464050293\n",
      "step 258\n",
      "training loss: 2.8442695140838623\n",
      "step 259\n",
      "training loss: 2.8471734523773193\n",
      "step 260\n",
      "training loss: 2.847409248352051\n",
      "validation loss: 2.900634765625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8580472469329834\n",
      "step 262\n",
      "training loss: 2.8596348762512207\n",
      "step 263\n",
      "training loss: 2.848203420639038\n",
      "step 264\n",
      "training loss: 2.8640077114105225\n",
      "step 265\n",
      "training loss: 2.871718168258667\n",
      "step 266\n",
      "training loss: 2.8460988998413086\n",
      "step 267\n",
      "training loss: 2.8728878498077393\n",
      "step 268\n",
      "training loss: 2.873434543609619\n",
      "step 269\n",
      "training loss: 2.8430473804473877\n",
      "step 270\n",
      "training loss: 2.8480873107910156\n",
      "validation loss: 2.862663984298706\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.870793342590332\n",
      "----------3.0 min per epoch----------\n",
      "epoch 36\n",
      "step 0\n",
      "training loss: 2.8643367290496826\n",
      "validation loss: 2.854383707046509\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8604111671447754\n",
      "step 2\n",
      "training loss: 2.8610966205596924\n",
      "step 3\n",
      "training loss: 2.8655717372894287\n",
      "step 4\n",
      "training loss: 2.8555314540863037\n",
      "step 5\n",
      "training loss: 2.8611223697662354\n",
      "step 6\n",
      "training loss: 2.859713077545166\n",
      "step 7\n",
      "training loss: 2.849365472793579\n",
      "step 8\n",
      "training loss: 2.8491780757904053\n",
      "step 9\n",
      "training loss: 2.8164143562316895\n",
      "step 10\n",
      "training loss: 2.8469362258911133\n",
      "validation loss: 2.8189637660980225\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8698766231536865\n",
      "step 12\n",
      "training loss: 2.842998504638672\n",
      "step 13\n",
      "training loss: 2.8425133228302\n",
      "step 14\n",
      "training loss: 2.8828115463256836\n",
      "step 15\n",
      "training loss: 2.859348773956299\n",
      "step 16\n",
      "training loss: 2.8758797645568848\n",
      "step 17\n",
      "training loss: 2.878269672393799\n",
      "step 18\n",
      "training loss: 2.864138603210449\n",
      "step 19\n",
      "training loss: 2.843724250793457\n",
      "step 20\n",
      "training loss: 2.876112461090088\n",
      "validation loss: 2.83730149269104\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8765978813171387\n",
      "step 22\n",
      "training loss: 2.867549180984497\n",
      "step 23\n",
      "training loss: 2.862931728363037\n",
      "step 24\n",
      "training loss: 2.8735485076904297\n",
      "step 25\n",
      "training loss: 2.883139133453369\n",
      "step 26\n",
      "training loss: 2.8001976013183594\n",
      "step 27\n",
      "training loss: 2.8429629802703857\n",
      "step 28\n",
      "training loss: 2.781001567840576\n",
      "step 29\n",
      "training loss: 2.8302054405212402\n",
      "step 30\n",
      "training loss: 2.8786282539367676\n",
      "validation loss: 2.8061764240264893\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.87135910987854\n",
      "step 32\n",
      "training loss: 2.878272294998169\n",
      "step 33\n",
      "training loss: 2.866239547729492\n",
      "step 34\n",
      "training loss: 2.7628705501556396\n",
      "step 35\n",
      "training loss: 2.6859729290008545\n",
      "step 36\n",
      "training loss: 2.806565761566162\n",
      "step 37\n",
      "training loss: 2.8906118869781494\n",
      "step 38\n",
      "training loss: 2.8702263832092285\n",
      "step 39\n",
      "training loss: 2.8549625873565674\n",
      "step 40\n",
      "training loss: 2.860229969024658\n",
      "validation loss: 2.8684515953063965\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8756046295166016\n",
      "step 42\n",
      "training loss: 2.876767158508301\n",
      "step 43\n",
      "training loss: 2.87701416015625\n",
      "step 44\n",
      "training loss: 2.864626169204712\n",
      "step 45\n",
      "training loss: 2.879791736602783\n",
      "step 46\n",
      "training loss: 2.8841307163238525\n",
      "step 47\n",
      "training loss: 2.8376517295837402\n",
      "step 48\n",
      "training loss: 2.841212511062622\n",
      "step 49\n",
      "training loss: 2.8566367626190186\n",
      "step 50\n",
      "training loss: 2.8593554496765137\n",
      "validation loss: 2.865117311477661\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8703453540802\n",
      "step 52\n",
      "training loss: 2.8652288913726807\n",
      "step 53\n",
      "training loss: 2.8734817504882812\n",
      "step 54\n",
      "training loss: 2.8759021759033203\n",
      "step 55\n",
      "training loss: 2.8685433864593506\n",
      "step 56\n",
      "training loss: 2.8886239528656006\n",
      "step 57\n",
      "training loss: 2.86138653755188\n",
      "step 58\n",
      "training loss: 2.871356248855591\n",
      "step 59\n",
      "training loss: 2.8814311027526855\n",
      "step 60\n",
      "training loss: 2.876023769378662\n",
      "validation loss: 2.864171266555786\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.8724822998046875\n",
      "step 62\n",
      "training loss: 2.8753764629364014\n",
      "step 63\n",
      "training loss: 2.8594257831573486\n",
      "step 64\n",
      "training loss: 2.834984064102173\n",
      "step 65\n",
      "training loss: 2.8594303131103516\n",
      "step 66\n",
      "training loss: 2.8732383251190186\n",
      "step 67\n",
      "training loss: 2.8430209159851074\n",
      "step 68\n",
      "training loss: 2.857930898666382\n",
      "step 69\n",
      "training loss: 2.859400987625122\n",
      "step 70\n",
      "training loss: 2.8461103439331055\n",
      "validation loss: 2.858842134475708\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8818233013153076\n",
      "step 72\n",
      "training loss: 2.852666139602661\n",
      "step 73\n",
      "training loss: 2.872382640838623\n",
      "step 74\n",
      "training loss: 2.852611780166626\n",
      "step 75\n",
      "training loss: 2.871184825897217\n",
      "step 76\n",
      "training loss: 2.821129083633423\n",
      "step 77\n",
      "training loss: 2.883009672164917\n",
      "step 78\n",
      "training loss: 2.8447937965393066\n",
      "step 79\n",
      "training loss: 2.88078236579895\n",
      "step 80\n",
      "training loss: 2.864785671234131\n",
      "validation loss: 2.8602750301361084\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8448383808135986\n",
      "step 82\n",
      "training loss: 2.8413705825805664\n",
      "step 83\n",
      "training loss: 2.86089825630188\n",
      "step 84\n",
      "training loss: 2.8499507904052734\n",
      "step 85\n",
      "training loss: 2.8390183448791504\n",
      "step 86\n",
      "training loss: 2.8772997856140137\n",
      "step 87\n",
      "training loss: 2.865490198135376\n",
      "step 88\n",
      "training loss: 2.85750412940979\n",
      "step 89\n",
      "training loss: 2.8422493934631348\n",
      "step 90\n",
      "training loss: 2.8560545444488525\n",
      "validation loss: 2.8352086544036865\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.857377052307129\n",
      "step 92\n",
      "training loss: 2.8316073417663574\n",
      "step 93\n",
      "training loss: 2.847625970840454\n",
      "step 94\n",
      "training loss: 2.8463521003723145\n",
      "step 95\n",
      "training loss: 2.8425333499908447\n",
      "step 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8332626819610596\n",
      "step 97\n",
      "training loss: 2.867988348007202\n",
      "step 98\n",
      "training loss: 2.841278553009033\n",
      "step 99\n",
      "training loss: 2.8753538131713867\n",
      "step 100\n",
      "training loss: 2.8588380813598633\n",
      "validation loss: 2.8394203186035156\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.837665319442749\n",
      "step 102\n",
      "training loss: 2.8737449645996094\n",
      "step 103\n",
      "training loss: 2.8510138988494873\n",
      "step 104\n",
      "training loss: 2.8580093383789062\n",
      "step 105\n",
      "training loss: 2.862027406692505\n",
      "step 106\n",
      "training loss: 2.829929828643799\n",
      "step 107\n",
      "training loss: 2.8741211891174316\n",
      "step 108\n",
      "training loss: 2.853858232498169\n",
      "step 109\n",
      "training loss: 2.8748679161071777\n",
      "step 110\n",
      "training loss: 2.8698065280914307\n",
      "validation loss: 2.8293075561523438\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8914318084716797\n",
      "step 112\n",
      "training loss: 2.8775243759155273\n",
      "step 113\n",
      "training loss: 2.8324363231658936\n",
      "step 114\n",
      "training loss: 2.869553565979004\n",
      "step 115\n",
      "training loss: 2.8549392223358154\n",
      "step 116\n",
      "training loss: 2.8450584411621094\n",
      "step 117\n",
      "training loss: 2.855931043624878\n",
      "step 118\n",
      "training loss: 2.8557021617889404\n",
      "step 119\n",
      "training loss: 2.877617359161377\n",
      "step 120\n",
      "training loss: 2.857481002807617\n",
      "validation loss: 2.8283658027648926\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.873934507369995\n",
      "step 122\n",
      "training loss: 2.8739266395568848\n",
      "step 123\n",
      "training loss: 2.8773176670074463\n",
      "step 124\n",
      "training loss: 2.8522071838378906\n",
      "step 125\n",
      "training loss: 2.867690086364746\n",
      "step 126\n",
      "training loss: 2.861109733581543\n",
      "step 127\n",
      "training loss: 2.852421522140503\n",
      "step 128\n",
      "training loss: 2.865861415863037\n",
      "step 129\n",
      "training loss: 2.8410470485687256\n",
      "step 130\n",
      "training loss: 2.867917060852051\n",
      "validation loss: 2.870084524154663\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.870633840560913\n",
      "step 132\n",
      "training loss: 2.8110837936401367\n",
      "step 133\n",
      "training loss: 2.8530919551849365\n",
      "step 134\n",
      "training loss: 2.848695755004883\n",
      "step 135\n",
      "training loss: 2.861032247543335\n",
      "step 136\n",
      "training loss: 2.868412971496582\n",
      "step 137\n",
      "training loss: 2.842376708984375\n",
      "step 138\n",
      "training loss: 2.8637535572052\n",
      "step 139\n",
      "training loss: 2.8126909732818604\n",
      "step 140\n",
      "training loss: 2.849246025085449\n",
      "validation loss: 2.8736116886138916\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.871812582015991\n",
      "step 142\n",
      "training loss: 2.863863945007324\n",
      "step 143\n",
      "training loss: 2.8563294410705566\n",
      "step 144\n",
      "training loss: 2.8451404571533203\n",
      "step 145\n",
      "training loss: 2.8636608123779297\n",
      "step 146\n",
      "training loss: 2.850059747695923\n",
      "step 147\n",
      "training loss: 2.8619651794433594\n",
      "step 148\n",
      "training loss: 2.862178325653076\n",
      "step 149\n",
      "training loss: 2.8622853755950928\n",
      "step 150\n",
      "training loss: 2.8547229766845703\n",
      "validation loss: 2.8288965225219727\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8564093112945557\n",
      "step 152\n",
      "training loss: 2.8313543796539307\n",
      "step 153\n",
      "training loss: 2.83760666847229\n",
      "step 154\n",
      "training loss: 2.829063653945923\n",
      "step 155\n",
      "training loss: 2.8639297485351562\n",
      "step 156\n",
      "training loss: 2.8687472343444824\n",
      "step 157\n",
      "training loss: 2.8612115383148193\n",
      "step 158\n",
      "training loss: 2.8597240447998047\n",
      "step 159\n",
      "training loss: 2.849851131439209\n",
      "step 160\n",
      "training loss: 2.8662612438201904\n",
      "validation loss: 2.8500120639801025\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8863439559936523\n",
      "step 162\n",
      "training loss: 2.809257745742798\n",
      "step 163\n",
      "training loss: 2.857422113418579\n",
      "step 164\n",
      "training loss: 2.852323293685913\n",
      "step 165\n",
      "training loss: 2.874340772628784\n",
      "step 166\n",
      "training loss: 2.855093479156494\n",
      "step 167\n",
      "training loss: 2.878633975982666\n",
      "step 168\n",
      "training loss: 2.852302074432373\n",
      "step 169\n",
      "training loss: 2.839448928833008\n",
      "step 170\n",
      "training loss: 2.850334882736206\n",
      "validation loss: 2.8734726905822754\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.864370584487915\n",
      "step 172\n",
      "training loss: 2.8495779037475586\n",
      "step 173\n",
      "training loss: 2.871685028076172\n",
      "step 174\n",
      "training loss: 2.858292579650879\n",
      "step 175\n",
      "training loss: 2.8212850093841553\n",
      "step 176\n",
      "training loss: 2.8492794036865234\n",
      "step 177\n",
      "training loss: 2.851327896118164\n",
      "step 178\n",
      "training loss: 2.8477368354797363\n",
      "step 179\n",
      "training loss: 2.8697891235351562\n",
      "step 180\n",
      "training loss: 2.8654868602752686\n",
      "validation loss: 2.869391441345215\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.865361213684082\n",
      "step 182\n",
      "training loss: 2.810199737548828\n",
      "step 183\n",
      "training loss: 2.862107276916504\n",
      "step 184\n",
      "training loss: 2.85701060295105\n",
      "step 185\n",
      "training loss: 2.860959053039551\n",
      "step 186\n",
      "training loss: 2.8602092266082764\n",
      "step 187\n",
      "training loss: 2.87096905708313\n",
      "step 188\n",
      "training loss: 2.8665900230407715\n",
      "step 189\n",
      "training loss: 2.8510489463806152\n",
      "step 190\n",
      "training loss: 2.876824378967285\n",
      "validation loss: 2.8596348762512207\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.874023914337158\n",
      "step 192\n",
      "training loss: 2.8640427589416504\n",
      "step 193\n",
      "training loss: 2.8582522869110107\n",
      "step 194\n",
      "training loss: 2.8664214611053467\n",
      "step 195\n",
      "training loss: 2.857700824737549\n",
      "step 196\n",
      "training loss: 2.8564703464508057\n",
      "step 197\n",
      "training loss: 2.8401646614074707\n",
      "step 198\n",
      "training loss: 2.8191871643066406\n",
      "step 199\n",
      "training loss: 2.8691368103027344\n",
      "step 200\n",
      "training loss: 2.8498694896698\n",
      "validation loss: 2.934817314147949\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.846942186355591\n",
      "step 202\n",
      "training loss: 2.858614444732666\n",
      "step 203\n",
      "training loss: 2.8892061710357666\n",
      "step 204\n",
      "training loss: 2.8409018516540527\n",
      "step 205\n",
      "training loss: 2.8700168132781982\n",
      "step 206\n",
      "training loss: 2.862802743911743\n",
      "step 207\n",
      "training loss: 2.867344856262207\n",
      "step 208\n",
      "training loss: 2.8460397720336914\n",
      "step 209\n",
      "training loss: 2.8571596145629883\n",
      "step 210\n",
      "training loss: 2.8363842964172363\n",
      "validation loss: 2.8804209232330322\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.849550485610962\n",
      "step 212\n",
      "training loss: 2.8556506633758545\n",
      "step 213\n",
      "training loss: 2.842085123062134\n",
      "step 214\n",
      "training loss: 2.883906841278076\n",
      "step 215\n",
      "training loss: 2.855642557144165\n",
      "step 216\n",
      "training loss: 2.846048593521118\n",
      "step 217\n",
      "training loss: 2.859954595565796\n",
      "step 218\n",
      "training loss: 2.8654592037200928\n",
      "step 219\n",
      "training loss: 2.850661277770996\n",
      "step 220\n",
      "training loss: 2.8398211002349854\n",
      "validation loss: 2.8400309085845947\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8237147331237793\n",
      "step 222\n",
      "training loss: 2.8119771480560303\n",
      "step 223\n",
      "training loss: 2.8751533031463623\n",
      "step 224\n",
      "training loss: 2.8522515296936035\n",
      "step 225\n",
      "training loss: 2.8778109550476074\n",
      "step 226\n",
      "training loss: 2.8527252674102783\n",
      "step 227\n",
      "training loss: 2.8607888221740723\n",
      "step 228\n",
      "training loss: 2.8795130252838135\n",
      "step 229\n",
      "training loss: 2.857609272003174\n",
      "step 230\n",
      "training loss: 2.8449246883392334\n",
      "validation loss: 2.8538339138031006\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.859448194503784\n",
      "step 232\n",
      "training loss: 2.8425047397613525\n",
      "step 233\n",
      "training loss: 2.854033946990967\n",
      "step 234\n",
      "training loss: 2.8420627117156982\n",
      "step 235\n",
      "training loss: 2.843693256378174\n",
      "step 236\n",
      "training loss: 2.831364393234253\n",
      "step 237\n",
      "training loss: 2.856722116470337\n",
      "step 238\n",
      "training loss: 2.8591527938842773\n",
      "step 239\n",
      "training loss: 2.845585346221924\n",
      "step 240\n",
      "training loss: 2.8727917671203613\n",
      "validation loss: 2.895660400390625\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.849104642868042\n",
      "step 242\n",
      "training loss: 2.859697103500366\n",
      "step 243\n",
      "training loss: 2.844592809677124\n",
      "step 244\n",
      "training loss: 2.8635571002960205\n",
      "step 245\n",
      "training loss: 2.792405843734741\n",
      "step 246\n",
      "training loss: 2.847745895385742\n",
      "step 247\n",
      "training loss: 2.832637310028076\n",
      "step 248\n",
      "training loss: 2.8390581607818604\n",
      "step 249\n",
      "training loss: 2.8661088943481445\n",
      "step 250\n",
      "training loss: 2.871206760406494\n",
      "validation loss: 2.7998218536376953\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8632311820983887\n",
      "step 252\n",
      "training loss: 2.860327959060669\n",
      "step 253\n",
      "training loss: 2.8599748611450195\n",
      "step 254\n",
      "training loss: 2.847990036010742\n",
      "step 255\n",
      "training loss: 2.8540589809417725\n",
      "step 256\n",
      "training loss: 2.869868278503418\n",
      "step 257\n",
      "training loss: 2.8852055072784424\n",
      "step 258\n",
      "training loss: 2.855518102645874\n",
      "step 259\n",
      "training loss: 2.845799446105957\n",
      "step 260\n",
      "training loss: 2.847062826156616\n",
      "validation loss: 2.8510735034942627\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8449089527130127\n",
      "step 262\n",
      "training loss: 2.859318971633911\n",
      "step 263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.855757713317871\n",
      "step 264\n",
      "training loss: 2.8464958667755127\n",
      "step 265\n",
      "training loss: 2.866717576980591\n",
      "step 266\n",
      "training loss: 2.873157501220703\n",
      "step 267\n",
      "training loss: 2.846836566925049\n",
      "step 268\n",
      "training loss: 2.8722457885742188\n",
      "step 269\n",
      "training loss: 2.875079393386841\n",
      "step 270\n",
      "training loss: 2.8444576263427734\n",
      "validation loss: 2.8553378582000732\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8469693660736084\n",
      "----------3.0 min per epoch----------\n",
      "epoch 37\n",
      "step 0\n",
      "training loss: 2.869276285171509\n",
      "validation loss: 2.8647539615631104\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8628830909729004\n",
      "step 2\n",
      "training loss: 2.8613266944885254\n",
      "step 3\n",
      "training loss: 2.863750457763672\n",
      "step 4\n",
      "training loss: 2.864367723464966\n",
      "step 5\n",
      "training loss: 2.856848955154419\n",
      "step 6\n",
      "training loss: 2.8624696731567383\n",
      "step 7\n",
      "training loss: 2.859600782394409\n",
      "step 8\n",
      "training loss: 2.8477542400360107\n",
      "step 9\n",
      "training loss: 2.848332166671753\n",
      "step 10\n",
      "training loss: 2.8114852905273438\n",
      "validation loss: 2.898238182067871\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8497462272644043\n",
      "step 12\n",
      "training loss: 2.870619535446167\n",
      "step 13\n",
      "training loss: 2.8401076793670654\n",
      "step 14\n",
      "training loss: 2.8333609104156494\n",
      "step 15\n",
      "training loss: 2.886075019836426\n",
      "step 16\n",
      "training loss: 2.8611576557159424\n",
      "step 17\n",
      "training loss: 2.8724825382232666\n",
      "step 18\n",
      "training loss: 2.880091667175293\n",
      "step 19\n",
      "training loss: 2.8650739192962646\n",
      "step 20\n",
      "training loss: 2.8444623947143555\n",
      "validation loss: 2.873297691345215\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.876164674758911\n",
      "step 22\n",
      "training loss: 2.8768093585968018\n",
      "step 23\n",
      "training loss: 2.867915630340576\n",
      "step 24\n",
      "training loss: 2.862976312637329\n",
      "step 25\n",
      "training loss: 2.8765006065368652\n",
      "step 26\n",
      "training loss: 2.8843882083892822\n",
      "step 27\n",
      "training loss: 2.803603410720825\n",
      "step 28\n",
      "training loss: 2.8399112224578857\n",
      "step 29\n",
      "training loss: 2.774826765060425\n",
      "step 30\n",
      "training loss: 2.8261611461639404\n",
      "validation loss: 2.8517568111419678\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8780601024627686\n",
      "step 32\n",
      "training loss: 2.870588541030884\n",
      "step 33\n",
      "training loss: 2.8809218406677246\n",
      "step 34\n",
      "training loss: 2.8701388835906982\n",
      "step 35\n",
      "training loss: 2.7499306201934814\n",
      "step 36\n",
      "training loss: 2.671370267868042\n",
      "step 37\n",
      "training loss: 2.806936740875244\n",
      "step 38\n",
      "training loss: 2.8769657611846924\n",
      "step 39\n",
      "training loss: 2.869109630584717\n",
      "step 40\n",
      "training loss: 2.8579869270324707\n",
      "validation loss: 2.833364248275757\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.859954595565796\n",
      "step 42\n",
      "training loss: 2.8749053478240967\n",
      "step 43\n",
      "training loss: 2.873471975326538\n",
      "step 44\n",
      "training loss: 2.8788654804229736\n",
      "step 45\n",
      "training loss: 2.863234519958496\n",
      "step 46\n",
      "training loss: 2.877899408340454\n",
      "step 47\n",
      "training loss: 2.8791356086730957\n",
      "step 48\n",
      "training loss: 2.835845708847046\n",
      "step 49\n",
      "training loss: 2.842595338821411\n",
      "step 50\n",
      "training loss: 2.8527159690856934\n",
      "validation loss: 2.839980125427246\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8555099964141846\n",
      "step 52\n",
      "training loss: 2.8709776401519775\n",
      "step 53\n",
      "training loss: 2.8670928478240967\n",
      "step 54\n",
      "training loss: 2.871948003768921\n",
      "step 55\n",
      "training loss: 2.8760242462158203\n",
      "step 56\n",
      "training loss: 2.8682594299316406\n",
      "step 57\n",
      "training loss: 2.886237859725952\n",
      "step 58\n",
      "training loss: 2.864532947540283\n",
      "step 59\n",
      "training loss: 2.8719370365142822\n",
      "step 60\n",
      "training loss: 2.8811705112457275\n",
      "validation loss: 2.8233933448791504\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.875509738922119\n",
      "step 62\n",
      "training loss: 2.872386932373047\n",
      "step 63\n",
      "training loss: 2.872380495071411\n",
      "step 64\n",
      "training loss: 2.856191635131836\n",
      "step 65\n",
      "training loss: 2.835108757019043\n",
      "step 66\n",
      "training loss: 2.859567165374756\n",
      "step 67\n",
      "training loss: 2.871506929397583\n",
      "step 68\n",
      "training loss: 2.838677167892456\n",
      "step 69\n",
      "training loss: 2.858027696609497\n",
      "step 70\n",
      "training loss: 2.8597121238708496\n",
      "validation loss: 2.85758113861084\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8468616008758545\n",
      "step 72\n",
      "training loss: 2.8811562061309814\n",
      "step 73\n",
      "training loss: 2.8564250469207764\n",
      "step 74\n",
      "training loss: 2.8712756633758545\n",
      "step 75\n",
      "training loss: 2.8515920639038086\n",
      "step 76\n",
      "training loss: 2.868870973587036\n",
      "step 77\n",
      "training loss: 2.8214833736419678\n",
      "step 78\n",
      "training loss: 2.8819358348846436\n",
      "step 79\n",
      "training loss: 2.8455519676208496\n",
      "step 80\n",
      "training loss: 2.87899112701416\n",
      "validation loss: 2.864781618118286\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.8667678833007812\n",
      "step 82\n",
      "training loss: 2.8461365699768066\n",
      "step 83\n",
      "training loss: 2.8420238494873047\n",
      "step 84\n",
      "training loss: 2.8606691360473633\n",
      "step 85\n",
      "training loss: 2.8498432636260986\n",
      "step 86\n",
      "training loss: 2.839912176132202\n",
      "step 87\n",
      "training loss: 2.877248764038086\n",
      "step 88\n",
      "training loss: 2.8661539554595947\n",
      "step 89\n",
      "training loss: 2.8568804264068604\n",
      "step 90\n",
      "training loss: 2.8393521308898926\n",
      "validation loss: 2.858752727508545\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8554797172546387\n",
      "step 92\n",
      "training loss: 2.8551762104034424\n",
      "step 93\n",
      "training loss: 2.827176094055176\n",
      "step 94\n",
      "training loss: 2.84828782081604\n",
      "step 95\n",
      "training loss: 2.846503973007202\n",
      "step 96\n",
      "training loss: 2.843096971511841\n",
      "step 97\n",
      "training loss: 2.8336575031280518\n",
      "step 98\n",
      "training loss: 2.8698456287384033\n",
      "step 99\n",
      "training loss: 2.8417446613311768\n",
      "step 100\n",
      "training loss: 2.8760294914245605\n",
      "validation loss: 2.8561010360717773\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.857043743133545\n",
      "step 102\n",
      "training loss: 2.8352296352386475\n",
      "step 103\n",
      "training loss: 2.8722596168518066\n",
      "step 104\n",
      "training loss: 2.8488264083862305\n",
      "step 105\n",
      "training loss: 2.8552684783935547\n",
      "step 106\n",
      "training loss: 2.8619611263275146\n",
      "step 107\n",
      "training loss: 2.8288094997406006\n",
      "step 108\n",
      "training loss: 2.872899293899536\n",
      "step 109\n",
      "training loss: 2.8537757396698\n",
      "step 110\n",
      "training loss: 2.873746633529663\n",
      "validation loss: 2.8561480045318604\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8690834045410156\n",
      "step 112\n",
      "training loss: 2.8919456005096436\n",
      "step 113\n",
      "training loss: 2.876426935195923\n",
      "step 114\n",
      "training loss: 2.832000732421875\n",
      "step 115\n",
      "training loss: 2.871565103530884\n",
      "step 116\n",
      "training loss: 2.8567986488342285\n",
      "step 117\n",
      "training loss: 2.8454036712646484\n",
      "step 118\n",
      "training loss: 2.854038953781128\n",
      "step 119\n",
      "training loss: 2.855768918991089\n",
      "step 120\n",
      "training loss: 2.87418532371521\n",
      "validation loss: 2.834845781326294\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.8576266765594482\n",
      "step 122\n",
      "training loss: 2.87406063079834\n",
      "step 123\n",
      "training loss: 2.8730685710906982\n",
      "step 124\n",
      "training loss: 2.875934362411499\n",
      "step 125\n",
      "training loss: 2.8530972003936768\n",
      "step 126\n",
      "training loss: 2.8666186332702637\n",
      "step 127\n",
      "training loss: 2.859210729598999\n",
      "step 128\n",
      "training loss: 2.853618621826172\n",
      "step 129\n",
      "training loss: 2.8664655685424805\n",
      "step 130\n",
      "training loss: 2.8401122093200684\n",
      "validation loss: 2.837920904159546\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.8666841983795166\n",
      "step 132\n",
      "training loss: 2.8715286254882812\n",
      "step 133\n",
      "training loss: 2.810608148574829\n",
      "step 134\n",
      "training loss: 2.853304624557495\n",
      "step 135\n",
      "training loss: 2.8486318588256836\n",
      "step 136\n",
      "training loss: 2.862057685852051\n",
      "step 137\n",
      "training loss: 2.8681037425994873\n",
      "step 138\n",
      "training loss: 2.8430240154266357\n",
      "step 139\n",
      "training loss: 2.8652729988098145\n",
      "step 140\n",
      "training loss: 2.811967134475708\n",
      "validation loss: 2.8281307220458984\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.847200393676758\n",
      "step 142\n",
      "training loss: 2.871617078781128\n",
      "step 143\n",
      "training loss: 2.8642425537109375\n",
      "step 144\n",
      "training loss: 2.8575096130371094\n",
      "step 145\n",
      "training loss: 2.846503257751465\n",
      "step 146\n",
      "training loss: 2.864424467086792\n",
      "step 147\n",
      "training loss: 2.8489694595336914\n",
      "step 148\n",
      "training loss: 2.8607687950134277\n",
      "step 149\n",
      "training loss: 2.864208936691284\n",
      "step 150\n",
      "training loss: 2.8597424030303955\n",
      "validation loss: 2.8260068893432617\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.853733539581299\n",
      "step 152\n",
      "training loss: 2.8575611114501953\n",
      "step 153\n",
      "training loss: 2.83005952835083\n",
      "step 154\n",
      "training loss: 2.8383803367614746\n",
      "step 155\n",
      "training loss: 2.8304247856140137\n",
      "step 156\n",
      "training loss: 2.8646533489227295\n",
      "step 157\n",
      "training loss: 2.868147850036621\n",
      "step 158\n",
      "training loss: 2.8605947494506836\n",
      "step 159\n",
      "training loss: 2.8588802814483643\n",
      "step 160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8507537841796875\n",
      "validation loss: 2.869924306869507\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.865739107131958\n",
      "step 162\n",
      "training loss: 2.8867757320404053\n",
      "step 163\n",
      "training loss: 2.8077688217163086\n",
      "step 164\n",
      "training loss: 2.856053590774536\n",
      "step 165\n",
      "training loss: 2.8512885570526123\n",
      "step 166\n",
      "training loss: 2.8727147579193115\n",
      "step 167\n",
      "training loss: 2.8548271656036377\n",
      "step 168\n",
      "training loss: 2.8763725757598877\n",
      "step 169\n",
      "training loss: 2.851839542388916\n",
      "step 170\n",
      "training loss: 2.8426380157470703\n",
      "validation loss: 2.8732407093048096\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.8521971702575684\n",
      "step 172\n",
      "training loss: 2.8645410537719727\n",
      "step 173\n",
      "training loss: 2.8505218029022217\n",
      "step 174\n",
      "training loss: 2.871896505355835\n",
      "step 175\n",
      "training loss: 2.8572144508361816\n",
      "step 176\n",
      "training loss: 2.8220713138580322\n",
      "step 177\n",
      "training loss: 2.8499069213867188\n",
      "step 178\n",
      "training loss: 2.850280284881592\n",
      "step 179\n",
      "training loss: 2.8487496376037598\n",
      "step 180\n",
      "training loss: 2.8704123497009277\n",
      "validation loss: 2.829228401184082\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.864109992980957\n",
      "step 182\n",
      "training loss: 2.866806983947754\n",
      "step 183\n",
      "training loss: 2.813006639480591\n",
      "step 184\n",
      "training loss: 2.8627631664276123\n",
      "step 185\n",
      "training loss: 2.853910207748413\n",
      "step 186\n",
      "training loss: 2.859971284866333\n",
      "step 187\n",
      "training loss: 2.862537384033203\n",
      "step 188\n",
      "training loss: 2.8724453449249268\n",
      "step 189\n",
      "training loss: 2.867645740509033\n",
      "step 190\n",
      "training loss: 2.850205898284912\n",
      "validation loss: 2.854027032852173\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8773632049560547\n",
      "step 192\n",
      "training loss: 2.873131513595581\n",
      "step 193\n",
      "training loss: 2.8663041591644287\n",
      "step 194\n",
      "training loss: 2.8560287952423096\n",
      "step 195\n",
      "training loss: 2.867621660232544\n",
      "step 196\n",
      "training loss: 2.8588337898254395\n",
      "step 197\n",
      "training loss: 2.854273557662964\n",
      "step 198\n",
      "training loss: 2.834263324737549\n",
      "step 199\n",
      "training loss: 2.8198719024658203\n",
      "step 200\n",
      "training loss: 2.8687477111816406\n",
      "validation loss: 2.876763105392456\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.846140146255493\n",
      "step 202\n",
      "training loss: 2.845559597015381\n",
      "step 203\n",
      "training loss: 2.8570594787597656\n",
      "step 204\n",
      "training loss: 2.8894379138946533\n",
      "step 205\n",
      "training loss: 2.84090518951416\n",
      "step 206\n",
      "training loss: 2.8662776947021484\n",
      "step 207\n",
      "training loss: 2.8622868061065674\n",
      "step 208\n",
      "training loss: 2.8695008754730225\n",
      "step 209\n",
      "training loss: 2.8459060192108154\n",
      "step 210\n",
      "training loss: 2.8566534519195557\n",
      "validation loss: 2.8709983825683594\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8355631828308105\n",
      "step 212\n",
      "training loss: 2.847069501876831\n",
      "step 213\n",
      "training loss: 2.8547635078430176\n",
      "step 214\n",
      "training loss: 2.8415822982788086\n",
      "step 215\n",
      "training loss: 2.884237289428711\n",
      "step 216\n",
      "training loss: 2.857792615890503\n",
      "step 217\n",
      "training loss: 2.849283456802368\n",
      "step 218\n",
      "training loss: 2.8587474822998047\n",
      "step 219\n",
      "training loss: 2.866349220275879\n",
      "step 220\n",
      "training loss: 2.851120710372925\n",
      "validation loss: 2.8611016273498535\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.83868670463562\n",
      "step 222\n",
      "training loss: 2.824856758117676\n",
      "step 223\n",
      "training loss: 2.8091087341308594\n",
      "step 224\n",
      "training loss: 2.874955654144287\n",
      "step 225\n",
      "training loss: 2.853808641433716\n",
      "step 226\n",
      "training loss: 2.878037214279175\n",
      "step 227\n",
      "training loss: 2.851916790008545\n",
      "step 228\n",
      "training loss: 2.8613595962524414\n",
      "step 229\n",
      "training loss: 2.879405975341797\n",
      "step 230\n",
      "training loss: 2.8574271202087402\n",
      "validation loss: 2.937217950820923\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.846285820007324\n",
      "step 232\n",
      "training loss: 2.8610117435455322\n",
      "step 233\n",
      "training loss: 2.842994451522827\n",
      "step 234\n",
      "training loss: 2.851609230041504\n",
      "step 235\n",
      "training loss: 2.8426389694213867\n",
      "step 236\n",
      "training loss: 2.8424744606018066\n",
      "step 237\n",
      "training loss: 2.834578514099121\n",
      "step 238\n",
      "training loss: 2.8552443981170654\n",
      "step 239\n",
      "training loss: 2.855949640274048\n",
      "step 240\n",
      "training loss: 2.845421314239502\n",
      "validation loss: 2.8779799938201904\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.873579740524292\n",
      "step 242\n",
      "training loss: 2.848933696746826\n",
      "step 243\n",
      "training loss: 2.8615052700042725\n",
      "step 244\n",
      "training loss: 2.84381365776062\n",
      "step 245\n",
      "training loss: 2.862156391143799\n",
      "step 246\n",
      "training loss: 2.7887773513793945\n",
      "step 247\n",
      "training loss: 2.846961498260498\n",
      "step 248\n",
      "training loss: 2.8305258750915527\n",
      "step 249\n",
      "training loss: 2.8410046100616455\n",
      "step 250\n",
      "training loss: 2.8648040294647217\n",
      "validation loss: 2.819458484649658\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8714425563812256\n",
      "step 252\n",
      "training loss: 2.8606350421905518\n",
      "step 253\n",
      "training loss: 2.859837532043457\n",
      "step 254\n",
      "training loss: 2.8583805561065674\n",
      "step 255\n",
      "training loss: 2.847175359725952\n",
      "step 256\n",
      "training loss: 2.853609561920166\n",
      "step 257\n",
      "training loss: 2.869021415710449\n",
      "step 258\n",
      "training loss: 2.882206678390503\n",
      "step 259\n",
      "training loss: 2.8543076515197754\n",
      "step 260\n",
      "training loss: 2.844545841217041\n",
      "validation loss: 2.865283966064453\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.847883939743042\n",
      "step 262\n",
      "training loss: 2.8447983264923096\n",
      "step 263\n",
      "training loss: 2.859696388244629\n",
      "step 264\n",
      "training loss: 2.857818603515625\n",
      "step 265\n",
      "training loss: 2.848430871963501\n",
      "step 266\n",
      "training loss: 2.8656039237976074\n",
      "step 267\n",
      "training loss: 2.870789051055908\n",
      "step 268\n",
      "training loss: 2.8461031913757324\n",
      "step 269\n",
      "training loss: 2.8719799518585205\n",
      "step 270\n",
      "training loss: 2.875286817550659\n",
      "validation loss: 2.889965295791626\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8431711196899414\n",
      "----------3.0 min per epoch----------\n",
      "epoch 38\n",
      "step 0\n",
      "training loss: 2.845111608505249\n",
      "validation loss: 2.8022592067718506\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.8694088459014893\n",
      "step 2\n",
      "training loss: 2.8636066913604736\n",
      "step 3\n",
      "training loss: 2.861893653869629\n",
      "step 4\n",
      "training loss: 2.862920045852661\n",
      "step 5\n",
      "training loss: 2.8652079105377197\n",
      "step 6\n",
      "training loss: 2.8552019596099854\n",
      "step 7\n",
      "training loss: 2.862335681915283\n",
      "step 8\n",
      "training loss: 2.8579225540161133\n",
      "step 9\n",
      "training loss: 2.8469245433807373\n",
      "step 10\n",
      "training loss: 2.8464622497558594\n",
      "validation loss: 2.8487894535064697\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8134799003601074\n",
      "step 12\n",
      "training loss: 2.8500709533691406\n",
      "step 13\n",
      "training loss: 2.868534564971924\n",
      "step 14\n",
      "training loss: 2.8393726348876953\n",
      "step 15\n",
      "training loss: 2.8392248153686523\n",
      "step 16\n",
      "training loss: 2.881708860397339\n",
      "step 17\n",
      "training loss: 2.8577423095703125\n",
      "step 18\n",
      "training loss: 2.8782150745391846\n",
      "step 19\n",
      "training loss: 2.877972364425659\n",
      "step 20\n",
      "training loss: 2.86610746383667\n",
      "validation loss: 2.8619298934936523\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.8476316928863525\n",
      "step 22\n",
      "training loss: 2.8761229515075684\n",
      "step 23\n",
      "training loss: 2.874321937561035\n",
      "step 24\n",
      "training loss: 2.8667612075805664\n",
      "step 25\n",
      "training loss: 2.8619821071624756\n",
      "step 26\n",
      "training loss: 2.8756303787231445\n",
      "step 27\n",
      "training loss: 2.883539915084839\n",
      "step 28\n",
      "training loss: 2.804269552230835\n",
      "step 29\n",
      "training loss: 2.8458144664764404\n",
      "step 30\n",
      "training loss: 2.780858039855957\n",
      "validation loss: 2.8733203411102295\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.8322203159332275\n",
      "step 32\n",
      "training loss: 2.8788931369781494\n",
      "step 33\n",
      "training loss: 2.874209403991699\n",
      "step 34\n",
      "training loss: 2.8829569816589355\n",
      "step 35\n",
      "training loss: 2.8755791187286377\n",
      "step 36\n",
      "training loss: 2.7484335899353027\n",
      "step 37\n",
      "training loss: 2.674036741256714\n",
      "step 38\n",
      "training loss: 2.8060662746429443\n",
      "step 39\n",
      "training loss: 2.8807084560394287\n",
      "step 40\n",
      "training loss: 2.8732521533966064\n",
      "validation loss: 2.9193930625915527\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.8551218509674072\n",
      "step 42\n",
      "training loss: 2.8613696098327637\n",
      "step 43\n",
      "training loss: 2.877363443374634\n",
      "step 44\n",
      "training loss: 2.8786957263946533\n",
      "step 45\n",
      "training loss: 2.8790926933288574\n",
      "step 46\n",
      "training loss: 2.862090826034546\n",
      "step 47\n",
      "training loss: 2.880406379699707\n",
      "step 48\n",
      "training loss: 2.8802566528320312\n",
      "step 49\n",
      "training loss: 2.839963436126709\n",
      "step 50\n",
      "training loss: 2.852794885635376\n",
      "validation loss: 2.867856502532959\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8564155101776123\n",
      "step 52\n",
      "training loss: 2.86092209815979\n",
      "step 53\n",
      "training loss: 2.8735432624816895\n",
      "step 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8652939796447754\n",
      "step 55\n",
      "training loss: 2.8755850791931152\n",
      "step 56\n",
      "training loss: 2.8751845359802246\n",
      "step 57\n",
      "training loss: 2.8690288066864014\n",
      "step 58\n",
      "training loss: 2.8872363567352295\n",
      "step 59\n",
      "training loss: 2.8663036823272705\n",
      "step 60\n",
      "training loss: 2.872039794921875\n",
      "validation loss: 2.8631396293640137\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.882110118865967\n",
      "step 62\n",
      "training loss: 2.8772778511047363\n",
      "step 63\n",
      "training loss: 2.870805025100708\n",
      "step 64\n",
      "training loss: 2.8745086193084717\n",
      "step 65\n",
      "training loss: 2.859215259552002\n",
      "step 66\n",
      "training loss: 2.8373684883117676\n",
      "step 67\n",
      "training loss: 2.8626065254211426\n",
      "step 68\n",
      "training loss: 2.8741302490234375\n",
      "step 69\n",
      "training loss: 2.8421671390533447\n",
      "step 70\n",
      "training loss: 2.860039472579956\n",
      "validation loss: 2.8275561332702637\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8633384704589844\n",
      "step 72\n",
      "training loss: 2.8483896255493164\n",
      "step 73\n",
      "training loss: 2.880664110183716\n",
      "step 74\n",
      "training loss: 2.8550825119018555\n",
      "step 75\n",
      "training loss: 2.8727073669433594\n",
      "step 76\n",
      "training loss: 2.856172561645508\n",
      "step 77\n",
      "training loss: 2.871156930923462\n",
      "step 78\n",
      "training loss: 2.823225975036621\n",
      "step 79\n",
      "training loss: 2.8843986988067627\n",
      "step 80\n",
      "training loss: 2.847991943359375\n",
      "validation loss: 2.8340702056884766\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.881959915161133\n",
      "step 82\n",
      "training loss: 2.864626407623291\n",
      "step 83\n",
      "training loss: 2.8466174602508545\n",
      "step 84\n",
      "training loss: 2.8429391384124756\n",
      "step 85\n",
      "training loss: 2.861333131790161\n",
      "step 86\n",
      "training loss: 2.851963996887207\n",
      "step 87\n",
      "training loss: 2.8403539657592773\n",
      "step 88\n",
      "training loss: 2.8792712688446045\n",
      "step 89\n",
      "training loss: 2.8685953617095947\n",
      "step 90\n",
      "training loss: 2.8589963912963867\n",
      "validation loss: 2.8239076137542725\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.8430678844451904\n",
      "step 92\n",
      "training loss: 2.859401226043701\n",
      "step 93\n",
      "training loss: 2.8567349910736084\n",
      "step 94\n",
      "training loss: 2.831303834915161\n",
      "step 95\n",
      "training loss: 2.8476316928863525\n",
      "step 96\n",
      "training loss: 2.848245143890381\n",
      "step 97\n",
      "training loss: 2.8431754112243652\n",
      "step 98\n",
      "training loss: 2.833805561065674\n",
      "step 99\n",
      "training loss: 2.8697757720947266\n",
      "step 100\n",
      "training loss: 2.8460853099823\n",
      "validation loss: 2.8597829341888428\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.878573179244995\n",
      "step 102\n",
      "training loss: 2.8577449321746826\n",
      "step 103\n",
      "training loss: 2.8363029956817627\n",
      "step 104\n",
      "training loss: 2.87361741065979\n",
      "step 105\n",
      "training loss: 2.8498079776763916\n",
      "step 106\n",
      "training loss: 2.859076499938965\n",
      "step 107\n",
      "training loss: 2.8631956577301025\n",
      "step 108\n",
      "training loss: 2.8284666538238525\n",
      "step 109\n",
      "training loss: 2.876274824142456\n",
      "step 110\n",
      "training loss: 2.8550684452056885\n",
      "validation loss: 2.865168809890747\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.8740100860595703\n",
      "step 112\n",
      "training loss: 2.8706438541412354\n",
      "step 113\n",
      "training loss: 2.8934996128082275\n",
      "step 114\n",
      "training loss: 2.8781139850616455\n",
      "step 115\n",
      "training loss: 2.8343253135681152\n",
      "step 116\n",
      "training loss: 2.8729772567749023\n",
      "step 117\n",
      "training loss: 2.8575336933135986\n",
      "step 118\n",
      "training loss: 2.845223903656006\n",
      "step 119\n",
      "training loss: 2.8560914993286133\n",
      "step 120\n",
      "training loss: 2.8553614616394043\n",
      "validation loss: 2.864692211151123\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.875842571258545\n",
      "step 122\n",
      "training loss: 2.8628616333007812\n",
      "step 123\n",
      "training loss: 2.878284454345703\n",
      "step 124\n",
      "training loss: 2.8726539611816406\n",
      "step 125\n",
      "training loss: 2.8788957595825195\n",
      "step 126\n",
      "training loss: 2.854079008102417\n",
      "step 127\n",
      "training loss: 2.8688955307006836\n",
      "step 128\n",
      "training loss: 2.8615214824676514\n",
      "step 129\n",
      "training loss: 2.8535547256469727\n",
      "step 130\n",
      "training loss: 2.866743326187134\n",
      "validation loss: 2.86055064201355\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.840785264968872\n",
      "step 132\n",
      "training loss: 2.866021156311035\n",
      "step 133\n",
      "training loss: 2.8714985847473145\n",
      "step 134\n",
      "training loss: 2.813929557800293\n",
      "step 135\n",
      "training loss: 2.8532800674438477\n",
      "step 136\n",
      "training loss: 2.8518171310424805\n",
      "step 137\n",
      "training loss: 2.8632113933563232\n",
      "step 138\n",
      "training loss: 2.870884418487549\n",
      "step 139\n",
      "training loss: 2.8445982933044434\n",
      "step 140\n",
      "training loss: 2.866380453109741\n",
      "validation loss: 2.8625643253326416\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.8125624656677246\n",
      "step 142\n",
      "training loss: 2.84987473487854\n",
      "step 143\n",
      "training loss: 2.8736727237701416\n",
      "step 144\n",
      "training loss: 2.863828420639038\n",
      "step 145\n",
      "training loss: 2.855851888656616\n",
      "step 146\n",
      "training loss: 2.845600128173828\n",
      "step 147\n",
      "training loss: 2.863311529159546\n",
      "step 148\n",
      "training loss: 2.8495676517486572\n",
      "step 149\n",
      "training loss: 2.8606460094451904\n",
      "step 150\n",
      "training loss: 2.8638057708740234\n",
      "validation loss: 2.837982416152954\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8620829582214355\n",
      "step 152\n",
      "training loss: 2.855714797973633\n",
      "step 153\n",
      "training loss: 2.859052896499634\n",
      "step 154\n",
      "training loss: 2.832573175430298\n",
      "step 155\n",
      "training loss: 2.838982343673706\n",
      "step 156\n",
      "training loss: 2.8324596881866455\n",
      "step 157\n",
      "training loss: 2.8647117614746094\n",
      "step 158\n",
      "training loss: 2.8694188594818115\n",
      "step 159\n",
      "training loss: 2.8607735633850098\n",
      "step 160\n",
      "training loss: 2.8577892780303955\n",
      "validation loss: 2.840419292449951\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.851673126220703\n",
      "step 162\n",
      "training loss: 2.8676259517669678\n",
      "step 163\n",
      "training loss: 2.8909263610839844\n",
      "step 164\n",
      "training loss: 2.8115804195404053\n",
      "step 165\n",
      "training loss: 2.857697010040283\n",
      "step 166\n",
      "training loss: 2.8536314964294434\n",
      "step 167\n",
      "training loss: 2.876709461212158\n",
      "step 168\n",
      "training loss: 2.8565797805786133\n",
      "step 169\n",
      "training loss: 2.877713203430176\n",
      "step 170\n",
      "training loss: 2.8521149158477783\n",
      "validation loss: 2.831517457962036\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.843858480453491\n",
      "step 172\n",
      "training loss: 2.851625442504883\n",
      "step 173\n",
      "training loss: 2.8625361919403076\n",
      "step 174\n",
      "training loss: 2.849668025970459\n",
      "step 175\n",
      "training loss: 2.874499559402466\n",
      "step 176\n",
      "training loss: 2.8580920696258545\n",
      "step 177\n",
      "training loss: 2.8235204219818115\n",
      "step 178\n",
      "training loss: 2.8502466678619385\n",
      "step 179\n",
      "training loss: 2.851670980453491\n",
      "step 180\n",
      "training loss: 2.8510162830352783\n",
      "validation loss: 2.836852788925171\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.869917154312134\n",
      "step 182\n",
      "training loss: 2.8662848472595215\n",
      "step 183\n",
      "training loss: 2.865518093109131\n",
      "step 184\n",
      "training loss: 2.81199312210083\n",
      "step 185\n",
      "training loss: 2.8614516258239746\n",
      "step 186\n",
      "training loss: 2.856720209121704\n",
      "step 187\n",
      "training loss: 2.862213373184204\n",
      "step 188\n",
      "training loss: 2.8632943630218506\n",
      "step 189\n",
      "training loss: 2.87321138381958\n",
      "step 190\n",
      "training loss: 2.8683605194091797\n",
      "validation loss: 2.871829032897949\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8500659465789795\n",
      "step 192\n",
      "training loss: 2.87681245803833\n",
      "step 193\n",
      "training loss: 2.875990390777588\n",
      "step 194\n",
      "training loss: 2.8661320209503174\n",
      "step 195\n",
      "training loss: 2.8598062992095947\n",
      "step 196\n",
      "training loss: 2.8676135540008545\n",
      "step 197\n",
      "training loss: 2.857532262802124\n",
      "step 198\n",
      "training loss: 2.8549885749816895\n",
      "step 199\n",
      "training loss: 2.839338779449463\n",
      "step 200\n",
      "training loss: 2.8211288452148438\n",
      "validation loss: 2.8741824626922607\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.86796236038208\n",
      "step 202\n",
      "training loss: 2.8473310470581055\n",
      "step 203\n",
      "training loss: 2.8476786613464355\n",
      "step 204\n",
      "training loss: 2.8581621646881104\n",
      "step 205\n",
      "training loss: 2.8900234699249268\n",
      "step 206\n",
      "training loss: 2.842620372772217\n",
      "step 207\n",
      "training loss: 2.8695855140686035\n",
      "step 208\n",
      "training loss: 2.861365795135498\n",
      "step 209\n",
      "training loss: 2.8678953647613525\n",
      "step 210\n",
      "training loss: 2.846121072769165\n",
      "validation loss: 2.8302433490753174\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.858741044998169\n",
      "step 212\n",
      "training loss: 2.834421157836914\n",
      "step 213\n",
      "training loss: 2.850181818008423\n",
      "step 214\n",
      "training loss: 2.8554017543792725\n",
      "step 215\n",
      "training loss: 2.844762086868286\n",
      "step 216\n",
      "training loss: 2.88555645942688\n",
      "step 217\n",
      "training loss: 2.8564255237579346\n",
      "step 218\n",
      "training loss: 2.847120523452759\n",
      "step 219\n",
      "training loss: 2.8595306873321533\n",
      "step 220\n",
      "training loss: 2.864931106567383\n",
      "validation loss: 2.8517274856567383\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8512258529663086\n",
      "step 222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8391237258911133\n",
      "step 223\n",
      "training loss: 2.82356595993042\n",
      "step 224\n",
      "training loss: 2.815164089202881\n",
      "step 225\n",
      "training loss: 2.876331329345703\n",
      "step 226\n",
      "training loss: 2.8553380966186523\n",
      "step 227\n",
      "training loss: 2.8785111904144287\n",
      "step 228\n",
      "training loss: 2.852041482925415\n",
      "step 229\n",
      "training loss: 2.859849214553833\n",
      "step 230\n",
      "training loss: 2.8782379627227783\n",
      "validation loss: 2.8767387866973877\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.858262777328491\n",
      "step 232\n",
      "training loss: 2.843719005584717\n",
      "step 233\n",
      "training loss: 2.8590409755706787\n",
      "step 234\n",
      "training loss: 2.843041181564331\n",
      "step 235\n",
      "training loss: 2.853226900100708\n",
      "step 236\n",
      "training loss: 2.844680070877075\n",
      "step 237\n",
      "training loss: 2.8417537212371826\n",
      "step 238\n",
      "training loss: 2.833395004272461\n",
      "step 239\n",
      "training loss: 2.855253219604492\n",
      "step 240\n",
      "training loss: 2.8565287590026855\n",
      "validation loss: 2.8742165565490723\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.8456361293792725\n",
      "step 242\n",
      "training loss: 2.871065378189087\n",
      "step 243\n",
      "training loss: 2.8457088470458984\n",
      "step 244\n",
      "training loss: 2.860963821411133\n",
      "step 245\n",
      "training loss: 2.8409929275512695\n",
      "step 246\n",
      "training loss: 2.864166736602783\n",
      "step 247\n",
      "training loss: 2.7872321605682373\n",
      "step 248\n",
      "training loss: 2.847144842147827\n",
      "step 249\n",
      "training loss: 2.8309881687164307\n",
      "step 250\n",
      "training loss: 2.8406248092651367\n",
      "validation loss: 2.866957187652588\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.865426778793335\n",
      "step 252\n",
      "training loss: 2.8725428581237793\n",
      "step 253\n",
      "training loss: 2.8622941970825195\n",
      "step 254\n",
      "training loss: 2.8620383739471436\n",
      "step 255\n",
      "training loss: 2.8597216606140137\n",
      "step 256\n",
      "training loss: 2.8477797508239746\n",
      "step 257\n",
      "training loss: 2.853976011276245\n",
      "step 258\n",
      "training loss: 2.868886947631836\n",
      "step 259\n",
      "training loss: 2.882880210876465\n",
      "step 260\n",
      "training loss: 2.855211019515991\n",
      "validation loss: 2.9264450073242188\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.8482673168182373\n",
      "step 262\n",
      "training loss: 2.84723162651062\n",
      "step 263\n",
      "training loss: 2.84605073928833\n",
      "step 264\n",
      "training loss: 2.8587405681610107\n",
      "step 265\n",
      "training loss: 2.8608648777008057\n",
      "step 266\n",
      "training loss: 2.851215124130249\n",
      "step 267\n",
      "training loss: 2.8659486770629883\n",
      "step 268\n",
      "training loss: 2.871370792388916\n",
      "step 269\n",
      "training loss: 2.844496011734009\n",
      "step 270\n",
      "training loss: 2.872281074523926\n",
      "validation loss: 2.879511833190918\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.875455379486084\n",
      "----------3.0 min per epoch----------\n",
      "epoch 39\n",
      "step 0\n",
      "training loss: 2.844348907470703\n",
      "validation loss: 2.828124761581421\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 1\n",
      "training loss: 2.844205856323242\n",
      "step 2\n",
      "training loss: 2.871195077896118\n",
      "step 3\n",
      "training loss: 2.8634700775146484\n",
      "step 4\n",
      "training loss: 2.862771511077881\n",
      "step 5\n",
      "training loss: 2.865569591522217\n",
      "step 6\n",
      "training loss: 2.866046905517578\n",
      "step 7\n",
      "training loss: 2.8562729358673096\n",
      "step 8\n",
      "training loss: 2.8631982803344727\n",
      "step 9\n",
      "training loss: 2.8598995208740234\n",
      "step 10\n",
      "training loss: 2.8507463932037354\n",
      "validation loss: 2.8573336601257324\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 11\n",
      "training loss: 2.8488411903381348\n",
      "step 12\n",
      "training loss: 2.813955783843994\n",
      "step 13\n",
      "training loss: 2.848917245864868\n",
      "step 14\n",
      "training loss: 2.8694140911102295\n",
      "step 15\n",
      "training loss: 2.8384547233581543\n",
      "step 16\n",
      "training loss: 2.833897590637207\n",
      "step 17\n",
      "training loss: 2.8845160007476807\n",
      "step 18\n",
      "training loss: 2.856666088104248\n",
      "step 19\n",
      "training loss: 2.8762056827545166\n",
      "step 20\n",
      "training loss: 2.8799233436584473\n",
      "validation loss: 2.891716241836548\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 21\n",
      "training loss: 2.864515542984009\n",
      "step 22\n",
      "training loss: 2.8430604934692383\n",
      "step 23\n",
      "training loss: 2.8765769004821777\n",
      "step 24\n",
      "training loss: 2.8737845420837402\n",
      "step 25\n",
      "training loss: 2.867392063140869\n",
      "step 26\n",
      "training loss: 2.863802194595337\n",
      "step 27\n",
      "training loss: 2.8769748210906982\n",
      "step 28\n",
      "training loss: 2.885401964187622\n",
      "step 29\n",
      "training loss: 2.803532361984253\n",
      "step 30\n",
      "training loss: 2.8454437255859375\n",
      "validation loss: 2.788722276687622\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 31\n",
      "training loss: 2.773681402206421\n",
      "step 32\n",
      "training loss: 2.8275187015533447\n",
      "step 33\n",
      "training loss: 2.876077890396118\n",
      "step 34\n",
      "training loss: 2.8686041831970215\n",
      "step 35\n",
      "training loss: 2.8819780349731445\n",
      "step 36\n",
      "training loss: 2.8681399822235107\n",
      "step 37\n",
      "training loss: 2.737805128097534\n",
      "step 38\n",
      "training loss: 2.6751301288604736\n",
      "step 39\n",
      "training loss: 2.8082096576690674\n",
      "step 40\n",
      "training loss: 2.879896402359009\n",
      "validation loss: 2.854065418243408\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 41\n",
      "training loss: 2.86909818649292\n",
      "step 42\n",
      "training loss: 2.8502538204193115\n",
      "step 43\n",
      "training loss: 2.859954595565796\n",
      "step 44\n",
      "training loss: 2.8750033378601074\n",
      "step 45\n",
      "training loss: 2.8744211196899414\n",
      "step 46\n",
      "training loss: 2.878734588623047\n",
      "step 47\n",
      "training loss: 2.8642382621765137\n",
      "step 48\n",
      "training loss: 2.87933349609375\n",
      "step 49\n",
      "training loss: 2.879271984100342\n",
      "step 50\n",
      "training loss: 2.8391828536987305\n",
      "validation loss: 2.8640553951263428\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 51\n",
      "training loss: 2.8486177921295166\n",
      "step 52\n",
      "training loss: 2.8574106693267822\n",
      "step 53\n",
      "training loss: 2.860285997390747\n",
      "step 54\n",
      "training loss: 2.8746261596679688\n",
      "step 55\n",
      "training loss: 2.867658853530884\n",
      "step 56\n",
      "training loss: 2.8719215393066406\n",
      "step 57\n",
      "training loss: 2.8733294010162354\n",
      "step 58\n",
      "training loss: 2.866957187652588\n",
      "step 59\n",
      "training loss: 2.88748836517334\n",
      "step 60\n",
      "training loss: 2.863286018371582\n",
      "validation loss: 2.8802082538604736\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 61\n",
      "training loss: 2.871922016143799\n",
      "step 62\n",
      "training loss: 2.882988929748535\n",
      "step 63\n",
      "training loss: 2.877187490463257\n",
      "step 64\n",
      "training loss: 2.8720927238464355\n",
      "step 65\n",
      "training loss: 2.875135898590088\n",
      "step 66\n",
      "training loss: 2.8570494651794434\n",
      "step 67\n",
      "training loss: 2.835935354232788\n",
      "step 68\n",
      "training loss: 2.8624682426452637\n",
      "step 69\n",
      "training loss: 2.872483968734741\n",
      "step 70\n",
      "training loss: 2.834143877029419\n",
      "validation loss: 2.906633138656616\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 71\n",
      "training loss: 2.8574371337890625\n",
      "step 72\n",
      "training loss: 2.8621253967285156\n",
      "step 73\n",
      "training loss: 2.8492183685302734\n",
      "step 74\n",
      "training loss: 2.8821024894714355\n",
      "step 75\n",
      "training loss: 2.853069305419922\n",
      "step 76\n",
      "training loss: 2.8715603351593018\n",
      "step 77\n",
      "training loss: 2.853221893310547\n",
      "step 78\n",
      "training loss: 2.8695380687713623\n",
      "step 79\n",
      "training loss: 2.821460485458374\n",
      "step 80\n",
      "training loss: 2.8828113079071045\n",
      "validation loss: 2.8668582439422607\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 81\n",
      "training loss: 2.847149133682251\n",
      "step 82\n",
      "training loss: 2.878276824951172\n",
      "step 83\n",
      "training loss: 2.8652775287628174\n",
      "step 84\n",
      "training loss: 2.8462986946105957\n",
      "step 85\n",
      "training loss: 2.841499090194702\n",
      "step 86\n",
      "training loss: 2.8624463081359863\n",
      "step 87\n",
      "training loss: 2.8554701805114746\n",
      "step 88\n",
      "training loss: 2.8381950855255127\n",
      "step 89\n",
      "training loss: 2.877110004425049\n",
      "step 90\n",
      "training loss: 2.866483211517334\n",
      "validation loss: 2.8578686714172363\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 91\n",
      "training loss: 2.860409736633301\n",
      "step 92\n",
      "training loss: 2.8408780097961426\n",
      "step 93\n",
      "training loss: 2.8563685417175293\n",
      "step 94\n",
      "training loss: 2.858793020248413\n",
      "step 95\n",
      "training loss: 2.829509973526001\n",
      "step 96\n",
      "training loss: 2.848548650741577\n",
      "step 97\n",
      "training loss: 2.8493666648864746\n",
      "step 98\n",
      "training loss: 2.8441457748413086\n",
      "step 99\n",
      "training loss: 2.8353183269500732\n",
      "step 100\n",
      "training loss: 2.868927001953125\n",
      "validation loss: 2.823655366897583\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 101\n",
      "training loss: 2.8431341648101807\n",
      "step 102\n",
      "training loss: 2.878483295440674\n",
      "step 103\n",
      "training loss: 2.858883857727051\n",
      "step 104\n",
      "training loss: 2.837547779083252\n",
      "step 105\n",
      "training loss: 2.874171495437622\n",
      "step 106\n",
      "training loss: 2.8488311767578125\n",
      "step 107\n",
      "training loss: 2.8577980995178223\n",
      "step 108\n",
      "training loss: 2.8629531860351562\n",
      "step 109\n",
      "training loss: 2.8289034366607666\n",
      "step 110\n",
      "training loss: 2.8742780685424805\n",
      "validation loss: 2.8316800594329834\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 111\n",
      "training loss: 2.852940797805786\n",
      "step 112\n",
      "training loss: 2.8729677200317383\n",
      "step 113\n",
      "training loss: 2.8704965114593506\n",
      "step 114\n",
      "training loss: 2.8908467292785645\n",
      "step 115\n",
      "training loss: 2.878361701965332\n",
      "step 116\n",
      "training loss: 2.8344528675079346\n",
      "step 117\n",
      "training loss: 2.8709397315979004\n",
      "step 118\n",
      "training loss: 2.855611562728882\n",
      "step 119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8458597660064697\n",
      "step 120\n",
      "training loss: 2.8537704944610596\n",
      "validation loss: 2.819408655166626\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 121\n",
      "training loss: 2.856252670288086\n",
      "step 122\n",
      "training loss: 2.8743338584899902\n",
      "step 123\n",
      "training loss: 2.8588085174560547\n",
      "step 124\n",
      "training loss: 2.8762454986572266\n",
      "step 125\n",
      "training loss: 2.8738770484924316\n",
      "step 126\n",
      "training loss: 2.8783910274505615\n",
      "step 127\n",
      "training loss: 2.851677179336548\n",
      "step 128\n",
      "training loss: 2.8704214096069336\n",
      "step 129\n",
      "training loss: 2.8610620498657227\n",
      "step 130\n",
      "training loss: 2.853886365890503\n",
      "validation loss: 2.8564889430999756\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 131\n",
      "training loss: 2.864698648452759\n",
      "step 132\n",
      "training loss: 2.838554859161377\n",
      "step 133\n",
      "training loss: 2.866264581680298\n",
      "step 134\n",
      "training loss: 2.871492624282837\n",
      "step 135\n",
      "training loss: 2.810499668121338\n",
      "step 136\n",
      "training loss: 2.8513383865356445\n",
      "step 137\n",
      "training loss: 2.8495984077453613\n",
      "step 138\n",
      "training loss: 2.8641164302825928\n",
      "step 139\n",
      "training loss: 2.8729348182678223\n",
      "step 140\n",
      "training loss: 2.8420305252075195\n",
      "validation loss: 2.8639588356018066\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 141\n",
      "training loss: 2.867570400238037\n",
      "step 142\n",
      "training loss: 2.8132011890411377\n",
      "step 143\n",
      "training loss: 2.8488965034484863\n",
      "step 144\n",
      "training loss: 2.8707263469696045\n",
      "step 145\n",
      "training loss: 2.8653104305267334\n",
      "step 146\n",
      "training loss: 2.857530117034912\n",
      "step 147\n",
      "training loss: 2.8463430404663086\n",
      "step 148\n",
      "training loss: 2.863614320755005\n",
      "step 149\n",
      "training loss: 2.8509202003479004\n",
      "step 150\n",
      "training loss: 2.8624343872070312\n",
      "validation loss: 2.8580548763275146\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 151\n",
      "training loss: 2.8622422218322754\n",
      "step 152\n",
      "training loss: 2.8625972270965576\n",
      "step 153\n",
      "training loss: 2.8546462059020996\n",
      "step 154\n",
      "training loss: 2.856612205505371\n",
      "step 155\n",
      "training loss: 2.832411050796509\n",
      "step 156\n",
      "training loss: 2.8381481170654297\n",
      "step 157\n",
      "training loss: 2.8324453830718994\n",
      "step 158\n",
      "training loss: 2.8647241592407227\n",
      "step 159\n",
      "training loss: 2.867767333984375\n",
      "step 160\n",
      "training loss: 2.8619418144226074\n",
      "validation loss: 2.858156204223633\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 161\n",
      "training loss: 2.8582632541656494\n",
      "step 162\n",
      "training loss: 2.8524694442749023\n",
      "step 163\n",
      "training loss: 2.866154193878174\n",
      "step 164\n",
      "training loss: 2.886934995651245\n",
      "step 165\n",
      "training loss: 2.8081891536712646\n",
      "step 166\n",
      "training loss: 2.858299732208252\n",
      "step 167\n",
      "training loss: 2.8520748615264893\n",
      "step 168\n",
      "training loss: 2.8744704723358154\n",
      "step 169\n",
      "training loss: 2.8579635620117188\n",
      "step 170\n",
      "training loss: 2.8760316371917725\n",
      "validation loss: 2.860503673553467\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 171\n",
      "training loss: 2.849649667739868\n",
      "step 172\n",
      "training loss: 2.840651273727417\n",
      "step 173\n",
      "training loss: 2.8485636711120605\n",
      "step 174\n",
      "training loss: 2.865354537963867\n",
      "step 175\n",
      "training loss: 2.850088119506836\n",
      "step 176\n",
      "training loss: 2.8721842765808105\n",
      "step 177\n",
      "training loss: 2.8569183349609375\n",
      "step 178\n",
      "training loss: 2.8224403858184814\n",
      "step 179\n",
      "training loss: 2.8494434356689453\n",
      "step 180\n",
      "training loss: 2.851271629333496\n",
      "validation loss: 2.8361644744873047\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 181\n",
      "training loss: 2.84562611579895\n",
      "step 182\n",
      "training loss: 2.8677592277526855\n",
      "step 183\n",
      "training loss: 2.8663172721862793\n",
      "step 184\n",
      "training loss: 2.866971015930176\n",
      "step 185\n",
      "training loss: 2.8082025051116943\n",
      "step 186\n",
      "training loss: 2.8623719215393066\n",
      "step 187\n",
      "training loss: 2.8557164669036865\n",
      "step 188\n",
      "training loss: 2.860457181930542\n",
      "step 189\n",
      "training loss: 2.8611221313476562\n",
      "step 190\n",
      "training loss: 2.8732621669769287\n",
      "validation loss: 2.837705373764038\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 191\n",
      "training loss: 2.8680126667022705\n",
      "step 192\n",
      "training loss: 2.8501222133636475\n",
      "step 193\n",
      "training loss: 2.877871513366699\n",
      "step 194\n",
      "training loss: 2.8732213973999023\n",
      "step 195\n",
      "training loss: 2.8641092777252197\n",
      "step 196\n",
      "training loss: 2.8590283393859863\n",
      "step 197\n",
      "training loss: 2.866896152496338\n",
      "step 198\n",
      "training loss: 2.8574604988098145\n",
      "step 199\n",
      "training loss: 2.8559935092926025\n",
      "step 200\n",
      "training loss: 2.838136911392212\n",
      "validation loss: 2.8272788524627686\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 201\n",
      "training loss: 2.819854974746704\n",
      "step 202\n",
      "training loss: 2.8699705600738525\n",
      "step 203\n",
      "training loss: 2.846884250640869\n",
      "step 204\n",
      "training loss: 2.8467724323272705\n",
      "step 205\n",
      "training loss: 2.858109712600708\n",
      "step 206\n",
      "training loss: 2.888288974761963\n",
      "step 207\n",
      "training loss: 2.842003107070923\n",
      "step 208\n",
      "training loss: 2.8675363063812256\n",
      "step 209\n",
      "training loss: 2.8618416786193848\n",
      "step 210\n",
      "training loss: 2.8681623935699463\n",
      "validation loss: 2.8337841033935547\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 211\n",
      "training loss: 2.8468284606933594\n",
      "step 212\n",
      "training loss: 2.8577706813812256\n",
      "step 213\n",
      "training loss: 2.834728240966797\n",
      "step 214\n",
      "training loss: 2.848738193511963\n",
      "step 215\n",
      "training loss: 2.860901117324829\n",
      "step 216\n",
      "training loss: 2.8427815437316895\n",
      "step 217\n",
      "training loss: 2.8830983638763428\n",
      "step 218\n",
      "training loss: 2.8544187545776367\n",
      "step 219\n",
      "training loss: 2.845644950866699\n",
      "step 220\n",
      "training loss: 2.8586783409118652\n",
      "validation loss: 2.8697052001953125\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 221\n",
      "training loss: 2.8663806915283203\n",
      "step 222\n",
      "training loss: 2.8516533374786377\n",
      "step 223\n",
      "training loss: 2.8408420085906982\n",
      "step 224\n",
      "training loss: 2.8270163536071777\n",
      "step 225\n",
      "training loss: 2.8143763542175293\n",
      "step 226\n",
      "training loss: 2.874922752380371\n",
      "step 227\n",
      "training loss: 2.8563179969787598\n",
      "step 228\n",
      "training loss: 2.8772976398468018\n",
      "step 229\n",
      "training loss: 2.8517444133758545\n",
      "step 230\n",
      "training loss: 2.859849452972412\n",
      "validation loss: 2.872040033340454\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 231\n",
      "training loss: 2.8785619735717773\n",
      "step 232\n",
      "training loss: 2.8582186698913574\n",
      "step 233\n",
      "training loss: 2.845010995864868\n",
      "step 234\n",
      "training loss: 2.857344388961792\n",
      "step 235\n",
      "training loss: 2.843259811401367\n",
      "step 236\n",
      "training loss: 2.8547468185424805\n",
      "step 237\n",
      "training loss: 2.8453006744384766\n",
      "step 238\n",
      "training loss: 2.8392136096954346\n",
      "step 239\n",
      "training loss: 2.832444906234741\n",
      "step 240\n",
      "training loss: 2.854806661605835\n",
      "validation loss: 2.8288071155548096\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 241\n",
      "training loss: 2.857522487640381\n",
      "step 242\n",
      "training loss: 2.845597743988037\n",
      "step 243\n",
      "training loss: 2.8707196712493896\n",
      "step 244\n",
      "training loss: 2.8487086296081543\n",
      "step 245\n",
      "training loss: 2.8602030277252197\n",
      "step 246\n",
      "training loss: 2.8427886962890625\n",
      "step 247\n",
      "training loss: 2.865079641342163\n",
      "step 248\n",
      "training loss: 2.7876639366149902\n",
      "step 249\n",
      "training loss: 2.849478244781494\n",
      "step 250\n",
      "training loss: 2.829547882080078\n",
      "validation loss: 2.8554270267486572\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 251\n",
      "training loss: 2.8428735733032227\n",
      "step 252\n",
      "training loss: 2.8673434257507324\n",
      "step 253\n",
      "training loss: 2.8713316917419434\n",
      "step 254\n",
      "training loss: 2.863046169281006\n",
      "step 255\n",
      "training loss: 2.860597610473633\n",
      "step 256\n",
      "training loss: 2.8575613498687744\n",
      "step 257\n",
      "training loss: 2.84816575050354\n",
      "step 258\n",
      "training loss: 2.8542912006378174\n",
      "step 259\n",
      "training loss: 2.869797468185425\n",
      "step 260\n",
      "training loss: 2.881277322769165\n",
      "validation loss: 2.8818743228912354\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 261\n",
      "training loss: 2.853670835494995\n",
      "step 262\n",
      "training loss: 2.8451294898986816\n",
      "step 263\n",
      "training loss: 2.8483340740203857\n",
      "step 264\n",
      "training loss: 2.844355583190918\n",
      "step 265\n",
      "training loss: 2.860250234603882\n",
      "step 266\n",
      "training loss: 2.8592171669006348\n",
      "step 267\n",
      "training loss: 2.846078634262085\n",
      "step 268\n",
      "training loss: 2.866084575653076\n",
      "step 269\n",
      "training loss: 2.872068405151367\n",
      "step 270\n",
      "training loss: 2.846052408218384\n",
      "validation loss: 2.8759758472442627\n",
      "xxxxxxxxxxxxxx\n",
      "<class 'list'>\n",
      "step 271\n",
      "training loss: 2.8731658458709717\n",
      "----------3.0 min per epoch----------\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict \n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "all_training_loss = OrderedDict()\n",
    "all_val_loss = OrderedDict()\n",
    "argmax_to_save=[]\n",
    "\n",
    "for x in range(40):\n",
    "    print(f\"epoch {x}\")\n",
    "    start = time.time()\n",
    "\n",
    "    training_loss = OrderedDict()\n",
    "    val_loss = OrderedDict()\n",
    "    \n",
    "    \n",
    "    for i in range(NUM_BATCHES):\n",
    "        print(\"step {}\".format(i))\n",
    "        model.train()\n",
    "\n",
    "        tmp = next(train_loader)\n",
    "        input_ids = tmp['input_ids']\n",
    "        attention_mask = tmp['attention_mask']\n",
    "        labels = tmp['labels']\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, prediction_scores = outputs[:2]\n",
    "        loss.backward()\n",
    "        \n",
    "        training_loss[f\"Epoch {x} Step {i}\"] = loss.item()\n",
    "        all_training_loss[f\"Epoch {x} Step {i}\"] = loss.item()\n",
    "        print(f'training loss: {loss.item()}')\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % VALIDATE_EVERY == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                tmp = next(val_loader)\n",
    "                input_ids = tmp['input_ids']\n",
    "                attention_mask = tmp['attention_mask']\n",
    "                labels = tmp['labels']\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss, prediction_scores = outputs[:2]\n",
    "\n",
    "                val_loss[f\"Epoch {x} Step {i}\"] = loss.item()\n",
    "                all_val_loss[f\"Epoch {x} Step {i}\"] = loss.item()\n",
    "                print(f'validation loss: {loss.item()}')\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print('xxxxxxxxxxxxxx')\n",
    "            y = torch.argmax(prediction_scores[0,:,:], dim=1).cpu().numpy() #batch x seq_len x num_tokens -> batch x seq_len\n",
    "            argmax_to_save.append(y.tolist())\n",
    "            print(type(argmax_to_save[0]))\n",
    "            with open('saved_0624/argmax_saved.txt', 'w') as f:\n",
    "                for ele in argmax_to_save:\n",
    "                    f.write(str(ele)+'\\n')\n",
    "\n",
    "            del(y)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"saved_0624/model/saved_model_epoch_{x}.pth\")\n",
    "    \n",
    "    with open(f'saved_0624/saved_losses/training_loss_epoch_{x}.json', 'w') as f:\n",
    "        f.write(json.dumps(training_loss))\n",
    "    \n",
    "    with open(f'saved_0624/saved_losses/val_loss_epoch_{x}.json', 'w') as f:\n",
    "        f.write(json.dumps(val_loss))\n",
    "    end = time.time()\n",
    "    print(f\"----------{(end-start)//60} min per epoch----------\")\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with open(\"saved_0624/saved_losses/training_loss_all.json\", 'w') as f:\n",
    "    f.write(json.dumps(all_training_loss))\n",
    "with open(\"saved_0624/saved_losses/val_loss_all.json\", 'w') as f:\n",
    "    f.write(json.dumps(all_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 13, 13, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 12, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 12, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 13, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 12, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 13, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 12, 20, 20, 20, 20, 13, 13, 20, 13, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 12, 20, 12, 12, 20, 20, 20, 20, 12, 20, 20, 20, 12],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 12, 20, 20, 20, 12, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
       "        [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(prediction_scores, dim=2)[:,0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import sentencepiece as spm\n",
    "from transformers import ReformerConfig, ReformerModelWithLMHead, ReformerTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "NUM_BATCHES = None\n",
    "BATCH_SIZE = 6\n",
    "GRADIENT_ACCUMULATE_EVERY = 3\n",
    "LEARNING_RATE = 0.01\n",
    "VALIDATE_EVERY  = 20\n",
    "SEQ_LEN = 4608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Encoding\n",
    "def encode(list_of_strings, pad_to_max_length=True, pad_token_id=0):\n",
    "    max_length = max([len(string) for string in list_of_strings])\n",
    "\n",
    "    # create emtpy tensors\n",
    "    attention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)\n",
    "    input_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)\n",
    "\n",
    "    for idx, string in enumerate(list_of_strings):\n",
    "        # make sure string is in byte format\n",
    "        if not isinstance(string, bytes):\n",
    "            string = str.encode(string)\n",
    "\n",
    "        input_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])\n",
    "        attention_masks[idx, :len(string)] = 1\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Decoding\n",
    "def decode(outputs_ids):\n",
    "    decoded_outputs = []\n",
    "    for output_ids in outputs_ids.tolist():\n",
    "        # transform id back to char IDs < 2 are simply transformed to \"\"\n",
    "        decoded_outputs.append(\"\".join([chr(x - 2) if x > 1 else \"\" for x in output_ids]))\n",
    "    return decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[67, 68, 69, 70, 71, 72]]), tensor([[1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(['ABCDEF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\"--input=./data/tokenizer_training/AAresiduals.txt \\\n",
    "                                --vocab_size=28 \\\n",
    "                                --model_prefix=sequence_tokenizer \\\n",
    "                                --model_type=char \\\n",
    "                                --character_coverage=1.0\")\n",
    "tokenizer = ReformerTokenizer(vocab_file=\"sequence_tokenizer.model\", do_lower_case=False, model_max_length=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = ReformerConfig.from_pretrained(\"google/reformer-enwik8\")\n",
    "configuration.axial_pos_shape = (64, 72)\n",
    "configuration.max_position_embeddings=SEQ_LEN\n",
    "configuration.vocab_size=tokenizer.vocab_size\n",
    "configuration.save_pretrained('model/config_enwik8_modified/')\n",
    "configuration = ReformerConfig.from_pretrained('model/config_enwik8_modified/')\n",
    "model = ReformerModelWithLMHead(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerModelWithLMHead(\n",
       "  (reformer): ReformerModel(\n",
       "    (embeddings): ReformerEmbeddings(\n",
       "      (word_embeddings): Embedding(28, 1024)\n",
       "      (position_embeddings): AxialPositionEmbeddings(\n",
       "        (weights): ParameterList(\n",
       "            (0): Parameter containing: [torch.FloatTensor of size 64x1x256]\n",
       "            (1): Parameter containing: [torch.FloatTensor of size 1x72x768]\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): ReformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LSHSelfAttention(\n",
       "              (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): ReformerLayer(\n",
       "          (attention): ReformerAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (self_attention): LocalSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (output): ReformerSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (feed_forward): ChunkReformerFeedForward(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dense): ReformerFeedForwardDense(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): ReformerFeedForwardOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): ReformerOnlyLMHead(\n",
       "    (decoder): Linear(in_features=2048, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4608"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = torch.tensor(tokenizer.encode(\"ABCDEFGH\", add_special_tokens=True, pad_to_max_length=True)).unsqueeze(0)  # Batch size 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4608])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_id, labels = input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, prediction_scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4608, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[torch.argmax(prediction_scores, dim=2) == input_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 4,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 6, 1, 6, 2, 6, 2, 5]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.randint(7, (1,)).item() for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.roll(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4608\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4608, 28])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8784,  1.2836, -1.0793,  ..., -0.6524, -2.6258, -1.0666],\n",
       "         [ 1.1133,  1.1670, -0.8323,  ..., -1.0635,  0.9155, -0.1506],\n",
       "         [ 0.3937,  1.0289,  0.8168,  ..., -0.4873, -0.8674,  0.1013],\n",
       "         ...,\n",
       "         [ 0.9368, -2.1731,  0.8456,  ...,  1.9906, -3.1547, -1.3232],\n",
       "         [ 1.2605, -1.3634, -0.4402,  ...,  0.3854,  0.1005, -1.1920],\n",
       "         [ 1.1513,  0.2879, -0.3987,  ..., -0.1991, -0.5784, -1.4584]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8784,  1.2836, -1.0793,  ..., -0.6524, -2.6258, -1.0666],\n",
       "         [ 1.1133,  1.1670, -0.8323,  ..., -1.0635,  0.9155, -0.1506],\n",
       "         [ 0.3937,  1.0289,  0.8168,  ..., -0.4873, -0.8674,  0.1013],\n",
       "         ...,\n",
       "         [ 0.6132, -0.1413,  0.1096,  ...,  1.2919, -1.4866, -0.8239],\n",
       "         [ 0.9368, -2.1731,  0.8456,  ...,  1.9906, -3.1547, -1.3232],\n",
       "         [ 1.2605, -1.3634, -0.4402,  ...,  0.3854,  0.1005, -1.1920]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores[..., :-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.8784,  1.2836, -1.0793, -1.7684,  0.4985,  0.6058,  0.6368, -1.1766,\n",
       "        -1.2031,  0.4661,  0.4723,  0.5633, -2.3839,  0.6446, -0.5566,  1.0457,\n",
       "         0.3205,  1.0964,  0.6490,  0.0175,  1.2438, -1.3752,  0.1407,  0.1297,\n",
       "        -1.0813, -0.6524, -2.6258, -1.0666], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores[0,0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 3, 4,  ..., 0, 0, 0]]), torch.Size([1, 4608]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id, input_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 5,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[..., 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fct = CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fct(prediction_scores[..., :-1, :].view(-1, 28), input_id[..., 1:].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9867, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8784,  1.2836, -1.0793,  ..., -0.6524, -2.6258, -1.0666],\n",
       "        [ 1.1133,  1.1670, -0.8323,  ..., -1.0635,  0.9155, -0.1506],\n",
       "        [ 0.3937,  1.0289,  0.8168,  ..., -0.4873, -0.8674,  0.1013],\n",
       "        ...,\n",
       "        [ 0.6132, -0.1413,  0.1096,  ...,  1.2919, -1.4866, -0.8239],\n",
       "        [ 0.9368, -2.1731,  0.8456,  ...,  1.9906, -3.1547, -1.3232],\n",
       "        [ 1.2605, -1.3634, -0.4402,  ...,  0.3854,  0.1005, -1.1920]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores[..., :-1, :].view(-1, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 19,  4,  5,  6,  7,  8,  9, 10, 19,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[:,0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4,  5,  6,  7,  8,  9, 10,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[:,0:15][..., 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 19, 20, 10, 25,  0, 17, 15, 20, 20, 10, 14, 25, 20, 11]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(prediction_scores[..., :-1, :], dim=2)[:, 0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9867, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fct(prediction_scores[..., :-1, :].view(-1, 28), input_id[..., 1:].view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id[0,1] = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7635, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fct(prediction_scores[..., :-1, :].view(-1, 28), input_id[..., 1:].view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id[0,10] = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6840, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fct(prediction_scores[..., :-1, :].view(-1, 28), input_id[..., 1:].view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19,  4,  5,  6,  7,  8,  9, 10, 19, 19,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[:,0:15][..., 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 19, 20, 10, 25,  0, 17, 15, 20, 20, 10, 14, 25, 20, 11]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(prediction_scores[..., :-1, :], dim=2)[:, 0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id[0,2]=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6215, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fct(prediction_scores[..., :-1, :].view(-1, 28), input_id[..., 1:].view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4052, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[0,3]=20\n",
    "loss = loss_fct(prediction_scores[..., :-1, :].view(-1, 28), input_id[..., 1:].view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineByLineTextDataset(Dataset):\n",
    "    \"\"\"modified: \n",
    "    https://github.com/huggingface/transformers/blob/cb3c2212c79d7ff0a4a4e84c3db48371ecc1c15d/src/transformers/data/datasets/language_modeling.py#L77\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, file_path: str):\n",
    "        assert os.path.isfile(file_path)\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "#         lines = lines[:50_000]\n",
    "        batch_encoding = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=tokenizer.vocab_size)\n",
    "        self.examples = batch_encoding[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor(tokenizer.encode(\"ALKLAKALK\", \n",
    "#                                           add_special_tokens=True, \n",
    "#                                           max_length=SEQ_LEN, \n",
    "#                                           pad_to_max_length=True)).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
